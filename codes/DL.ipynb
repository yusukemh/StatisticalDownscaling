{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#tensorflow\n",
    "#import tensorflow as tf\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# others\n",
    "from copy import deepcopy\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m air1000_500 hgt500 hgt1000 omega500 pottemp1000-500 pottemp1000-850 pr_wtr shum-uwnd-700 shum-uwnd-925 shum-vwnd-700 shum-vwnd-950 shum700 shum925 skt slp season_wet elevation lat lon "
     ]
    }
   ],
   "source": [
    "# Split the stations by the number of samples available\n",
    "columns = deepcopy(LABELS)\n",
    "columns.extend([\"season_wet\", \"elevation\", \"lat\", \"lon\"])\n",
    "for item in columns:\n",
    "    print(item, end=' ')\n",
    "\n",
    "# # load datasets\n",
    "# df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_combined = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\")\n",
    "\n",
    "# inner join with valid station (more than 300 stations)\n",
    "threshold = 300\n",
    "df_skn = df_load.groupby('skn').size().reset_index().rename(columns={0: \"n_samples\"})\n",
    "df_skn_valid = df_skn[df_skn['n_samples'] > threshold]\n",
    "\n",
    "df_data = df_load.merge(right=df_skn_valid, left_on='skn', right_on='skn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skn = df_data['skn'].sample().values[0]\n",
    "\n",
    "# df_temp = df_data[df_data['year'] <= 2007]\n",
    "# df_test = df_data[df_data['year'] > 2007]\n",
    "\n",
    "# df_temp.sort_values(['year', 'month'], inplace=True)\n",
    "# df_test.sort_values(['year', 'month'], inplace=True)\n",
    "\n",
    "# Xtemp = np.array(df_temp[df_temp['skn'] == skn][columns])\n",
    "# Ytemp = np.array(df_temp[df_temp['skn'] == skn]['data_in'])\n",
    "\n",
    "# Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtemp, Ytemp, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Xtest = np.array(df_test[df_test['skn'] == skn][columns])\n",
    "# Ytest = np.array(df_test[df_test['skn'] == skn]['data_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255.0\n"
     ]
    }
   ],
   "source": [
    "skn = df_data['skn'].sample().values[0]\n",
    "print(f'{skn}')\n",
    "df_station = df_data[df_data['skn'] == skn]\n",
    "\n",
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "\n",
    "Xtemp, Xtest, Ytemp, Ytest = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtemp, Ytemp, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xvalid = scaler.transform(Xvalid)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def define_model(num_inputs=20, lr=0.01):\n",
    "    inputs = Input(shape=(num_inputs,))\n",
    "    x = Dense(units=20, activation='relu')(inputs)\n",
    "    x = Dense(units=16, activation='relu')(x)\n",
    "    x = Dense(units=8, activation='relu')(x)\n",
    "    x = Dense(units=4, activation='relu')(x)\n",
    "    outputs = Dense(units=1, kernel_initializer='normal')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 13.7252 - root_mean_squared_error: 3.7045 - val_loss: 11.6045 - val_root_mean_squared_error: 3.4065\n",
      "Epoch 2/20\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 13.4588 - root_mean_squared_error: 3.6686 - val_loss: 11.3683 - val_root_mean_squared_error: 3.3717\n",
      "Epoch 3/20\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 13.1983 - root_mean_squared_error: 3.6329 - val_loss: 10.9418 - val_root_mean_squared_error: 3.3078\n",
      "Epoch 4/20\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 12.4080 - root_mean_squared_error: 3.5223 - val_loss: 10.2955 - val_root_mean_squared_error: 3.2087\n",
      "Epoch 5/20\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 11.8507 - root_mean_squared_error: 3.4425 - val_loss: 9.3414 - val_root_mean_squared_error: 3.0564\n",
      "Epoch 6/20\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 10.6043 - root_mean_squared_error: 3.2563 - val_loss: 7.9962 - val_root_mean_squared_error: 2.8278\n",
      "Epoch 7/20\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 9.2773 - root_mean_squared_error: 3.0458 - val_loss: 6.3301 - val_root_mean_squared_error: 2.5160\n",
      "Epoch 8/20\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 7.3509 - root_mean_squared_error: 2.7112 - val_loss: 4.6177 - val_root_mean_squared_error: 2.1489\n",
      "Epoch 9/20\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 5.5735 - root_mean_squared_error: 2.3607 - val_loss: 3.6897 - val_root_mean_squared_error: 1.9209\n",
      "Epoch 10/20\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.7983 - root_mean_squared_error: 2.1905 - val_loss: 4.0418 - val_root_mean_squared_error: 2.0104\n",
      "Epoch 11/20\n",
      "2/2 [==============================] - 0s 26ms/step - loss: 5.7593 - root_mean_squared_error: 2.3999 - val_loss: 3.8188 - val_root_mean_squared_error: 1.9542\n",
      "Epoch 12/20\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 5.5016 - root_mean_squared_error: 2.3454 - val_loss: 3.4435 - val_root_mean_squared_error: 1.8557\n",
      "Epoch 13/20\n",
      "2/2 [==============================] - 0s 27ms/step - loss: 4.6630 - root_mean_squared_error: 2.1594 - val_loss: 3.5242 - val_root_mean_squared_error: 1.8773\n",
      "Epoch 14/20\n",
      "2/2 [==============================] - 0s 25ms/step - loss: 4.4364 - root_mean_squared_error: 2.1063 - val_loss: 3.7950 - val_root_mean_squared_error: 1.9481\n",
      "Epoch 15/20\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 4.5441 - root_mean_squared_error: 2.1317 - val_loss: 3.9197 - val_root_mean_squared_error: 1.9798\n",
      "1.8651224779434123\n"
     ]
    }
   ],
   "source": [
    "model = define_model()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, mode='min')\n",
    "epochs=20\n",
    "\n",
    "history = model.fit(\n",
    "    Xtrain, Ytrain, \n",
    "    epochs=epochs, \n",
    "    validation_data = (Xvalid, Yvalid),\n",
    "    callbacks=[callback],\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "yhat = model.predict(Xtest)\n",
    "print(mean_squared_error(Ytest, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.525728016000597\n"
     ]
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(Xtrain, Ytrain)\n",
    "yhat = linear_regression.predict(Xtest)\n",
    "\n",
    "print(mean_squared_error(Ytest, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8924042344701761\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 260, 'learning_rate': 0.1, 'max_depth': 3, 'early_stopping_rounds': 8, 'verbosity': 0}\n",
    "xgboost = XGBRegressor(**params)\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "yhat = xgboost.predict(Xtest)\n",
    "print(mean_squared_error(Ytest, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-21 20:18:35.701595: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-21 20:18:35.706247: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-04-21 20:18:35.727848: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-04-21 20:18:35.727891: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: gpu-0008\n",
      "2022-04-21 20:18:35.727902: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: gpu-0008\n",
      "2022-04-21 20:18:35.728030: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 495.29.5\n",
      "2022-04-21 20:18:35.728075: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 495.29.5\n",
      "2022-04-21 20:18:35.728084: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 495.29.5\n",
      "2022-04-21 20:18:35.728625: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-21 20:18:35.729973: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2022-04-21 20:18:35.852962: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-21 20:18:35.853760: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2400000000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2117/2117 [==============================] - 5s 2ms/step - loss: 40.1199 - val_loss: 29.8457\n",
      "Epoch 2/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 25.6364 - val_loss: 27.5414\n",
      "Epoch 3/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 23.3881 - val_loss: 26.6088\n",
      "Epoch 4/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 22.3660 - val_loss: 25.7302\n",
      "Epoch 5/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 21.8574 - val_loss: 25.0664\n",
      "Epoch 6/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 21.1332 - val_loss: 24.7238\n",
      "Epoch 7/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.8463 - val_loss: 24.4219\n",
      "Epoch 8/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.5769 - val_loss: 23.9221\n",
      "Epoch 9/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.2927 - val_loss: 23.7671\n",
      "Epoch 10/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.4057 - val_loss: 23.8525\n",
      "Epoch 11/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.1106 - val_loss: 23.5282\n",
      "Epoch 12/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 20.0854 - val_loss: 23.3584\n",
      "Epoch 13/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 19.9880 - val_loss: 23.7062\n",
      "Epoch 14/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 19.8382 - val_loss: 23.6925\n",
      "Epoch 15/20\n",
      "2117/2117 [==============================] - 4s 2ms/step - loss: 19.7234 - val_loss: 23.6892\n"
     ]
    }
   ],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, mode='min')\n",
    "# construct a model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "model.add(layers.Dense(8, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='relu'))\n",
    "model.add(layers.Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "epochs = 20\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'mean_squared_error'\n",
    ")\n",
    "history = model.fit(\n",
    "    Xtrain, Ytrain, \n",
    "    epochs=epochs, \n",
    "    validation_data = (Xvalid, Yvalid),\n",
    "    callbacks=[callback],\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: nan (nan) MSE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yusukemh/.conda/envs/climate/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "10 fits failed out of a total of 10.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/yusukemh/.conda/envs/climate/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/yusukemh/.conda/envs/climate/lib/python3.9/site-packages/tensorflow/python/keras/wrappers/scikit_learn.py\", line 157, in fit\n",
      "    self.model = self.build_fn(**self.filter_sk_params(self.build_fn))\n",
      "  File \"/tmp/ipykernel_234848/1957097577.py\", line 3, in baseline_model\n",
      "    model = Sequential()\n",
      "NameError: name 'Sequential' is not defined\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(19, input_dim=19, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    # Compile model\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "# evaluate model\n",
    "estimator = KerasRegressor(build_fn=baseline_model, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, X, Y, cv=kfold)\n",
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
