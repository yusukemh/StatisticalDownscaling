{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "\n",
    "# others\n",
    "from copy import deepcopy\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m air1000_500 hgt500 hgt1000 omega500 pottemp1000-500 pottemp1000-850 pr_wtr shum-uwnd-700 shum-uwnd-925 shum-vwnd-700 shum-vwnd-950 shum700 shum925 skt slp season_wet elevation lat lon "
     ]
    }
   ],
   "source": [
    "# Split the stations by the number of samples available\n",
    "columns = deepcopy(LABELS)\n",
    "columns.extend([\"season_wet\", \"elevation\", \"lat\", \"lon\"])\n",
    "for item in columns:\n",
    "    print(item, end=' ')\n",
    "\n",
    "# # load datasets\n",
    "# df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "# df_combined = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\")\n",
    "\n",
    "# inner join with valid station (more than 300 stations)\n",
    "threshold = 300\n",
    "df_skn = df_load.groupby('skn').size().reset_index().rename(columns={0: \"n_samples\"})\n",
    "df_skn_valid = df_skn[df_skn['n_samples'] > threshold]\n",
    "\n",
    "df_data = df_load.merge(right=df_skn_valid, left_on='skn', right_on='skn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skn = df_data['skn'].sample().values[0]\n",
    "\n",
    "# df_temp = df_data[df_data['year'] <= 2007]\n",
    "# df_test = df_data[df_data['year'] > 2007]\n",
    "\n",
    "# df_temp.sort_values(['year', 'month'], inplace=True)\n",
    "# df_test.sort_values(['year', 'month'], inplace=True)\n",
    "\n",
    "# Xtemp = np.array(df_temp[df_temp['skn'] == skn][columns])\n",
    "# Ytemp = np.array(df_temp[df_temp['skn'] == skn]['data_in'])\n",
    "\n",
    "# Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtemp, Ytemp, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Xtest = np.array(df_test[df_test['skn'] == skn][columns])\n",
    "# Ytest = np.array(df_test[df_test['skn'] == skn]['data_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    return mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "def define_model(num_inputs=20, lr=0.0065):\n",
    "    inputs = Input(shape=(num_inputs,))\n",
    "    x = Dense(units=20, activation='relu')(inputs)\n",
    "    # x = Dense(units=16, activation='relu')(inputs)\n",
    "    x = Dense(units=8, activation='relu')(x)\n",
    "    x = Dense(units=4, activation='relu')(x)\n",
    "    outputs = Dense(units=1, kernel_initializer='normal')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try testing on a single station with simple single train/valid/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1114.0\n"
     ]
    }
   ],
   "source": [
    "skn = df_data['skn'].sample().values[0]\n",
    "print(f'{skn}')\n",
    "df_station = df_data[df_data['skn'] == skn]\n",
    "\n",
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "\n",
    "Xtemp, Xtest, Ytemp, Ytest = train_test_split(X, Y, test_size=0.2, shuffle=False)\n",
    "Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtemp, Ytemp, test_size=0.2, shuffle=False)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "Xtrain = scaler.fit_transform(Xtrain)\n",
    "Xvalid = scaler.transform(Xvalid)\n",
    "Xtest = scaler.transform(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_predict_function.<locals>.predict_function at 0x2b00ff79d9d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "2.6401822684077336\n"
     ]
    }
   ],
   "source": [
    "model = define_model()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 5, mode='min')\n",
    "epochs=20\n",
    "\n",
    "history = model.fit(\n",
    "    Xtrain, Ytrain, \n",
    "    epochs=epochs, \n",
    "    validation_data = (Xvalid, Yvalid),\n",
    "    callbacks=[callback],\n",
    "    batch_size=64,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "yhat = model.predict(Xtest)\n",
    "print(mean_squared_error(Ytest, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.839487093427457\n"
     ]
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(Xtrain, Ytrain)\n",
    "yhat = linear_regression.predict(Xtest)\n",
    "\n",
    "print(mean_squared_error(Ytest, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12209866513206\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {'n_estimators': 260, 'learning_rate': 0.1, 'max_depth': 3, 'early_stopping_rounds': 8, 'verbosity': 0}\n",
    "xgboost = XGBRegressor(**params)\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "yhat = xgboost.predict(Xtest)\n",
    "print(mean_squared_error(Ytest, yhat, squared=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looks good. Now implement cross_val_predict for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict_for_nn(model, X, Y, callback, batch_size, epochs, verbose):\n",
    "    kf = KFold(n_splits=5)\n",
    "    y_pred = []\n",
    "\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        Xtemp, Xtest = X[train_index], X[test_index]\n",
    "        Ytemp, Ytest = Y[train_index], Y[test_index]\n",
    "        \n",
    "        Xtrain, Xvalid, Ytrain, Yvalid = train_test_split(Xtemp, Ytemp, test_size=0.2, shuffle=True)\n",
    "        \n",
    "        # scale the input\n",
    "        scaler = StandardScaler()\n",
    "        Xtrain = scaler.fit_transform(Xtrain)\n",
    "        Xvalid = scaler.transform(Xvalid)\n",
    "        Xtest = scaler.transform(Xtest)\n",
    "        \n",
    "        model.fit(\n",
    "            Xtrain, Ytrain, epochs=epochs,\n",
    "            validation_data = (Xvalid, Yvalid),\n",
    "            callbacks=[callback],\n",
    "            batch_size=batch_size,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        y_pred.extend(model.predict(Xtest).tolist())\n",
    "    \n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.271283341053673"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skn = df_data['skn'].sample().values[0]\n",
    "print(f'{skn}')\n",
    "df_station = df_data[df_data['skn'] == skn]\n",
    "\n",
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "\n",
    "model = define_model()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, mode='min')\n",
    "epochs=20\n",
    "batch_size=64\n",
    "\n",
    "yhat = cross_val_predict_for_nn(model, X, Y, callback, batch_size, epochs, verbose=0)\n",
    "mean_squared_error(Y, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1039270543980253"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = cross_val_predict(LinearRegression(), X, Y, n_jobs=-1)\n",
    "mean_squared_error(Y, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SKN: 147.1\n",
      "RMSE on NN: 1.088\n",
      "RMSE on LR: 1.148\n",
      "SKN: 930.0\n",
      "RMSE on NN: 1.984\n",
      "RMSE on LR: 2.190\n",
      "SKN: 474.0\n",
      "RMSE on NN: 3.572\n",
      "RMSE on LR: 3.722\n",
      "SKN: 1020.5\n",
      "RMSE on NN: 2.423\n",
      "RMSE on LR: 2.502\n",
      "SKN: 93.11\n",
      "RMSE on NN: 3.440\n",
      "RMSE on LR: 3.462\n",
      "SKN: 723.4\n",
      "RMSE on NN: 1.800\n",
      "RMSE on LR: 2.004\n",
      "SKN: 666.0\n",
      "RMSE on NN: 1.908\n",
      "RMSE on LR: 2.012\n",
      "SKN: 249.1\n",
      "RMSE on NN: 1.314\n",
      "RMSE on LR: 1.460\n",
      "SKN: 713.0\n",
      "RMSE on NN: 2.092\n",
      "RMSE on LR: 2.228\n",
      "SKN: 800.3\n",
      "RMSE on NN: 2.180\n",
      "RMSE on LR: 2.348\n"
     ]
    }
   ],
   "source": [
    "# try on ten random stations\n",
    "\n",
    "skns = df_data['skn'].sample(n=10).values\n",
    "for skn in skns:\n",
    "    df_station = df_data[df_data['skn'] == skn]\n",
    "\n",
    "    X = np.array(df_station[columns])\n",
    "    Y = np.array(df_station['data_in'])\n",
    "\n",
    "    model = define_model()\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 3, mode='min')\n",
    "    epochs=20\n",
    "    batch_size=64\n",
    "\n",
    "    yhat = cross_val_predict_for_nn(model, X, Y, callback, batch_size, epochs, verbose=0)\n",
    "    rmse_nn = mean_squared_error(Y, yhat, squared=False)\n",
    "    \n",
    "    yhat = cross_val_predict(LinearRegression(), X, Y, n_jobs=-1)\n",
    "    rmse_lr = mean_squared_error(Y, yhat, squared=False)\n",
    "    print(f\"SKN: {skn}\")\n",
    "    print(\"RMSE on NN: {:.3f}\\nRMSE on LR: {:.3f}\".format(rmse_nn, rmse_lr))\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
