{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#others\n",
    "from xgboost import XGBRegressor\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import time\n",
    "import xarray as xr\n",
    "import sherpa\n",
    "import time\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy import interpolate\n",
    "from copy import deepcopy\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES\n",
    "from math import pi as PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m air1000_500 hgt500 hgt1000 omega500 pottemp1000-500 pottemp1000-850 pr_wtr shum-uwnd-700 shum-uwnd-925 shum-vwnd-700 shum-vwnd-950 shum700 shum925 skt slp season_wet elevation lat lon "
     ]
    }
   ],
   "source": [
    "# Split the stations by the number of samples available\n",
    "columns = deepcopy(LABELS)\n",
    "columns.extend([\"season_wet\", \"elevation\", \"lat\", \"lon\"])\n",
    "for item in columns:\n",
    "    print(item, end=' ')\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_combined = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "if False: # no need to run this cell\n",
    "    df_original = pd.read_excel(f\"{BASE_DIR}/FilledDataset2012.xlsx\", sheet_name=\"Source\")\n",
    "    X = []\n",
    "    for index, row in df_original.iterrows():\n",
    "        if row.Year < 1948:\n",
    "            # No need to keep data older than 1948 becase no data exists in netCDF files\n",
    "            continue\n",
    "        for i, cell in enumerate(row[2:]):\n",
    "            X.append([row.SKN, row.Year, i + 1, cell])\n",
    "    df_nonan = pd.DataFrame(X, columns=['skn', 'year', 'month', 'method']).dropna()\n",
    "\n",
    "    valid_method = [\n",
    "        'State/NCDC', 'State', 'NCDC', 'RAWS', 'Hydronet', 'SCAN', 'USGS',\n",
    "        'Hydronet/NCDC', 'HaleNet', 'HC&S', 'AlanMair', 'AlanMair/State',\n",
    "        'USGS/State', 'USGS/NCDC'\n",
    "    ]\n",
    "    df_valid_method = pd.DataFrame({\"method\": valid_method})\n",
    "    df_valid_method = df_nonan.merge(right=df_valid_method, left_on='method', right_on='method')\n",
    "\n",
    "    # inner join with valid method\n",
    "    df_data_valid = df_combined.merge(right=df_valid_method, left_on=[\"skn\", \"year\", \"month\"], right_on=['skn', 'year', 'month'])\n",
    "\n",
    "    df_data_valid.to_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_load = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join with valid station (more than 300 stations)\n",
    "threshold = 300\n",
    "df_skn = df_load.groupby('skn').size().reset_index().rename(columns={0: \"n_samples\"})\n",
    "df_skn_valid = df_skn[df_skn['n_samples'] > threshold]\n",
    "\n",
    "df_data = df_load.merge(right=df_skn_valid, left_on='skn', right_on='skn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_data[df_data['year'] <= 2007]\n",
    "df_test = df_data[df_data['month'] > 2007]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "df_result_linear = []\n",
    "for name, group in df_data.groupby(by='skn'):\n",
    "    group.sort_values(by=['year', 'month'], inplace=True)\n",
    "    \n",
    "    X = np.array(group[columns])\n",
    "    Y = np.array(group['data_in'])\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    \n",
    "    yhat = cross_val_predict(model, X, Y, n_jobs=-1)\n",
    "    df_result_linear.append(pd.DataFrame({\"skn\": [name] * group.shape[0], \"data_in\": Y, \"pred\": yhat}))\n",
    "df_result_linear = pd.concat(df_result_linear)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.5282127272604025"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(df_result_linear['data_in'], df_result_linear['pred'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rmse_per_station = (\n",
    "    df_result_linear.groupby('skn')\n",
    "    .apply(\n",
    "        lambda group: mean_squared_error(group['data_in'], group['pred'], squared=False)\n",
    "    )\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"rmse\"})\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "549/550\r"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 260, 'learning_rate': 0.1, 'max_depth': 3, 'early_stopping_rounds': 8, 'verbosity': 0}\n",
    "n_stations = df_data.groupby('skn').size().shape[0]\n",
    "df_result_xgb = []\n",
    "for i, (name, group) in enumerate(df_data.groupby(by='skn')):\n",
    "    \n",
    "    group.sort_values(by=['year', 'month'], inplace=True)\n",
    "    \n",
    "    X = np.array(group[columns])\n",
    "    Y = np.array(group['data_in'])\n",
    "    \n",
    "    model = XGBRegressor(**params)\n",
    "    yhat = cross_val_predict(model, X, Y, n_jobs=-1)\n",
    "    \n",
    "    df_result_xgb.append(pd.DataFrame({\"skn\": [name] * group.shape[0], \"data_in\": Y, \"pred\": yhat}))\n",
    "    print(f\"{i}/{n_stations}\",end='\\r')\n",
    "\n",
    "df_result_xgb = pd.concat(df_result_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_linear.to_csv(f\"{BASE_DIR}/realdata/result_linear.csv\", index=False)\n",
    "df_result_xgb.to_csv(f\"{BASE_DIR}/realdata/result_xgb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_xgb = pd.read_csv(f\"{BASE_DIR}/realdata/result_xgb.csv\")\n",
    "df_resilt_linear =  pd.read_csv(f\"{BASE_DIR}/realdata/result_linear.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on linear regression:3.528\n",
      "RMSE on xgb:3.805\n"
     ]
    }
   ],
   "source": [
    "rmse_linear = mean_squared_error(df_result_linear['data_in'], df_result_linear['pred'], squared=False)\n",
    "rmse_xgb = mean_squared_error(df_result_xgb['data_in'], df_result_xgb['pred'], squared=False)\n",
    "\n",
    "print(\"RMSE on linear regression:{:.3f}\\nRMSE on xgb:{:.3f}\".format(rmse_linear, rmse_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
