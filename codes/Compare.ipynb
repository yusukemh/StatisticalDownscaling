{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#others\n",
    "from xgboost import XGBRegressor\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import time\n",
    "import xarray as xr\n",
    "import sherpa\n",
    "\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m_0 air1000_500_0 hgt500_0 hgt1000_0 omega500_0 pottemp1000-500_0 pottemp1000-850_0 pr_wtr_0 shum-uwnd-700_0 shum-uwnd-925_0 shum-vwnd-700_0 shum-vwnd-950_0 shum700_0 shum925_0 skt_0 slp_0 air2m_1 air1000_500_1 hgt500_1 hgt1000_1 omega500_1 pottemp1000-500_1 pottemp1000-850_1 pr_wtr_1 shum-uwnd-700_1 shum-uwnd-925_1 shum-vwnd-700_1 shum-vwnd-950_1 shum700_1 shum925_1 skt_1 slp_1 air2m_2 air1000_500_2 hgt500_2 hgt1000_2 omega500_2 pottemp1000-500_2 pottemp1000-850_2 pr_wtr_2 shum-uwnd-700_2 shum-uwnd-925_2 shum-vwnd-700_2 shum-vwnd-950_2 shum700_2 shum925_2 skt_2 slp_2 air2m_3 air1000_500_3 hgt500_3 hgt1000_3 omega500_3 pottemp1000-500_3 pottemp1000-850_3 pr_wtr_3 shum-uwnd-700_3 shum-uwnd-925_3 shum-vwnd-700_3 shum-vwnd-950_3 shum700_3 shum925_3 skt_3 slp_3 air2m_4 air1000_500_4 hgt500_4 hgt1000_4 omega500_4 pottemp1000-500_4 pottemp1000-850_4 pr_wtr_4 shum-uwnd-700_4 shum-uwnd-925_4 shum-vwnd-700_4 shum-vwnd-950_4 shum700_4 shum925_4 skt_4 slp_4 air2m_5 air1000_500_5 hgt500_5 hgt1000_5 omega500_5 pottemp1000-500_5 pottemp1000-850_5 pr_wtr_5 shum-uwnd-700_5 shum-uwnd-925_5 shum-vwnd-700_5 shum-vwnd-950_5 shum700_5 shum925_5 skt_5 slp_5 data_in lat lon elevation season_wet season_dry "
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "# Load the dataset\n",
    "# df_metadata = pd.read_excel(f\"{BASE_DIR}/FilledDataset2012.xlsx\", sheet_name=\"Header\")\n",
    "# df_data_original = pd.read_csv(f\"{BASE_DIR}/dataset.csv\")\n",
    "\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\")\n",
    "df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\")\n",
    "df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\")\n",
    "\n",
    "# Nov-Apr = \"wet\", May-Oct = \"dry\"\n",
    "wet = [11, 12, 1, 2, 3, 4]\n",
    "dry = [5, 6, 7, 8, 9, 10]\n",
    "df_train['season_dry'] = df_train.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_train['season_wet'] = df_train.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "df_valid['season_dry'] = df_valid.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_valid['season_wet'] = df_valid.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "df_test['season_dry'] = df_test.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_test['season_wet'] = df_test.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "reanalysis_data = [\n",
    "    'air2m', 'air1000_500', 'hgt500', 'hgt1000', 'omega500',\n",
    "    'pottemp1000-500', 'pottemp1000-850', 'pr_wtr', 'shum-uwnd-700',\n",
    "    'shum-uwnd-925', 'shum-vwnd-700', 'shum-vwnd-950', 'shum700', 'shum925', \n",
    "    'skt', 'slp'\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for i in range(6):\n",
    "    for item in reanalysis_data:\n",
    "        columns.append(f\"{item}_{i}\")\n",
    "\n",
    "columns.extend(['data_in', 'lat', 'lon', 'elevation', 'season_wet', 'season_dry'])\n",
    "for item in columns:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m_0 air1000_500_0 hgt500_0 hgt1000_0 omega500_0 pottemp1000-500_0 pottemp1000-850_0 pr_wtr_0 shum-uwnd-700_0 shum-uwnd-925_0 shum-vwnd-700_0 shum-vwnd-950_0 shum700_0 shum925_0 skt_0 slp_0 air2m_1 air1000_500_1 hgt500_1 hgt1000_1 omega500_1 pottemp1000-500_1 pottemp1000-850_1 pr_wtr_1 shum-uwnd-700_1 shum-uwnd-925_1 shum-vwnd-700_1 shum-vwnd-950_1 shum700_1 shum925_1 skt_1 slp_1 air2m_2 air1000_500_2 hgt500_2 hgt1000_2 omega500_2 pottemp1000-500_2 pottemp1000-850_2 pr_wtr_2 shum-uwnd-700_2 shum-uwnd-925_2 shum-vwnd-700_2 shum-vwnd-950_2 shum700_2 shum925_2 skt_2 slp_2 air2m_3 air1000_500_3 hgt500_3 hgt1000_3 omega500_3 pottemp1000-500_3 pottemp1000-850_3 pr_wtr_3 shum-uwnd-700_3 shum-uwnd-925_3 shum-vwnd-700_3 shum-vwnd-950_3 shum700_3 shum925_3 skt_3 slp_3 air2m_4 air1000_500_4 hgt500_4 hgt1000_4 omega500_4 pottemp1000-500_4 pottemp1000-850_4 pr_wtr_4 shum-uwnd-700_4 shum-uwnd-925_4 shum-vwnd-700_4 shum-vwnd-950_4 shum700_4 shum925_4 skt_4 slp_4 air2m_5 air1000_500_5 hgt500_5 hgt1000_5 omega500_5 pottemp1000-500_5 pottemp1000-850_5 pr_wtr_5 shum-uwnd-700_5 shum-uwnd-925_5 shum-vwnd-700_5 shum-vwnd-950_5 shum700_5 shum925_5 skt_5 slp_5 data_in lat lon elevation season_wet season_dry "
     ]
    }
   ],
   "source": [
    "reanalysis_data = [\n",
    "    'air2m', 'air1000_500', 'hgt500', 'hgt1000', 'omega500',\n",
    "    'pottemp1000-500', 'pottemp1000-850', 'pr_wtr', 'shum-uwnd-700',\n",
    "    'shum-uwnd-925', 'shum-vwnd-700', 'shum-vwnd-950', 'shum700', 'shum925', \n",
    "    'skt', 'slp'\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for i in range(6):\n",
    "    for item in reanalysis_data:\n",
    "        columns.append(f\"{item}_{i}\")\n",
    "\n",
    "columns.extend(['data_in', 'lat', 'lon', 'elevation', 'season_wet', 'season_dry'])\n",
    "for item in columns:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_bigbog = np.where(df_metadata[\"Name\"] == \"Big Bog\")[0]\n",
    "# idx_hilo = np.where(df_metadata[\"Name\"] == \"HILO\")[0]\n",
    "# idx_hono = np.where(df_metadata[\"Name\"] == \"HONOLULU\")[0]\n",
    "# idx_lihue = np.where(df_metadata[\"Name\"] == \"LIHUE\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skn_bigbog = float(df_metadata.iloc[idx_bigbog]['SKN'])\n",
    "# skn_hilo = float(df_metadata.iloc[idx_hilo]['SKN'])\n",
    "# skn_hono = float(df_metadata.iloc[idx_hono]['SKN'])\n",
    "# skn_lihue = float(df_metadata.iloc[idx_lihue]['SKN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_bigbog = df_train[df_train[\"name\"] == \"Big Bog\"]\n",
    "df_test_bigbog = df_test[df_test[\"name\"]==\"Big Bog\"]\n",
    "\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "\n",
    "# Xvalid = np.array(df_valid[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "# Yvalid = np.array(df_valid[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_bigbog[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_bigbog[\"data_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on xgboost (test) : 280.83985\n",
      "MSE on xgboost (train): 4.65344\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Linear Regression (test) : 266.88864\n",
      "MSE on Linear Regression (train): 155.14460\n"
     ]
    }
   ],
   "source": [
    "Xtrain = np.array(df_train_bigbog[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_bigbog[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_bigbog[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_bigbog[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_hilo = df_train[df_train[\"name\"] == \"HILO\"]\n",
    "df_test_hilo = df_test[df_test[\"name\"]==\"HILO\"]\n",
    "\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "\n",
    "# Xvalid = np.array(df_valid[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "# Yvalid = np.array(df_valid[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_hilo[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_hilo[\"data_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on xgboost (test) : 36.22984\n",
      "MSE on xgboost (train): 4.65344\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Linear Regression (test) : 33.09867\n",
      "MSE on Linear Regression (train): 24.18849\n"
     ]
    }
   ],
   "source": [
    "Xtrain = np.array(df_train_hilo[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_hilo[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_hilo[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_hilo[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test[\"name\"] == \"HONOLULU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_117474/96854125.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE on xgboost (test) : {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MSE on xgboost (train): {:.5f}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/climate/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;36m0.825\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \"\"\"\n\u001b[0;32m--> 438\u001b[0;31m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultioutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m     )\n",
      "\u001b[0;32m~/.conda/envs/climate/lib/python3.9/site-packages/sklearn/metrics/_regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \"\"\"\n\u001b[1;32m     94\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/climate/lib/python3.9/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    806\u001b[0m                 \u001b[0;34m\"Found array with %d sample(s) (shape=%s) while a\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m                 \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "df_train_honolulu = df_train[df_train[\"name\"] == \"HONOLULU\"]\n",
    "df_test_honolulu = df_test[df_test[\"name\"]==\"HONOLULU\"]\n",
    "\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "\n",
    "# Xvalid = np.array(df_valid[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "# Yvalid = np.array(df_valid[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_honolulu[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_honolulu[\"data_in\"])\n",
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))\n",
    "\n",
    "Xtrain = np.array(df_train_honolulu[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_honolulu[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_honolulu[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_honolulu[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_lihue = df_train[df_train[\"name\"] == \"LIHUE\"]\n",
    "df_test_lihue = df_test[df_test[\"name\"]==\"LIHUE\"]\n",
    "\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "\n",
    "# Xvalid = np.array(df_valid[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "# Yvalid = np.array(df_valid[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_lihue[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_lihue[\"data_in\"])\n",
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))\n",
    "\n",
    "Xtrain = np.array(df_train_lihue[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_lihue[\"data_in\"])\n",
    "\n",
    "Xtest = np.array(df_test_lihue[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_lihue[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
