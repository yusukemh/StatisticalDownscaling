{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#others\n",
    "from xgboost import XGBRegressor\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import time\n",
    "import xarray as xr\n",
    "import sherpa\n",
    "\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m_0 air1000_500_0 hgt500_0 hgt1000_0 omega500_0 pottemp1000-500_0 pottemp1000-850_0 pr_wtr_0 shum-uwnd-700_0 shum-uwnd-925_0 shum-vwnd-700_0 shum-vwnd-950_0 shum700_0 shum925_0 skt_0 slp_0 air2m_1 air1000_500_1 hgt500_1 hgt1000_1 omega500_1 pottemp1000-500_1 pottemp1000-850_1 pr_wtr_1 shum-uwnd-700_1 shum-uwnd-925_1 shum-vwnd-700_1 shum-vwnd-950_1 shum700_1 shum925_1 skt_1 slp_1 air2m_2 air1000_500_2 hgt500_2 hgt1000_2 omega500_2 pottemp1000-500_2 pottemp1000-850_2 pr_wtr_2 shum-uwnd-700_2 shum-uwnd-925_2 shum-vwnd-700_2 shum-vwnd-950_2 shum700_2 shum925_2 skt_2 slp_2 air2m_3 air1000_500_3 hgt500_3 hgt1000_3 omega500_3 pottemp1000-500_3 pottemp1000-850_3 pr_wtr_3 shum-uwnd-700_3 shum-uwnd-925_3 shum-vwnd-700_3 shum-vwnd-950_3 shum700_3 shum925_3 skt_3 slp_3 air2m_4 air1000_500_4 hgt500_4 hgt1000_4 omega500_4 pottemp1000-500_4 pottemp1000-850_4 pr_wtr_4 shum-uwnd-700_4 shum-uwnd-925_4 shum-vwnd-700_4 shum-vwnd-950_4 shum700_4 shum925_4 skt_4 slp_4 air2m_5 air1000_500_5 hgt500_5 hgt1000_5 omega500_5 pottemp1000-500_5 pottemp1000-850_5 pr_wtr_5 shum-uwnd-700_5 shum-uwnd-925_5 shum-vwnd-700_5 shum-vwnd-950_5 shum700_5 shum925_5 skt_5 slp_5 data_in lat lon elevation season_wet season_dry "
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "# Load the dataset\n",
    "# df_metadata = pd.read_excel(f\"{BASE_DIR}/FilledDataset2012.xlsx\", sheet_name=\"Header\")\n",
    "# df_data_original = pd.read_csv(f\"{BASE_DIR}/dataset.csv\")\n",
    "\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\")\n",
    "df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\")\n",
    "df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\")\n",
    "\n",
    "# Nov-Apr = \"wet\", May-Oct = \"dry\"\n",
    "wet = [11, 12, 1, 2, 3, 4]\n",
    "dry = [5, 6, 7, 8, 9, 10]\n",
    "df_train['season_dry'] = df_train.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_train['season_wet'] = df_train.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "df_valid['season_dry'] = df_valid.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_valid['season_wet'] = df_valid.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "df_test['season_dry'] = df_test.apply(lambda row: 1 if row.month in dry else 0, axis=1)\n",
    "df_test['season_wet'] = df_test.apply(lambda row: 1 if row.month in wet else 0, axis=1)\n",
    "\n",
    "reanalysis_data = [\n",
    "    'air2m', 'air1000_500', 'hgt500', 'hgt1000', 'omega500',\n",
    "    'pottemp1000-500', 'pottemp1000-850', 'pr_wtr', 'shum-uwnd-700',\n",
    "    'shum-uwnd-925', 'shum-vwnd-700', 'shum-vwnd-950', 'shum700', 'shum925', \n",
    "    'skt', 'slp'\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for i in range(6):\n",
    "    for item in reanalysis_data:\n",
    "        columns.append(f\"{item}_{i}\")\n",
    "\n",
    "columns.extend(['data_in', 'lat', 'lon', 'elevation', 'season_wet', 'season_dry'])\n",
    "for item in columns:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m_0 air1000_500_0 hgt500_0 hgt1000_0 omega500_0 pottemp1000-500_0 pottemp1000-850_0 pr_wtr_0 shum-uwnd-700_0 shum-uwnd-925_0 shum-vwnd-700_0 shum-vwnd-950_0 shum700_0 shum925_0 skt_0 slp_0 air2m_1 air1000_500_1 hgt500_1 hgt1000_1 omega500_1 pottemp1000-500_1 pottemp1000-850_1 pr_wtr_1 shum-uwnd-700_1 shum-uwnd-925_1 shum-vwnd-700_1 shum-vwnd-950_1 shum700_1 shum925_1 skt_1 slp_1 air2m_2 air1000_500_2 hgt500_2 hgt1000_2 omega500_2 pottemp1000-500_2 pottemp1000-850_2 pr_wtr_2 shum-uwnd-700_2 shum-uwnd-925_2 shum-vwnd-700_2 shum-vwnd-950_2 shum700_2 shum925_2 skt_2 slp_2 air2m_3 air1000_500_3 hgt500_3 hgt1000_3 omega500_3 pottemp1000-500_3 pottemp1000-850_3 pr_wtr_3 shum-uwnd-700_3 shum-uwnd-925_3 shum-vwnd-700_3 shum-vwnd-950_3 shum700_3 shum925_3 skt_3 slp_3 air2m_4 air1000_500_4 hgt500_4 hgt1000_4 omega500_4 pottemp1000-500_4 pottemp1000-850_4 pr_wtr_4 shum-uwnd-700_4 shum-uwnd-925_4 shum-vwnd-700_4 shum-vwnd-950_4 shum700_4 shum925_4 skt_4 slp_4 air2m_5 air1000_500_5 hgt500_5 hgt1000_5 omega500_5 pottemp1000-500_5 pottemp1000-850_5 pr_wtr_5 shum-uwnd-700_5 shum-uwnd-925_5 shum-vwnd-700_5 shum-vwnd-950_5 shum700_5 shum925_5 skt_5 slp_5 data_in lat lon elevation season_wet season_dry "
     ]
    }
   ],
   "source": [
    "reanalysis_data = [\n",
    "    'air2m', 'air1000_500', 'hgt500', 'hgt1000', 'omega500',\n",
    "    'pottemp1000-500', 'pottemp1000-850', 'pr_wtr', 'shum-uwnd-700',\n",
    "    'shum-uwnd-925', 'shum-vwnd-700', 'shum-vwnd-950', 'shum700', 'shum925', \n",
    "    'skt', 'slp'\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for i in range(6):\n",
    "    for item in reanalysis_data:\n",
    "        columns.append(f\"{item}_{i}\")\n",
    "\n",
    "columns.extend(['data_in', 'lat', 'lon', 'elevation', 'season_wet', 'season_dry'])\n",
    "for item in columns:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_bigbog = np.where(df_metadata[\"Name\"] == \"Big Bog\")[0]\n",
    "# idx_hilo = np.where(df_metadata[\"Name\"] == \"HILO\")[0]\n",
    "# idx_hono = np.where(df_metadata[\"Name\"] == \"HONOLULU\")[0]\n",
    "# idx_lihue = np.where(df_metadata[\"Name\"] == \"LIHUE\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skn_bigbog = float(df_metadata.iloc[idx_bigbog]['SKN'])\n",
    "# skn_hilo = float(df_metadata.iloc[idx_hilo]['SKN'])\n",
    "# skn_hono = float(df_metadata.iloc[idx_hono]['SKN'])\n",
    "# skn_lihue = float(df_metadata.iloc[idx_lihue]['SKN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"Big Bog\"\n",
    "\n",
    "df_train_station = df_train[df_train['name'] == station]\n",
    "df_test_station  = df_test[df_test['name'] == station]\n",
    "\n",
    "# xgboost trains on the entire dataset\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on xgboost (test) : 280.83985\n",
      "MSE on xgboost (train): 4.65344\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE on Linear Regression (test) : 266.88864\n",
      "MSE on Linear Regression (train): 155.14460\n"
     ]
    }
   ],
   "source": [
    "# linear regression trains on the station dataset\n",
    "Xtrain = np.array(df_train_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_station[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"HILO\"\n",
    "\n",
    "df_train_station = df_train[df_train['name'] == station]\n",
    "df_test_station  = df_test[df_test['name'] == station]\n",
    "\n",
    "# xgboost trains on the entire dataset\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])\n",
    "\n",
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))\n",
    "\n",
    "# linear regression trains on the station dataset\n",
    "Xtrain = np.array(df_train_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_station[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station = \"LIHUE\"\n",
    "\n",
    "df_train_station = df_train[df_train['name'] == station]\n",
    "df_test_station  = df_test[df_test['name'] == station]\n",
    "\n",
    "# xgboost trains on the entire dataset\n",
    "Xtrain = np.array(df_train[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])\n",
    "\n",
    "# hyperparameters obtained by fine tuning\n",
    "xgboost = XGBRegressor(\n",
    "    n_estimators=170,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=9,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "xgboost.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on xgboost (test) : {:.5f}\".format(mean_squared_error(Ytest, xgboost.predict(Xtest))))\n",
    "print(\"MSE on xgboost (train): {:.5f}\".format(mean_squared_error(Ytrain, xgboost.predict(Xtrain))))\n",
    "\n",
    "# linear regression trains on the station dataset\n",
    "Xtrain = np.array(df_train_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytrain = np.array(df_train_station[\"data_in\"])\n",
    "# test on the station data\n",
    "Xtest = np.array(df_test_station[columns].drop(labels=[\"data_in\"], axis=1))\n",
    "Ytest = np.array(df_test_station[\"data_in\"])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"MSE on Linear Regression (test) : {:.5f}\".format(mean_squared_error(Ytest, model.predict(Xtest))))\n",
    "print(\"MSE on Linear Regression (train): {:.5f}\".format(mean_squared_error(Ytrain, model.predict(Xtrain))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
