{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment variable for warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# others\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from copy import deepcopy\n",
    "import sherpa\n",
    "from xgboost import XGBRegressor\n",
    "import sys\n",
    "import time\n",
    "from scipy.stats import gamma, norm, beta\n",
    "\n",
    "# Variables from config file\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/codes/')\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES, C_SINGLE, C_INT50, C_INT100, C_GRID, C_COMMON\n",
    "\n",
    "# util\n",
    "from util import cross_val_predict_for_nn, sample_station, estimate_epochs, define_model, define_hetero_model_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_air2m = xr.open_dataset(f\"{BASE_DIR}/air.2m.mon.mean.regridded.nc\")\n",
    "ds_air1000_500 = xr.open_dataset(f\"{BASE_DIR}/air.1000-500.mon.mean.nc\")\n",
    "ds_hgt500 = xr.open_dataset(f\"{BASE_DIR}/hgt500.mon.mean.nc\")\n",
    "ds_hgt1000 = xr.open_dataset(f\"{BASE_DIR}/hgt1000.mon.mean.nc\")\n",
    "ds_omega500 = xr.open_dataset(f\"{BASE_DIR}/omega500.mon.mean.nc\")\n",
    "ds_pottemp_1000_500 = xr.open_dataset(f\"{BASE_DIR}/pottmp.1000-500.mon.mean.nc\")\n",
    "ds_pottemp_1000_850 = xr.open_dataset(f\"{BASE_DIR}/pottmp.1000-850.mon.mean.nc\")\n",
    "ds_pwtr = xr.open_dataset(f\"{BASE_DIR}/pwtr.mon.mean.nc\")\n",
    "ds_u700 = xr.open_dataset(f\"{BASE_DIR}/shum_x_uwnd.700.mon.mean.nc\")\n",
    "ds_u925 = xr.open_dataset(f\"{BASE_DIR}/shum_x_uwnd.925.mon.mean.nc\")\n",
    "ds_v700 = xr.open_dataset(f\"{BASE_DIR}/shum_x_vwnd.700.mon.mean.nc\")\n",
    "ds_v950 = xr.open_dataset(f\"{BASE_DIR}/shum_x_vwnd.925.mon.mean.nc\")\n",
    "ds_shum700 = xr.open_dataset(f\"{BASE_DIR}/shum700.mon.mean.nc\")\n",
    "ds_shum925 = xr.open_dataset(f\"{BASE_DIR}/shum925.mon.mean.nc\")\n",
    "ds_skt = xr.open_dataset(f\"{BASE_DIR}/skt.mon.mean.regridded.nc\")\n",
    "ds_slp = xr.open_dataset(f\"{BASE_DIR}/slp.mon.mean.nc\")\n",
    "\n",
    "# ait temperature difference\n",
    "datasets = [ # list of tuples. (dataset object, attribute string in ds)\n",
    "    (ds_air2m, \"air\"), # surface air temperature 2m\n",
    "    (ds_air1000_500, \"air\"), # air temperature difference\n",
    "    (ds_hgt500, \"hgt\"), # geopotential height (500hPa)\n",
    "    (ds_hgt1000, \"hgt\"), # geopotential height (1000hPa)\n",
    "    (ds_omega500, \"omega\"), # omega\n",
    "    (ds_pottemp_1000_500, \"pottmp\"), # potential temperature difference 1000-500\n",
    "    (ds_pottemp_1000_850, \"pottmp\"), # potential temperature fifference 1000-850\n",
    "    (ds_pwtr, \"pr_wtr\"), # precipitable water\n",
    "    (ds_u700, \"shum\"), # zonal moisture (u) transport\n",
    "    (ds_u925, \"shum\"), # zonal moisture (u) transport\n",
    "    (ds_v700, \"shum\"), # meridional moisture (v) transport\n",
    "    (ds_v950, \"shum\"), # meridional moisture (v) transport\n",
    "    (ds_shum700, \"shum\"), # specific humidity: 700 hPa \n",
    "    (ds_shum925, \"shum\"), # specific humidity: 925 hPa\n",
    "    (ds_skt, \"skt\"), # skin temperature\n",
    "    (ds_slp, \"slp\") # sea level pressure\n",
    "]\n",
    "# combine all the cdf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: 'â–º';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: 'â–¼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (lat: 73, level: 1, lon: 144, time: 840)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
       "  * level    (level) float32 700.0\n",
       "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
       "  * time     (time) datetime64[ns] 1948-01-01 1948-02-01 ... 2017-12-01\n",
       "Data variables:\n",
       "    shum     (time, level, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    description:    Data is from NMC initialized reanalysis\\n(4x/day).  It co...\n",
       "    platform:       Model\n",
       "    Conventions:    COARDS\n",
       "    NCO:            4.2.6\n",
       "    history:        Sun Aug 26 20:22:35 2018: ncks -O -d level,700.000000 -d ...\n",
       "    title:          monthly mean shum from the NCEP Reanalysis\n",
       "    References:     http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reana...\n",
       "    dataset_title:  NCEP-NCAR Reanalysis 1</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-b4da56dc-0b96-4388-a1d1-ddc57e1c11b6' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-b4da56dc-0b96-4388-a1d1-ddc57e1c11b6' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>lat</span>: 73</li><li><span class='xr-has-index'>level</span>: 1</li><li><span class='xr-has-index'>lon</span>: 144</li><li><span class='xr-has-index'>time</span>: 840</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-bfad2530-3a33-41da-9726-cdb7b6bbb592' class='xr-section-summary-in' type='checkbox'  checked><label for='section-bfad2530-3a33-41da-9726-cdb7b6bbb592' class='xr-section-summary' >Coordinates: <span>(4)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>90.0 87.5 85.0 ... -87.5 -90.0</div><input id='attrs-cb30f434-0378-482f-aa42-1c317ca163bf' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-cb30f434-0378-482f-aa42-1c317ca163bf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d21d5538-519a-475a-9927-247c939c2c73' class='xr-var-data-in' type='checkbox'><label for='data-d21d5538-519a-475a-9927-247c939c2c73' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>long_name :</span></dt><dd>Latitude</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>axis :</span></dt><dd>Y</dd><dt><span>actual_range :</span></dt><dd>[ 90. -90.]</dd></dl></div><div class='xr-var-data'><pre>array([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n",
       "        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n",
       "        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n",
       "        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n",
       "       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n",
       "       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n",
       "       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n",
       "       -85. , -87.5, -90. ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>level</span></div><div class='xr-var-dims'>(level)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>700.0</div><input id='attrs-b5d2b117-1ea8-47ff-acf1-43df46da0f4e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-b5d2b117-1ea8-47ff-acf1-43df46da0f4e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8889dbf0-8ca7-4a94-94f9-a1da24293cc2' class='xr-var-data-in' type='checkbox'><label for='data-8889dbf0-8ca7-4a94-94f9-a1da24293cc2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>millibar</dd><dt><span>long_name :</span></dt><dd>Level</dd><dt><span>positive :</span></dt><dd>down</dd><dt><span>GRIB_id :</span></dt><dd>100</dd><dt><span>GRIB_name :</span></dt><dd>hPa</dd><dt><span>axis :</span></dt><dd>Z</dd><dt><span>actual_range :</span></dt><dd>[700. 700.]</dd></dl></div><div class='xr-var-data'><pre>array([700.], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0 2.5 5.0 ... 352.5 355.0 357.5</div><input id='attrs-eceef105-7970-4fda-b722-699781b2f032' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-eceef105-7970-4fda-b722-699781b2f032' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-09f0bb34-1be8-49c4-ac32-55d5347b3d6f' class='xr-var-data-in' type='checkbox'><label for='data-09f0bb34-1be8-49c4-ac32-55d5347b3d6f' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>long_name :</span></dt><dd>Longitude</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>axis :</span></dt><dd>X</dd><dt><span>actual_range :</span></dt><dd>[  0.  357.5]</dd></dl></div><div class='xr-var-data'><pre>array([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n",
       "        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n",
       "        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n",
       "        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n",
       "       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n",
       "       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n",
       "       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n",
       "       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n",
       "       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n",
       "       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n",
       "       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n",
       "       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n",
       "       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n",
       "       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n",
       "       350. , 352.5, 355. , 357.5], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>1948-01-01 ... 2017-12-01</div><input id='attrs-c6fae67c-febb-4f20-96bc-06638d273c3a' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-c6fae67c-febb-4f20-96bc-06638d273c3a' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-1b3f3b6e-ed74-4ec5-b775-8afc436149f2' class='xr-var-data-in' type='checkbox'><label for='data-1b3f3b6e-ed74-4ec5-b775-8afc436149f2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Time</dd><dt><span>delta_t :</span></dt><dd>0000-01-00 00:00:00</dd><dt><span>avg_period :</span></dt><dd>0000-01-00 00:00:00</dd><dt><span>prev_avg_period :</span></dt><dd>0000-00-01 00:00:00</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>actual_range :</span></dt><dd>[1297320. 1910208.]</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;1948-01-01T00:00:00.000000000&#x27;, &#x27;1948-02-01T00:00:00.000000000&#x27;,\n",
       "       &#x27;1948-03-01T00:00:00.000000000&#x27;, ..., &#x27;2017-10-01T00:00:00.000000000&#x27;,\n",
       "       &#x27;2017-11-01T00:00:00.000000000&#x27;, &#x27;2017-12-01T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-b414eef0-ebd9-44de-bb64-3e7ecc435eba' class='xr-section-summary-in' type='checkbox'  checked><label for='section-b414eef0-ebd9-44de-bb64-3e7ecc435eba' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>shum</span></div><div class='xr-var-dims'>(time, level, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-0d7daaf5-4498-4fc8-bb3d-4df20e412678' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-0d7daaf5-4498-4fc8-bb3d-4df20e412678' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-4047ff6a-a9b4-45f9-8397-641dd4b5f73c' class='xr-var-data-in' type='checkbox'><label for='data-4047ff6a-a9b4-45f9-8397-641dd4b5f73c' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Monthly Mean of Specific Humidity</dd><dt><span>units :</span></dt><dd>grams/kg</dd><dt><span>precision :</span></dt><dd>3</dd><dt><span>var_desc :</span></dt><dd>Specific Humidity</dd><dt><span>level_desc :</span></dt><dd>Multiple levels</dd><dt><span>statistic :</span></dt><dd>Mean</dd><dt><span>parent_stat :</span></dt><dd>Other</dd><dt><span>valid_range :</span></dt><dd>[-9.999999e-02  1.004300e+02]</dd><dt><span>dataset :</span></dt><dd>NCEP Reanalysis Derived Products</dd><dt><span>actual_range :</span></dt><dd>[7.9994202e-03 1.0523001e+01]</dd></dl></div><div class='xr-var-data'><pre>[8830080 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-20c1056a-9e5e-4cec-824b-55c529a44362' class='xr-section-summary-in' type='checkbox'  checked><label for='section-20c1056a-9e5e-4cec-824b-55c529a44362' class='xr-section-summary' >Attributes: <span>(8)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>description :</span></dt><dd>Data is from NMC initialized reanalysis\n",
       "(4x/day).  It consists of most variables interpolated to\n",
       "pressure surfaces from model (sigma) surfaces.</dd><dt><span>platform :</span></dt><dd>Model</dd><dt><span>Conventions :</span></dt><dd>COARDS</dd><dt><span>NCO :</span></dt><dd>4.2.6</dd><dt><span>history :</span></dt><dd>Sun Aug 26 20:22:35 2018: ncks -O -d level,700.000000 -d lat,-90.000000,90.000000 -d lon,0.000000,357.500000 -d time,0,839 /Datasets/ncep.reanalysis.derived/pressure/shum.mon.mean.nc /Public/www/X98.151.202.176.237.20.22.34.nc\n",
       "Mon Jul  5 22:28:55 1999: ncrcat shum.mon.mean.nc /Datasets/ncep.reanalysis.derived/pressure/shum.mon.mean.nc /dm/dmwork/nmc.rean.ingest/combinedMMs/shum.mon.mean.nc\n",
       "/home/hoop/crdc/cpreanjuke2farm/cpreanjuke2farm Tue Oct 17 20:07:08 1995 from shum.85.nc\n",
       "created 95/02/06 by Hoop (netCDF2.3)\n",
       "Converted to chunked, deflated non-packed NetCDF4 2014/09</dd><dt><span>title :</span></dt><dd>monthly mean shum from the NCEP Reanalysis</dd><dt><span>References :</span></dt><dd>http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.derived.html</dd><dt><span>dataset_title :</span></dt><dd>NCEP-NCAR Reanalysis 1</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 73, level: 1, lon: 144, time: 840)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
       "  * level    (level) float32 700.0\n",
       "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
       "  * time     (time) datetime64[ns] 1948-01-01 1948-02-01 ... 2017-12-01\n",
       "Data variables:\n",
       "    shum     (time, level, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    description:    Data is from NMC initialized reanalysis\\n(4x/day).  It co...\n",
       "    platform:       Model\n",
       "    Conventions:    COARDS\n",
       "    NCO:            4.2.6\n",
       "    history:        Sun Aug 26 20:22:35 2018: ncks -O -d level,700.000000 -d ...\n",
       "    title:          monthly mean shum from the NCEP Reanalysis\n",
       "    References:     http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reana...\n",
       "    dataset_title:  NCEP-NCAR Reanalysis 1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[12][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locations = pd.read_csv(f\"{BASE_DIR}/SKNlocations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18.91367961, 22.23135314)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_locations['Lat_DD'].min(), df_locations['Lat_DD'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m_0 air1000_500_0 hgt500_0 hgt1000_0 omega500_0 pottemp1000-500_0 pottemp1000-850_0 pr_wtr_0 shum-uwnd-700_0 shum-uwnd-925_0 shum-vwnd-700_0 shum-vwnd-950_0 shum700_0 shum925_0 skt_0 slp_0 air2m_1 air1000_500_1 hgt500_1 hgt1000_1 omega500_1 pottemp1000-500_1 pottemp1000-850_1 pr_wtr_1 shum-uwnd-700_1 shum-uwnd-925_1 shum-vwnd-700_1 shum-vwnd-950_1 shum700_1 shum925_1 skt_1 slp_1 air2m_2 air1000_500_2 hgt500_2 hgt1000_2 omega500_2 pottemp1000-500_2 pottemp1000-850_2 pr_wtr_2 shum-uwnd-700_2 shum-uwnd-925_2 shum-vwnd-700_2 shum-vwnd-950_2 shum700_2 shum925_2 skt_2 slp_2 air2m_3 air1000_500_3 hgt500_3 hgt1000_3 omega500_3 pottemp1000-500_3 pottemp1000-850_3 pr_wtr_3 shum-uwnd-700_3 shum-uwnd-925_3 shum-vwnd-700_3 shum-vwnd-950_3 shum700_3 shum925_3 skt_3 slp_3 air2m_4 air1000_500_4 hgt500_4 hgt1000_4 omega500_4 pottemp1000-500_4 pottemp1000-850_4 pr_wtr_4 shum-uwnd-700_4 shum-uwnd-925_4 shum-vwnd-700_4 shum-vwnd-950_4 shum700_4 shum925_4 skt_4 slp_4 air2m_5 air1000_500_5 hgt500_5 hgt1000_5 omega500_5 pottemp1000-500_5 pottemp1000-850_5 pr_wtr_5 shum-uwnd-700_5 shum-uwnd-925_5 shum-vwnd-700_5 shum-vwnd-950_5 shum700_5 shum925_5 skt_5 slp_5 data_in lat lon elevation season_wet season_dry "
     ]
    }
   ],
   "source": [
    "reanalysis_data = [\n",
    "    'air2m', 'air1000_500', 'hgt500', 'hgt1000', 'omega500',\n",
    "    'pottemp1000-500', 'pottemp1000-850', 'pr_wtr', 'shum-uwnd-700',\n",
    "    'shum-uwnd-925', 'shum-vwnd-700', 'shum-vwnd-950', 'shum700', 'shum925', \n",
    "    'skt', 'slp'\n",
    "]\n",
    "\n",
    "columns = []\n",
    "for i in range(6):\n",
    "    for item in reanalysis_data:\n",
    "        columns.append(f\"{item}_{i}\")\n",
    "\n",
    "columns.extend(['data_in', 'lat', 'lon', 'elevation', 'season_wet', 'season_dry'])\n",
    "for item in columns:\n",
    "    print(item, end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_combined = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def process(i):\n",
    "    return i * i\n",
    "    \n",
    "results = Parallel(n_jobs=2)(delayed(process)(i) for i in range(10))\n",
    "print(results)  # prints [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 2, 2, 0, 2, 0, 1, 1, 2, 0, 0, 0, 0, 0, 0, 0, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "n_cpu = cpu_count()\n",
    "def return_random_number(_):\n",
    "    return np.random.randint(3)\n",
    "with Pool(n_cpu) as p:\n",
    "    result = p.map(return_random_number, [_ for _ in range(n_cpu)])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't pickle local object 'hello.<locals>.loop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         result \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mmap(loop, [_ \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_cpu)])\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mhello\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [32]\u001b[0m, in \u001b[0;36mhello\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#     def loop(_):\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#         print(temp)\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Pool(n_cpu) \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[0;32m----> 9\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_cpu\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[0;32m~/sadow_lts/personal/yusukemh/Anaconda3/envs/tfp/lib/python3.10/multiprocessing/pool.py:364\u001b[0m, in \u001b[0;36mPool.map\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, iterable, chunksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Apply `func` to each element in `iterable`, collecting the results\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    in a list that is returned.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapstar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sadow_lts/personal/yusukemh/Anaconda3/envs/tfp/lib/python3.10/multiprocessing/pool.py:771\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value\n",
      "File \u001b[0;32m~/sadow_lts/personal/yusukemh/Anaconda3/envs/tfp/lib/python3.10/multiprocessing/pool.py:537\u001b[0m, in \u001b[0;36mPool._handle_tasks\u001b[0;34m(taskqueue, put, outqueue, pool, cache)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 537\u001b[0m     \u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    539\u001b[0m     job, idx \u001b[38;5;241m=\u001b[39m task[:\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/sadow_lts/personal/yusukemh/Anaconda3/envs/tfp/lib/python3.10/multiprocessing/connection.py:211\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_writable()\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_bytes(\u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/sadow_lts/personal/yusukemh/Anaconda3/envs/tfp/lib/python3.10/multiprocessing/reduction.py:51\u001b[0m, in \u001b[0;36mForkingPickler.dumps\u001b[0;34m(cls, obj, protocol)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdumps\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     50\u001b[0m     buf \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mgetbuffer()\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't pickle local object 'hello.<locals>.loop'"
     ]
    }
   ],
   "source": [
    "def hello():\n",
    "    temp = 19\n",
    "    n_cpu = cpu_count()\n",
    "    def loop(_):\n",
    "        return 19\n",
    "#     def loop(_):\n",
    "#         print(temp)\n",
    "    with Pool(n_cpu) as p:\n",
    "        result = p.map(loop, [_ for _ in range(n_cpu)])\n",
    "        print(result)\n",
    "hello()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3],\n",
       " [1, 2, 3]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1,2,3] for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def sqrt_func(i, j):\n",
    "    return i + j\n",
    "result = Parallel(n_jobs=-1)(delayed(sqrt_func)(i, j) for i in range(5) for j in range(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 2, 2, 3, 3, 4, 4, 5]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def func(X, Y):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    Y = scaler.transform(Y)\n",
    "    return X, Y\n",
    "    \n",
    "X = np.random.random((10,10))\n",
    "Y = np.random.random((10,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77278497, 0.3523481 , 0.44008981, 0.44132626, 0.74630071,\n",
       "        0.26041846, 0.6445427 , 0.37371003, 0.60245987, 0.76818882],\n",
       "       [0.22351023, 0.56613931, 0.97605876, 0.41191462, 0.60281716,\n",
       "        0.46592501, 0.80463901, 0.20344072, 0.43559862, 0.24372955],\n",
       "       [0.8808992 , 0.2514928 , 0.27569716, 0.2712373 , 0.77735979,\n",
       "        0.8186965 , 0.59878394, 0.9547254 , 0.93762773, 0.76537389],\n",
       "       [0.905788  , 0.72661338, 0.94508593, 0.79022218, 0.58737696,\n",
       "        0.35757129, 0.7151338 , 0.2031919 , 0.18484327, 0.54981129],\n",
       "       [0.28793288, 0.03955429, 0.77359109, 0.03490754, 0.6811471 ,\n",
       "        0.06922893, 0.6925942 , 0.05142748, 0.25145844, 0.85500327],\n",
       "       [0.60790958, 0.35081524, 0.22201916, 0.32241246, 0.82647519,\n",
       "        0.39572965, 0.42907647, 0.16655156, 0.33024554, 0.36269539],\n",
       "       [0.53498888, 0.86188524, 0.46927766, 0.4162485 , 0.32442874,\n",
       "        0.68898393, 0.58763007, 0.30337506, 0.65304007, 0.04284047],\n",
       "       [0.03659361, 0.33176209, 0.6835159 , 0.24873069, 0.37888982,\n",
       "        0.36935902, 0.78651116, 0.39081234, 0.80173842, 0.61950685],\n",
       "       [0.86996169, 0.53555408, 0.6291334 , 0.67321849, 0.29252803,\n",
       "        0.48099147, 0.68793495, 0.25598557, 0.27663934, 0.92111367],\n",
       "       [0.40232538, 0.97131511, 0.79607436, 0.72977227, 0.87078774,\n",
       "        0.15326682, 0.61031304, 0.83302589, 0.09549151, 0.67490252]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, B = func(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77278497, 0.3523481 , 0.44008981, 0.44132626, 0.74630071,\n",
       "        0.26041846, 0.6445427 , 0.37371003, 0.60245987, 0.76818882],\n",
       "       [0.22351023, 0.56613931, 0.97605876, 0.41191462, 0.60281716,\n",
       "        0.46592501, 0.80463901, 0.20344072, 0.43559862, 0.24372955],\n",
       "       [0.8808992 , 0.2514928 , 0.27569716, 0.2712373 , 0.77735979,\n",
       "        0.8186965 , 0.59878394, 0.9547254 , 0.93762773, 0.76537389],\n",
       "       [0.905788  , 0.72661338, 0.94508593, 0.79022218, 0.58737696,\n",
       "        0.35757129, 0.7151338 , 0.2031919 , 0.18484327, 0.54981129],\n",
       "       [0.28793288, 0.03955429, 0.77359109, 0.03490754, 0.6811471 ,\n",
       "        0.06922893, 0.6925942 , 0.05142748, 0.25145844, 0.85500327],\n",
       "       [0.60790958, 0.35081524, 0.22201916, 0.32241246, 0.82647519,\n",
       "        0.39572965, 0.42907647, 0.16655156, 0.33024554, 0.36269539],\n",
       "       [0.53498888, 0.86188524, 0.46927766, 0.4162485 , 0.32442874,\n",
       "        0.68898393, 0.58763007, 0.30337506, 0.65304007, 0.04284047],\n",
       "       [0.03659361, 0.33176209, 0.6835159 , 0.24873069, 0.37888982,\n",
       "        0.36935902, 0.78651116, 0.39081234, 0.80173842, 0.61950685],\n",
       "       [0.86996169, 0.53555408, 0.6291334 , 0.67321849, 0.29252803,\n",
       "        0.48099147, 0.68793495, 0.25598557, 0.27663934, 0.92111367],\n",
       "       [0.40232538, 0.97131511, 0.79607436, 0.72977227, 0.87078774,\n",
       "        0.15326682, 0.61031304, 0.83302589, 0.09549151, 0.67490252]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n",
      "\thel;\n"
     ]
    }
   ],
   "source": [
    "print('s')\n",
    "print('\\thel;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Once deleted, variables cannot be recovered. Proceed (y/[n])?  y\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# others\n",
    "from copy import deepcopy\n",
    "from xgboost import XGBRegressor\n",
    "import sherpa\n",
    "import sys\n",
    "\n",
    "# Variables from config file\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/codes/')\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES, C_SINGLE, C_INT50, C_INT100, C_GRID, C_COMMON\n",
    "\n",
    "# util\n",
    "from util import cross_val_predict_for_nn, estimate_epochs\n",
    "\n",
    "# define models\n",
    "def define_model(input_dim=20, lr=0.005):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units=256, activation='elu')(inputs)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    outputs = Dense(units=1, kernel_initializer='normal', activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def define_hetero_model_normal(input_dim=20, lr=0.0065):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units=512, activation='selu', kernel_initializer='normal')(inputs)\n",
    "    x = Dense(units=512, activation='selu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=512, activation='selu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    m = Dense(units=256, activation='selu', kernel_initializer='normal')(x)\n",
    "    m = Dense(units=10, activation='selu', kernel_initializer='normal')(m)\n",
    "    m = Dense(units=1, activation='linear', kernel_initializer='normal')(m)\n",
    "    \n",
    "    s = Dense(units=256, activation='selu', kernel_initializer='normal')(x)\n",
    "    s = Dense(units=10, activation='selu', kernel_initializer='normal')(s)\n",
    "    s = Dense(units=1, activation='linear', kernel_initializer='normal', kernel_regularizer=tf.keras.regularizers.L2(l2=100))(s)\n",
    "    \n",
    "    ms = Concatenate(axis=-1)([m, s])\n",
    "    outputs = tfp.layers.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfd.Normal(\n",
    "            loc=2.5 * t[...,0] + 0.01, scale=tf.math.softplus(0.001*t[...,1]+0.03)#this part is important\n",
    "        ),\n",
    "        convert_to_tensor_fn=lambda s: s.mean()\n",
    "    )(ms)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss=lambda y, p_y: -p_y.log_prob(y),\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def safe_nll(y, p_y):\n",
    "    epsilon=1e-5\n",
    "    return -p_y.log_prob(y + epsilon) # y = 0 yields nan\n",
    "\n",
    "def define_hetero_model_gamma(input_dim=20, lr=0.0065):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(inputs)\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    m = Dense(units=256, activation='elu', kernel_initializer='normal')(x)\n",
    "    m = Dense(units=10, activation='elu', kernel_initializer='normal')(m)\n",
    "    m = Dense(units=1, activation='linear', kernel_initializer='normal')(m)\n",
    "    \n",
    "    s = Dense(units=256, activation='elu', kernel_initializer='normal')(x)\n",
    "    s = Dense(units=10, activation='elu', kernel_initializer='normal')(s)\n",
    "    s = Dense(units=1, activation='linear', kernel_initializer='normal')(s)\n",
    "    \n",
    "    ms = Concatenate(axis=-1)([m, s])\n",
    "    outputs = tfp.layers.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfd.Gamma(\n",
    "            concentration=tf.math.softplus(t[...,0]), rate=tf.math.softplus(t[...,1])\n",
    "        ),\n",
    "        convert_to_tensor_fn=lambda d: d.mean()\n",
    "    )(ms)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        #loss=lambda y, p_y: -p_y.log_prob(y),\n",
    "        loss=safe_nll,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def run_single_experiment(\n",
    "    X, Y,\n",
    "    model_func, model_params,\n",
    "    n_trial, skn\n",
    "):\n",
    "    # first, run linear regression\n",
    "    linear_regression = LinearRegression()\n",
    "    y_pred = cross_val_predict(linear_regression, X, Y, n_jobs=-1)\n",
    "    rmse_lr = mean_squared_error(Y, y_pred, squared=False)\n",
    "    # estimate the # epochs\n",
    "    estimated_epochs = estimate_epochs(\n",
    "        X=X, Y=Y, model_func=model_func, model_params=model_params, n_iter=30\n",
    "    )\n",
    "    \n",
    "    rmses = []\n",
    "    for trial in range(n_trial):\n",
    "        y_pred = cross_val_predict_for_nn(\n",
    "            X=X, Y=Y, model_func=model_func, model_params=model_params, callback=None, batch_size=64,\n",
    "            epochs=int(estimated_epochs), early_stopping=False, verbose=False\n",
    "        )\n",
    "        rmse = mean_squared_error(Y, y_pred, squared=False)\n",
    "        rmses.append(rmse)\n",
    "    rmses = np.array(rmses)\n",
    "    m, s = np.mean(rmses), np.std(rmses)\n",
    "    return pd.DataFrame(\n",
    "        dict(\n",
    "            n_samples=X.shape[0],\n",
    "            estimated_epochs=estimated_epochs,\n",
    "            rmse_LR=rmse_lr,\n",
    "            rmse_NN_mean=m,\n",
    "            rmse_NN_std=s,\n",
    "            rel_imp=(rmse_lr - m)/rmse_lr\n",
    "        ),\n",
    "        index=[skn]\n",
    "    )\n",
    "\n",
    "def main():\n",
    "    file_name = './progress.txt'\n",
    "\n",
    "    columns = C_SINGLE\n",
    "    # load nonfilled dataset\n",
    "    df_nonfilled = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\", usecols=C_SINGLE + C_COMMON)\n",
    "    # sample a station: returned object is sorted.\n",
    "\n",
    "    df_n_data = df_nonfilled.groupby('skn').size().reset_index().rename(columns={0:\"n_data\"})\n",
    "    valid_skn = df_n_data[df_n_data['n_data'] > 750]['skn']\n",
    "\n",
    "    stats_regular = []\n",
    "    stats_normal = []\n",
    "    stats_gamma = []\n",
    "\n",
    "    for i, skn in enumerate(valid_skn):\n",
    "        with open(file_name, 'a') as f:\n",
    "            f.write(f'Running experiment on skn {skn}\\n')\n",
    "            f.write(f'{i}/{valid_skn.shape[0]}\\n')\n",
    "\n",
    "        df_station = df_nonfilled[df_nonfilled['skn'] == skn].sort_values(['year', 'month'])\n",
    "\n",
    "        X = np.array(df_station[columns])\n",
    "        Y = np.array(df_station['data_in'])\n",
    "\n",
    "        with open(file_name, 'a') as f:\n",
    "            f.write(f'\\tRunning regular NN\\n')\n",
    "\n",
    "        # regular NN\n",
    "        stats_regular.append(\n",
    "            run_single_experiment(\n",
    "                X, Y, model_func=define_model, model_params=dict(input_dim=len(columns), lr=0.0005),\n",
    "                n_trial=20, skn=skn\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with open(file_name, 'a') as f:\n",
    "            f.write(f'\\tRunning normal NN\\n')\n",
    "\n",
    "        stats_normal.append(\n",
    "            run_single_experiment(\n",
    "                X, Y, model_func=define_hetero_model_normal, model_params=dict(input_dim=len(columns), lr=0.0005),\n",
    "                n_trial=20, skn=skn\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with open(file_name, 'a') as f:\n",
    "            f.write(f'\\tRunning gamma NN\\n')\n",
    "\n",
    "        stats_gamma.append(\n",
    "            run_single_experiment(\n",
    "                X, Y, model_func=define_hetero_model_gamma, model_params=dict(input_dim=len(columns), lr=0.001),\n",
    "                n_trial=20, skn=skn\n",
    "            )\n",
    "        )\n",
    "\n",
    "    pd.concat(stats_regular).to_csv(f\"./stats_regular.csv\", index=False)\n",
    "    pd.concat(stats_normal).to_csv(f\"./stats_normal.csv\", index=False)\n",
    "    pd.concat(stats_gamma).to_csv(f\"./stats_gamma.csv\", index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-22 20:20:10.892195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/apps/software/tools/nmap/7.80/lib\n"
     ]
    }
   ],
   "source": [
    "# set the environment variable for warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# basic imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.layers import Dense, Input, Concatenate, Dropout\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# others\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "from copy import deepcopy\n",
    "import sherpa\n",
    "from xgboost import XGBRegressor\n",
    "import sys\n",
    "import time\n",
    "from scipy.stats import gamma, norm, beta\n",
    "\n",
    "# Variables from config file\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/codes/')\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES, C_SINGLE, C_INT50, C_INT100, C_GRID, C_COMMON\n",
    "\n",
    "# util\n",
    "from util import cross_val_predict_for_nn, sample_station, estimate_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station with skn: 396.0 was chosen out of all stations with more than 750 historical (non-filled) rainfall observations.\n",
      "There are 778 rainfall observations from this station.\n"
     ]
    }
   ],
   "source": [
    "columns = C_SINGLE\n",
    "# load nonfilled dataset\n",
    "df_nonfilled = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\", usecols=C_SINGLE + C_COMMON)\n",
    "# sample a station: returned object is sorted.\n",
    "df_station = sample_station(df=df_nonfilled, threshold=750, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a NN model\n",
    "def define_model(input_dim=20, lr=0.005):\n",
    "    \n",
    "    inputs = Input(shape=(input_dim,))\n",
    "        \n",
    "#     inputs_1 = tf.keras.layers.GaussianNoise(stddev=1)(inputs)\n",
    "#     inputs_2 = tf.keras.layers.GaussianNoise(stddev=0.5)(inputs)\n",
    "#     inputs_3 = tf.keras.layers.GaussianNoise(stddev=0.2)(inputs)\n",
    "    \n",
    "    x = Dense(units=256, activation='elu')(inputs)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    \n",
    "    outputs_1 = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    \n",
    "    outputs_2 = Dense(units=1, activation='linear')(x)\n",
    "    \n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    \n",
    "    outputs_3 = Dense(units=1, activation='linear')(x)\n",
    "\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=256, activation='elu')(x)\n",
    "    \n",
    "    outputs = Dense(units=1, kernel_initializer='normal', activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    rates = tf.constant([1.4, 0.9, 0.3, 0.1])\n",
    "    #rates = tf.constant([1.,1.,1.,1.])\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        loss_1 = mse(\n",
    "            tf.keras.layers.GaussianNoise(stddev=3)(y_true),\n",
    "            outputs_1\n",
    "        )\n",
    "        \n",
    "        loss_2 = mse(\n",
    "            tf.keras.layers.GaussianNoise(stddev=1)(y_true),\n",
    "            outputs_2\n",
    "        )\n",
    "        \n",
    "        loss_3 = mse(\n",
    "            tf.keras.layers.GaussianNoise(stddev=0.2)(y_true),\n",
    "            outputs_3\n",
    "        )\n",
    "        \n",
    "        loss = mse(y_true, y_pred)\n",
    "        # return rates[0] * loss_1 + rates[1] * loss_2 + rates[2] * loss_3 + rates[3] * loss\n",
    "\n",
    "        losses = tf.stack([loss_1, loss_2, loss_3, loss], axis=0)\n",
    "        # final_loss = tf.math.multiply(rates, [loss_1, loss_2, loss_3, loss])\n",
    "        final_loss = tf.math.multiply(rates, losses)\n",
    "        return tf.reduce_sum(final_loss, axis=-1)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss=custom_loss,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const_52:0' shape=(4,) dtype=float32>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant(np.arange(3 + 1, 0, -1).astype('float32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a NN model\n",
    "def define_model(input_dim=20, lr=0.005, n_stacks=3):\n",
    "    \n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = tf.keras.layers.Lambda(lambda x: x)(inputs)\n",
    "     \n",
    "    intermediate_outputs = []\n",
    "    for stack in range(n_stacks):\n",
    "        x = Dense(units=256, activation='elu')(x)\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "        x = Dense(units=256, activation='elu')(x)\n",
    "        _outputs = Dense(units=1, activation='linear')(x)\n",
    "        intermediate_outputs.append(_outputs)\n",
    "\n",
    "    # outputs = tf.keras.layers.Lambda(lambda x: x)(x)\n",
    "    outputs = Dense(units=1, kernel_initializer='normal', activation='linear')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    rates = tf.constant(np.arange(n_stacks + 1).astype('float32'))\n",
    "    std = tf.constant(np.log(np.arange(n_stacks + 1, 0, -1).astype('float32')))\n",
    "    \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        \n",
    "        mses = []\n",
    "        for i in range(n_stacks):\n",
    "            mses.append(mse(\n",
    "                tf.keras.layers.GaussianNoise(stddev=std[i])(y_true),\n",
    "                intermediate_outputs[i]\n",
    "            ))\n",
    "        mses.append(mse(y_true, y_pred))\n",
    "\n",
    "        losses = tf.stack(mses, axis=0)\n",
    "        final_loss = tf.math.multiply(rates, mses)\n",
    "        return tf.reduce_sum(final_loss, axis=-1)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss=custom_loss,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean number of epochs: 14.0\n",
      "Std: 0.000\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "# model = define_model(input_dim=len(columns), lr=0.005)\n",
    "model_params={\"input_dim\": len(columns), \"lr\": 0.005, \"n_stacks\": 10}\n",
    "estimated_epochs = estimate_epochs(\n",
    "    X=X, Y=Y,\n",
    "    model_func=define_model,\n",
    "    model_params=model_params,\n",
    "    n_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: single observation, normal NN\n",
      "RMSE with NN: 1.885\n",
      "RMSE with LR: 1.535\n"
     ]
    }
   ],
   "source": [
    "# now fix the epoch, remove early stopping, and run cross_val_predict\n",
    "yhat_nn = cross_val_predict_for_nn(# implemented in util.py\n",
    "    model_func=define_model,\n",
    "    model_params=model_params,\n",
    "    X=X, Y=Y,\n",
    "    callback=None,\n",
    "    batch_size=64,\n",
    "    epochs=int(estimated_epochs),\n",
    "    early_stopping=False,\n",
    "    verbose=False\n",
    ")\n",
    "rmse_nn = mean_squared_error(Y, yhat_nn, squared=False)\n",
    "linear_regression = LinearRegression()\n",
    "yhat_lr = cross_val_predict(linear_regression, X, Y, n_jobs=-1)\n",
    "rmse_lr = mean_squared_error(Y, yhat_lr, squared=False)\n",
    "\n",
    "print(\"Result: single observation, normal NN\")\n",
    "print(\"RMSE with NN: {:.3f}\".format(rmse_nn))\n",
    "print(\"RMSE with LR: {:.3f}\".format(rmse_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station with skn: 396.0 was chosen out of all stations with more than 750 historical (non-filled) rainfall observations.\n",
      "There are 778 rainfall observations from this station.\n"
     ]
    }
   ],
   "source": [
    "columns = C_SINGLE\n",
    "# load nonfilled dataset\n",
    "df_nonfilled = pd.read_csv(f\"{BASE_DIR}/nonfilled_dataset.csv\", usecols=C_SINGLE + C_COMMON)\n",
    "# sample a station: returned object is sorted.\n",
    "df_station = sample_station(df=df_nonfilled, threshold=750, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_hetero_model_gamma(input_dim=20, lr=0.0065):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(inputs)\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=512, activation='elu', kernel_initializer='normal')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    \n",
    "    m = Dense(units=256, activation='elu', kernel_initializer='normal')(x)\n",
    "    m = Dense(units=10, activation='elu', kernel_initializer='normal')(m)\n",
    "    m = Dense(units=1, activation='linear', kernel_initializer='normal')(m)\n",
    "    \n",
    "    s = Dense(units=256, activation='elu', kernel_initializer='normal')(x)\n",
    "    s = Dense(units=10, activation='elu', kernel_initializer='normal')(s)\n",
    "    s = Dense(units=1, activation='linear', kernel_initializer='normal')(s)\n",
    "    \n",
    "    ms = Concatenate(axis=-1)([m, s])\n",
    "    outputs = tfp.layers.DistributionLambda(\n",
    "        make_distribution_fn=lambda t: tfd.Gamma(\n",
    "            concentration=tf.math.softplus(t[...,0]), rate=tf.math.softplus(t[...,1])\n",
    "        ),\n",
    "        convert_to_tensor_fn=lambda d: d.mean()\n",
    "    )(ms)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    epsilon=1e-5 # for loss function\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss=lambda y, p_y: -p_y.log_prob(y + epsilon),\n",
    "        # loss=safe_nll,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean number of epochs: 13.2\n",
      "Std: 3.400\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "# Y = np.clip(Y, a_min=1e-10, a_max=None)\n",
    "\n",
    "\n",
    "model_params = dict(\n",
    "    input_dim=len(columns),\n",
    "    lr=0.001\n",
    ")\n",
    "batch_size = 64\n",
    "\n",
    "estimated_epochs = estimate_epochs(\n",
    "    X=X, Y=Y,\n",
    "    model_func=define_hetero_model_gamma,\n",
    "    model_params=model_params,\n",
    "    n_iter=10,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with NN: 1.456\n"
     ]
    }
   ],
   "source": [
    "# now fix the epoch, remove early stopping, and run cross_val_predict\n",
    "epochs = int(estimated_epochs)\n",
    "n_ensamble = 10\n",
    "y_preds = []\n",
    "for _ in range(n_ensamble):\n",
    "    yhat_nn = cross_val_predict_for_nn(# implemented in util.py\n",
    "        model_func=define_hetero_model_gamma,\n",
    "        model_params=model_params,\n",
    "        X=X, Y=Y,\n",
    "        callback=None,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        early_stopping=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    y_preds.append(yhat_nn)\n",
    "    print(f\"{_+1}/{n_ensamble}\", end='\\r')\n",
    "mean_pred = np.mean(y_preds, axis=0)\n",
    "rmse_nn = mean_squared_error(Y, mean_pred, squared=False)\n",
    "print(\"RMSE with NN: {:.3f}\".format(rmse_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean number of epochs: 33.7\n",
      "Std: 11.411\n"
     ]
    }
   ],
   "source": [
    "X = np.array(df_station[columns])\n",
    "Y = np.array(df_station['data_in'])\n",
    "# Y = np.clip(Y, a_min=1e-10, a_max=None)\n",
    "\n",
    "\n",
    "model_params = dict(\n",
    "    input_dim=len(columns),\n",
    "    lr=0.001\n",
    ")\n",
    "batch_size = 64\n",
    "\n",
    "estimated_epochs = estimate_epochs(\n",
    "    X=X, Y=Y,\n",
    "    model_func=define_hetero_model_normal_rev,\n",
    "    model_params=model_params,\n",
    "    n_iter=10,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE with NN: 1.474\n"
     ]
    }
   ],
   "source": [
    "# now fix the epoch, remove early stopping, and run cross_val_predict\n",
    "epochs = int(estimated_epochs)\n",
    "n_ensamble = 10\n",
    "y_preds = []\n",
    "for _ in range(n_ensamble):\n",
    "    yhat_nn = cross_val_predict_for_nn(# implemented in util.py\n",
    "        model_func=define_hetero_model_normal_rev,\n",
    "        model_params=model_params,\n",
    "        X=X, Y=Y,\n",
    "        callback=None,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        early_stopping=False,\n",
    "        verbose=False\n",
    "    )\n",
    "    y_preds.append(yhat_nn)\n",
    "    print(f\"{_+1}/{n_ensamble}\", end='\\r')\n",
    "mean_pred = np.mean(y_preds, axis=0)\n",
    "rmse_nn = mean_squared_error(Y, mean_pred, squared=False)\n",
    "print(\"RMSE with NN: {:.3f}\".format(rmse_nn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfp",
   "language": "python",
   "name": "tfp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
