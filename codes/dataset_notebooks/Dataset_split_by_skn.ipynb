{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#others\n",
    "from xgboost import XGBRegressor\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.mpl.ticker as cticker\n",
    "import time\n",
    "import xarray as xr\n",
    "import sherpa\n",
    "import time\n",
    "from scipy.spatial import Delaunay\n",
    "from scipy import interpolate\n",
    "from copy import deepcopy\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Variables from config file\n",
    "from config import BASE_DIR, FILE_NAMES, LABELS, ATTRIBUTES, BEST_MODEL_COLUMNS, ISLAND_RANGES\n",
    "from math import pi as PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air2m air1000_500 hgt500 hgt1000 omega500 pottemp1000-500 pottemp1000-850 pr_wtr shum-uwnd-700 shum-uwnd-925 shum-vwnd-700 shum-vwnd-950 shum700 shum925 skt slp season_wet elevation lat lon "
     ]
    }
   ],
   "source": [
    "columns = deepcopy(LABELS)\n",
    "columns.extend([\"season_wet\", \"elevation\", \"lat\", \"lon\"])\n",
    "for item in columns:\n",
    "    print(item, end=' ')\n",
    "\n",
    "# load datasets\n",
    "df_train = pd.read_csv(f\"{BASE_DIR}/train.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_valid = pd.read_csv(f\"{BASE_DIR}/valid.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_test = pd.read_csv(f\"{BASE_DIR}/test.csv\", usecols=columns + ['year', 'month', 'skn', 'data_in'])\n",
    "df_combined = pd.concat([df_train, df_valid, df_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the stations by the number of samples available\n",
    "threshold = 400\n",
    "df_split = df_combined.groupby('skn').size().reset_index().rename(columns={0: \"n_samples\"})\n",
    "df_split['class'] = df_split.apply(lambda row: 0 if row['n_samples'] < threshold else 1, axis=1)\n",
    "df_combined = df_combined.merge(right=df_split, left_on=\"skn\", right_on='skn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 162 156\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = (0, 0, 0)\n",
    "df_train = []\n",
    "df_valid = []\n",
    "df_test = []\n",
    "np.random.seed(40)\n",
    "for name, group in df_combined[df_combined['class']==1].groupby(by=[\"year\", \"month\"]):\n",
    "    # print(name, len(group))\n",
    "    label = np.random.choice(a=[\"train\", \"valid\", \"test\"], size=1, replace=True, p=[0.6, 0.2, 0.2])\n",
    "    if label == \"train\":\n",
    "        train += len(group)\n",
    "        df_train.append(group)\n",
    "    elif label == \"valid\":\n",
    "        valid += len(group)\n",
    "        df_valid.append(group)\n",
    "    else:\n",
    "        test += len(group)\n",
    "        df_test.append(group)\n",
    "print(len(df_train), len(df_valid), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((461374, 26), (158908, 26), (159178, 26))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.concat(df_train).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_valid = pd.concat(df_valid).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_test = pd.concat(df_test).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(f\"{BASE_DIR}/split_on_n_samples/high/train.csv\", index=False)\n",
    "df_valid.to_csv(f\"{BASE_DIR}/split_on_n_samples/high/valid.csv\", index=False)\n",
    "df_test.to_csv(f\"{BASE_DIR}/split_on_n_samples/high/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478 149 153\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((52789, 26), (16343, 26), (16969, 26))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stations with lower number of samples\n",
    "train, valid, test = (0, 0, 0)\n",
    "df_train = []\n",
    "df_valid = []\n",
    "df_test = []\n",
    "np.random.seed(5)\n",
    "for name, group in df_combined[df_combined['class']==0].groupby(by=[\"year\", \"month\"]):\n",
    "    # print(name, len(group))\n",
    "    label = np.random.choice(a=[\"train\", \"valid\", \"test\"], size=1, replace=True, p=[0.6, 0.2, 0.2])\n",
    "    if label == \"train\":\n",
    "        train += len(group)\n",
    "        df_train.append(group)\n",
    "    elif label == \"valid\":\n",
    "        valid += len(group)\n",
    "        df_valid.append(group)\n",
    "    else:\n",
    "        test += len(group)\n",
    "        df_test.append(group)\n",
    "print(len(df_train), len(df_valid), len(df_test))\n",
    "\n",
    "df_train = pd.concat(df_train).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_valid = pd.concat(df_valid).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_test = pd.concat(df_test).reset_index().drop(labels=[\"index\"], axis=1)\n",
    "df_train.shape, df_valid.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(f\"{BASE_DIR}/split_on_n_samples/low/train.csv\", index=False)\n",
    "df_valid.to_csv(f\"{BASE_DIR}/split_on_n_samples/low/valid.csv\", index=False)\n",
    "df_test.to_csv(f\"{BASE_DIR}/split_on_n_samples/low/test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
