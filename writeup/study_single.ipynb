{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1bcbf107-37c1-40ce-8a5a-4b57bb72263a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# others\n",
    "import multiprocessing as mp\n",
    "from xgboost import XGBRegressor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# config\n",
    "import sys\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/writeup')\n",
    "from config import C_COMMON, C_SINGLE, C_GRID, FILENAME\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4c6131f5-0cb8-4b5f-a0bd-099cb8251487",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILENAME, usecols=C_COMMON + C_SINGLE)\n",
    "columns = C_SINGLE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ca0124-c994-45d5-8dca-f00358c10504",
   "metadata": {},
   "source": [
    "# Linear Regression vs XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "10587d28-c7ca-4f09-a436-077ea7eb2b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_predict(df: pd.DataFrame, model, skn: int, columns: list, verbose=False):\n",
    "    \"\"\"\n",
    "    Runs cross_val_predict for a single skn, using XGB or LinearRegression.\n",
    "    The same functionality as sklearn.model_selection.cross_val_predict,\n",
    "    except the split is not exactly 1/5 (thus pre-determined by preprocessing).\n",
    "    This is because the split has to be made in the way it won't separate samples in the same month into different folds.\n",
    "        Args:\n",
    "            :param df: dataset for evaluation. Must contain 'fold' column that specifies assignment of each sample to the folds.\n",
    "            :param model: one of [sklearn.linear_models.LinearRegression, xgboost.XGBRegressor]\n",
    "            :param skn: identifier for stations\n",
    "            :param columns: list of str indicating which columns to use as input data for the model\n",
    "    \"\"\"\n",
    "    assert 'fold' in df.columns, \"Must contain a column 'fold' to specify assignment of samples to the folds.\"\n",
    "    n_folds = len(df['fold'].unique())\n",
    "    dfs = [] # list of dfs containing result for each fold\n",
    "    \n",
    "    iterator = tqdm(range(n_folds)) if verbose else range(n_folds)\n",
    "    \n",
    "    for fold in iterator:\n",
    "        df_train = df.query(f'(fold != {fold}) & (skn == {skn})')\n",
    "        df_test = df.query(f'(fold == {fold}) & (skn == {skn})')\n",
    "        \n",
    "        x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "        y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "        \n",
    "        model.fit(x_train, y_train)\n",
    "        yhat = model.predict(x_test)\n",
    "        \n",
    "        _df = pd.DataFrame(\n",
    "            {\n",
    "                'skn' : df_test['skn'].values,\n",
    "                'year': df_test['year'].values,\n",
    "                'month': df_test['month'].values,\n",
    "                'observed': df_test['data_in'].values,\n",
    "                'prediction': yhat,\n",
    "            }\n",
    "        )\n",
    "        dfs.append(_df)\n",
    "        \n",
    "    return pd.concat(dfs)\n",
    "\n",
    "def parallelize(func, args, n_jobs=-1):\n",
    "    \"\"\"\n",
    "    :param args: iterable. list of arguments for the function\n",
    "    \"\"\"\n",
    "    if n_jobs == -1:\n",
    "        pool = mp.Pool(mp.cpu_count())\n",
    "    else:\n",
    "        pool = mp.Pool(n_jobs)\n",
    "    result_objects = [pool.apply_async(func, args=_args) for _args in args]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return [r.get() for r in result_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2bd9035f-8737-4dd4-a5a7-47f6bc63e3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.004229222338949"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "df_result = cross_val_predict(df, linear_regression, skn=79, columns=columns, verbose=False)\n",
    "mean_squared_error(df_result['observed'], df_result['prediction'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "17142742-19d9-49f3-9fbe-01030a86d01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.792404950412038"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost = XGBRegressor()\n",
    "df_result = cross_val_predict(df, xgboost, skn=79, columns=columns, verbose=True)\n",
    "mean_squared_error(df_result['observed'], df_result['prediction'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "573e8982-9c9a-4f36-a037-d71e6ef678a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment on 54.0\n",
      "RMSE using LR: 5.18\n",
      "RMSE using XGB: 5.60\n",
      "=================================================================================\n",
      "experiment on 79.0\n",
      "RMSE using LR: 6.00\n",
      "RMSE using XGB: 6.76\n",
      "=================================================================================\n",
      "experiment on 338.0\n",
      "RMSE using LR: 4.21\n",
      "RMSE using XGB: 4.62\n",
      "=================================================================================\n",
      "experiment on 250.0\n",
      "RMSE using LR: 2.05\n",
      "RMSE using XGB: 2.22\n",
      "=================================================================================\n",
      "experiment on 267.0\n",
      "RMSE using LR: 2.22\n",
      "RMSE using XGB: 2.33\n",
      "=================================================================================\n",
      "experiment on 296.1\n",
      "RMSE using LR: 1.51\n",
      "RMSE using XGB: 1.62\n",
      "=================================================================================\n",
      "experiment on 311.0\n",
      "RMSE using LR: 1.36\n",
      "RMSE using XGB: 1.43\n",
      "=================================================================================\n",
      "experiment on 396.0\n",
      "RMSE using LR: 1.55\n",
      "RMSE using XGB: 1.56\n",
      "=================================================================================\n",
      "experiment on 400.0\n",
      "RMSE using LR: 1.62\n",
      "RMSE using XGB: 1.67\n",
      "=================================================================================\n",
      "experiment on 406.0\n",
      "RMSE using LR: 1.78\n",
      "RMSE using XGB: 1.83\n",
      "=================================================================================\n",
      "experiment on 410.0\n",
      "RMSE using LR: 1.71\n",
      "RMSE using XGB: 1.80\n",
      "=================================================================================\n",
      "experiment on 485.0\n",
      "RMSE using LR: 2.20\n",
      "RMSE using XGB: 2.27\n",
      "=================================================================================\n",
      "experiment on 703.0\n",
      "RMSE using LR: 1.95\n",
      "RMSE using XGB: 2.02\n",
      "=================================================================================\n",
      "experiment on 718.0\n",
      "RMSE using LR: 5.10\n",
      "RMSE using XGB: 5.43\n",
      "=================================================================================\n",
      "experiment on 770.0\n",
      "RMSE using LR: 2.19\n",
      "RMSE using XGB: 2.39\n",
      "=================================================================================\n",
      "experiment on 783.0\n",
      "RMSE using LR: 4.40\n",
      "RMSE using XGB: 4.78\n",
      "=================================================================================\n",
      "experiment on 784.0\n",
      "RMSE using LR: 5.40\n",
      "RMSE using XGB: 5.83\n",
      "=================================================================================\n",
      "experiment on 965.0\n",
      "RMSE using LR: 1.95\n",
      "RMSE using XGB: 1.99\n",
      "=================================================================================\n",
      "experiment on 1075.0\n",
      "RMSE using LR: 4.09\n",
      "RMSE using XGB: 4.26\n",
      "=================================================================================\n",
      "experiment on 1117.0\n",
      "RMSE using LR: 3.68\n",
      "RMSE using XGB: 3.90\n",
      "=================================================================================\n",
      "experiment on 1134.0\n",
      "RMSE using LR: 3.64\n",
      "RMSE using XGB: 3.69\n",
      "=================================================================================\n",
      "experiment on 87.0\n",
      "RMSE using LR: 5.42\n",
      "RMSE using XGB: 5.46\n",
      "=================================================================================\n",
      "experiment on 702.7\n",
      "RMSE using LR: 1.80\n",
      "RMSE using XGB: 1.94\n",
      "=================================================================================\n",
      "experiment on 1020.1\n",
      "RMSE using LR: 2.55\n",
      "RMSE using XGB: 2.69\n",
      "=================================================================================\n"
     ]
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "params = {'n_estimators': 260, 'learning_rate': 0.1, 'max_depth': 3, 'early_stopping_rounds': 8, 'verbosity': 0}\n",
    "xgboost = XGBRegressor(**params)\n",
    "\n",
    "for skn in df['skn'].unique():\n",
    "    print(f\"experiment on {skn}\")\n",
    "    df_result = cross_val_predict(df, linear_regression, skn=skn, columns=columns, verbose=False)\n",
    "    rmse = mean_squared_error(df_result['observed'], df_result['prediction'], squared=False)\n",
    "    print(f'RMSE using LR: {rmse:.2f}')\n",
    "    df_result = cross_val_predict(df, xgboost, skn=skn, columns=columns, verbose=False)\n",
    "    rmse = mean_squared_error(df_result['observed'], df_result['prediction'], squared=False)\n",
    "    print(f'RMSE using XGB: {rmse:3.2f}')\n",
    "    print('=================================================================================')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838ee4a3-6f10-449e-abf0-8d69740e41ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
