{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a792aa-d77a-4d2e-880c-1bb4b0f394aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-30 01:27:43.845979: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import sherpa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/writeup')\n",
    "from config import C_COMMON, C_GRID, C_SINGLE, FILENAME\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93b5f63-e3b0-481b-ba55-7c2d0850b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_inner_fold(df, n_folds=5):\n",
    "    # assign fold for each sample\n",
    "    df_len_by_month = pd.DataFrame(df.groupby(by=['year', 'month']).size()).reset_index().rename({0: \"len\"}, axis=1)\n",
    "    df_len_by_month = df_len_by_month.sort_values(['year', 'month'])\n",
    "    df_len_by_month['cumsum'] = df_len_by_month['len'].cumsum()\n",
    "    n_samples_total = df_len_by_month['cumsum'].iloc[-1]\n",
    "    n_samples_per_fold = np.ceil(n_samples_total / n_folds)\n",
    "    df_len_by_month['inner_fold'] = df_len_by_month.apply(lambda row: int(row['cumsum'] / n_samples_per_fold), axis=1)\n",
    "    df_w_fold = pd.merge(left=df, right=df_len_by_month, left_on=['year', 'month'], right_on=['year', 'month'])\n",
    "    \n",
    "    return df_w_fold\n",
    "\n",
    "def load_data(columns, filename):\n",
    "    \"\"\"\n",
    "    Loads dataset and splits into train and test.\n",
    "    It also splits training dataset into 5 folds as a column named 'inner_fold'\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(filename, usecols=C_COMMON + columns).sort_values(['year', 'month'])\n",
    "    df_train = df.query('fold != 4')\n",
    "    df_test = df.query('fold == 4')\n",
    "    assert (sorted(df_train['skn'].unique()) == sorted(df_test['skn'].unique()))\n",
    "    \n",
    "    df_train = assign_inner_fold(df_train)\n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96805134-9402-4339-a133-0f3cc5449b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = C_SINGLE\n",
    "df, _ = load_data(columns, FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5849cbca-b469-435e-9f52-945ee06a573d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(\n",
    "    input_dim=20,\n",
    "    n_units=512,\n",
    "    activation='selu',#selu\n",
    "    learning_rate=0.00001,\n",
    "    loss='mse',\n",
    "    batch_size=64\n",
    "):\n",
    "    inputs = Input(shape=(input_dim))\n",
    "    # x = Dense(units=n_units, activation=activation, kernel_regularizer='l1')(inputs)\n",
    "    x = Dense(units=n_units, activation=activation)(inputs)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)# serves as regularization\n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08f471a2-2aa3-4c88-8ea9-a52812d6e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, model_func, params, columns):\n",
    "        self.model_func = model_func\n",
    "        self.params = params\n",
    "        self.columns = columns\n",
    "        pass\n",
    "    \n",
    "    def cross_val_predict(self, df, skn, verbose=0, n_folds=5):\n",
    "        assert 'inner_fold' in df.columns, 'define fold with column name \"inner_fold\"'\n",
    "        df_station = df[df['skn'] == skn]\n",
    "        \n",
    "        list_ytrue = []\n",
    "        list_ypred = []\n",
    "        for k in range(n_folds):\n",
    "            # split the dataset\n",
    "            df_train = df_station[df_station['inner_fold'] != k]\n",
    "            df_test = df_station[df_station['inner_fold'] == k]\n",
    "            \n",
    "            # convert to numpy\n",
    "            x_train, x_test = np.array(df_train[self.columns]), np.array(df_test[self.columns])\n",
    "            y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "            \n",
    "            # scale the input and output\n",
    "            x_train, x_test = self.transform_x(x_train, x_test)\n",
    "            y_train, y_test, y_scaler = self.transform_y(y_train, y_test)\n",
    "            \n",
    "            # train the model\n",
    "            self.train(x_train, y_train, verbose=0, retrain_full=False) # to speed up computation for hyperparaemter tuning\n",
    "            \n",
    "            # make prediction and scale\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            y_pred = self.inverse_transform_y(y_pred, y_scaler)\n",
    "            # scale y_test\n",
    "            y_test = self.inverse_transform_y(y_test, y_scaler)\n",
    "            \n",
    "            # keep the record\n",
    "            list_ytrue.extend(y_test)\n",
    "            list_ypred.extend(y_pred)\n",
    "        \n",
    "        # calculate the loss and return\n",
    "        return {\n",
    "            \"mse\": mean_squared_error(list_ytrue, list_ypred, squared=False),\n",
    "            \"mae\": mean_absolute_error(list_ytrue, list_ypred)\n",
    "        }\n",
    "    def transform_x(self, x_train, x_test):\n",
    "        scaler = MinMaxScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        return x_train, x_test\n",
    "    \n",
    "    def transform_y(self, y_train, y_test):\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        y_train = np.log(y_train + 1.)\n",
    "        y_test = np.log(y_test + 1.)\n",
    "\n",
    "        y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        return y_train, y_test, scaler\n",
    "    \n",
    "    def inverse_transform_y(self, y, scaler):\n",
    "        y = scaler.inverse_transform(y)\n",
    "        y = np.power(np.e, y) - 1\n",
    "        return y\n",
    "    \n",
    "    def train(self, x, y, verbose=0, retrain_full=False):\n",
    "        # build the model\n",
    "        self.model, batch_size = self.model_func(**self.params)\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.95,\n",
    "                patience=10\n",
    "            )\n",
    "        ]\n",
    "        history = self.model.fit(\n",
    "            x, y,\n",
    "            epochs=500,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if retrain_full:\n",
    "            epochs = len(history.history['loss'])\n",
    "            # rebuild the model\n",
    "            self.model, batch_size = self.model_func(**params)\n",
    "            callbacks = [EarlyStopping(monitor='loss', min_delta=0, patience=1e3, restore_best_weights=True)]\n",
    "            history = self.model.fit(\n",
    "                x, y,\n",
    "                epochs=epochs,\n",
    "                validation_split=0,\n",
    "                callbacks=callbacks,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        return history   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3eefa-df51-4862-bba8-49c11ae27b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "skn = 54\n",
    "model = NeuralNetwork(\n",
    "    model_func=define_model,\n",
    "    params = {\n",
    "        'input_dim': 16,\n",
    "        'n_units': 362,\n",
    "        'learning_rate': 0.000695,\n",
    "        'loss': 'mse',\n",
    "        'batch_size': 128\n",
    "    },\n",
    "    columns=columns\n",
    ")\n",
    "ret = model.cross_val_predict(df, skn)\n",
    "print(skn, ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822975e0-20d9-436b-adf8-a0ed49bb527c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
