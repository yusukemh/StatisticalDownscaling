{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cef254e-2af1-4129-a6b0-bbdd6687d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sherpa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/writeup')\n",
    "from config import C_COMMON, C_GRID, C_SINGLE, FILENAME\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc39d4b4-02b6-4ffe-b9f4-de9f899deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(FILENAME, usecols=C_COMMON + C_SINGLE).sort_values(['year', 'month'])\n",
    "columns = C_SINGLE\n",
    "\n",
    "# we use the last 1/5 data as the heldout clean dataset. We do not use this fold for any use except for just reporting the result.\n",
    "df_train_outer = df.query('fold != 4')\n",
    "df_test_outer = df.query('fold == 4')\n",
    "assert (sorted(df_test_outer['skn'].unique()) == sorted(df_train_outer['skn'].unique()))\n",
    "\n",
    "# split the trainig data into 5 folds for inner cross validation\n",
    "def assign_inner_fold(df, n_folds=5):\n",
    "    # assign fold for each sample\n",
    "    df_len_by_month = pd.DataFrame(df.groupby(by=['year', 'month']).size()).reset_index().rename({0: \"len\"}, axis=1)\n",
    "    df_len_by_month = df_len_by_month.sort_values(['year', 'month'])\n",
    "    df_len_by_month['cumsum'] = df_len_by_month['len'].cumsum()\n",
    "    n_samples_total = df_len_by_month['cumsum'].iloc[-1]\n",
    "    n_samples_per_fold = np.ceil(n_samples_total / n_folds)\n",
    "    \n",
    "    df_len_by_month['inner_fold'] = df_len_by_month.apply(lambda row: int(row['cumsum'] / n_samples_per_fold), axis=1)\n",
    "    \n",
    "    df_w_fold = pd.merge(left=df, right=df_len_by_month, left_on=['year', 'month'], right_on=['year', 'month'])\n",
    "    \n",
    "    return df_w_fold\n",
    "\n",
    "df_inner_split = assign_inner_fold(df_train_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "53e98719-d62d-4c2e-addc-4d170b07565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(\n",
    "    input_dim=20, \n",
    "    lr=0.005, \n",
    "    activation='relu',\n",
    "    n_units=512,\n",
    "    n_layers=5,\n",
    "    dropout=0.5\n",
    "):\n",
    "    inputs = Input(shape=(input_dim,))\n",
    "    x = Dense(units=n_units, activation=activation)(inputs)\n",
    "    \n",
    "    for i in range(n_layers - 1):\n",
    "        if dropout:\n",
    "            x = Dropout(rate=dropout)(x)\n",
    "        x = Dense(units=n_units, activation=activation)(x)\n",
    "    outputs = Dense(units=1, kernel_initializer=tf.keras.initializers.HeNormal, activation='softplus')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=lr),\n",
    "        loss='mse',\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c52c61ab-7678-4ed8-beea-ec6aec33b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_(\n",
    "    input_dim=20\n",
    "):\n",
    "    inputs = Input(shape=(input_dim))\n",
    "    x = Dense(units=512, activation='relu')(inputs)\n",
    "    x = Dense(units=512, activation='relu')(x)\n",
    "    x = Dense(units=512, activation='relu')(x)\n",
    "    x = Dense(units=512, activation='relu')(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=216, activation='relu')(x)\n",
    "    outputs = Dense(units=1, activation='softplus')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=0.005),\n",
    "        loss='mse',\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1fa6020f-28fe-4cac-b0e9-8210730cc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "2bba97a3-8478-46aa-a8b0-1b65c62a569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "idx = 0\n",
    "skn = df_inner_split['skn'].unique()[0]\n",
    "df_train = df_inner_split.query(f'(fold == {fold}) & skn == {skn}')\n",
    "df_test = df_inner_split.query(f'fold != {fold} & skn == {skn}')\n",
    "x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "\n",
    "x_scaler = MinMaxScaler()\n",
    "# y_scaler = MinMaxScaler()\n",
    "\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "\n",
    "# y_train = y_scaler.fit_transform(y_train.reshape(-1,1))\n",
    "# y_test = y_scaler.transform(y_test.reshape(-1,1))\n",
    "\n",
    "y_train_scaled = np.log(y_train + 5)\n",
    "y_test_scaled = np.log(y_test + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "a979d19f-0f4b-4edc-b8c3-c910ce45dd66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "2/2 [==============================] - 1s 161ms/step - loss: 10.3987 - root_mean_squared_error: 3.1096 - val_loss: 2.5299 - val_root_mean_squared_error: 1.5906\n",
      "Epoch 2/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.9542 - root_mean_squared_error: 1.7183 - val_loss: 3.3159 - val_root_mean_squared_error: 1.8210\n",
      "Epoch 3/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 3.4152 - root_mean_squared_error: 1.8480 - val_loss: 2.7983 - val_root_mean_squared_error: 1.6728\n",
      "Epoch 4/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 2.6659 - root_mean_squared_error: 1.6311 - val_loss: 0.3347 - val_root_mean_squared_error: 0.5785\n",
      "Epoch 5/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 1.7416 - root_mean_squared_error: 1.2582 - val_loss: 0.1838 - val_root_mean_squared_error: 0.4288\n",
      "Epoch 6/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.5359 - root_mean_squared_error: 0.7258 - val_loss: 1.5924 - val_root_mean_squared_error: 1.2619\n",
      "Epoch 7/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 1.8494 - root_mean_squared_error: 1.3596 - val_loss: 2.1110 - val_root_mean_squared_error: 1.4529\n",
      "Epoch 8/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 2.2256 - root_mean_squared_error: 1.4918 - val_loss: 1.9843 - val_root_mean_squared_error: 1.4086\n",
      "Epoch 9/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.9678 - root_mean_squared_error: 1.4028 - val_loss: 1.3745 - val_root_mean_squared_error: 1.1724\n",
      "Epoch 10/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 1.2532 - root_mean_squared_error: 1.1181 - val_loss: 0.3390 - val_root_mean_squared_error: 0.5823\n",
      "Epoch 11/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.3917 - root_mean_squared_error: 0.6244 - val_loss: 0.8297 - val_root_mean_squared_error: 0.9109\n",
      "Epoch 12/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 1.0219 - root_mean_squared_error: 1.0109 - val_loss: 0.2993 - val_root_mean_squared_error: 0.5471\n",
      "Epoch 13/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3984 - root_mean_squared_error: 0.6300 - val_loss: 0.2808 - val_root_mean_squared_error: 0.5299\n",
      "Epoch 14/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3601 - root_mean_squared_error: 0.5974 - val_loss: 0.5434 - val_root_mean_squared_error: 0.7372\n",
      "Epoch 15/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.5688 - root_mean_squared_error: 0.7542 - val_loss: 0.4521 - val_root_mean_squared_error: 0.6724\n",
      "Epoch 16/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.4130 - root_mean_squared_error: 0.6425 - val_loss: 0.1961 - val_root_mean_squared_error: 0.4428\n",
      "Epoch 17/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2364 - root_mean_squared_error: 0.4858 - val_loss: 0.1204 - val_root_mean_squared_error: 0.3469\n",
      "Epoch 18/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.2140 - root_mean_squared_error: 0.4625 - val_loss: 0.1859 - val_root_mean_squared_error: 0.4312\n",
      "Epoch 19/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3192 - root_mean_squared_error: 0.5650 - val_loss: 0.1083 - val_root_mean_squared_error: 0.3290\n",
      "Epoch 20/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.2319 - root_mean_squared_error: 0.4815 - val_loss: 0.1740 - val_root_mean_squared_error: 0.4171\n",
      "Epoch 21/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.2378 - root_mean_squared_error: 0.4876 - val_loss: 0.2756 - val_root_mean_squared_error: 0.5249\n",
      "Epoch 22/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.3191 - root_mean_squared_error: 0.5648 - val_loss: 0.2394 - val_root_mean_squared_error: 0.4892\n",
      "Epoch 23/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2459 - root_mean_squared_error: 0.4959 - val_loss: 0.1289 - val_root_mean_squared_error: 0.3590\n",
      "Epoch 24/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.2006 - root_mean_squared_error: 0.4479 - val_loss: 0.0976 - val_root_mean_squared_error: 0.3124\n",
      "Epoch 25/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2472 - root_mean_squared_error: 0.4972 - val_loss: 0.0955 - val_root_mean_squared_error: 0.3090\n",
      "Epoch 26/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1565 - root_mean_squared_error: 0.3955 - val_loss: 0.1075 - val_root_mean_squared_error: 0.3278\n",
      "Epoch 27/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1626 - root_mean_squared_error: 0.4028 - val_loss: 0.1557 - val_root_mean_squared_error: 0.3946\n",
      "Epoch 28/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.2040 - root_mean_squared_error: 0.4513 - val_loss: 0.1636 - val_root_mean_squared_error: 0.4045\n",
      "Epoch 29/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1705 - root_mean_squared_error: 0.4129 - val_loss: 0.1149 - val_root_mean_squared_error: 0.3390\n",
      "Epoch 30/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1742 - root_mean_squared_error: 0.4171 - val_loss: 0.0911 - val_root_mean_squared_error: 0.3018\n",
      "Epoch 31/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1963 - root_mean_squared_error: 0.4431 - val_loss: 0.0910 - val_root_mean_squared_error: 0.3017\n",
      "Epoch 32/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1551 - root_mean_squared_error: 0.3937 - val_loss: 0.0995 - val_root_mean_squared_error: 0.3154\n",
      "Epoch 33/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1793 - root_mean_squared_error: 0.4234 - val_loss: 0.1137 - val_root_mean_squared_error: 0.3372\n",
      "Epoch 34/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1488 - root_mean_squared_error: 0.3852 - val_loss: 0.1247 - val_root_mean_squared_error: 0.3532\n",
      "Epoch 35/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1269 - root_mean_squared_error: 0.3556 - val_loss: 0.1093 - val_root_mean_squared_error: 0.3307\n",
      "Epoch 36/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1534 - root_mean_squared_error: 0.3908 - val_loss: 0.0963 - val_root_mean_squared_error: 0.3103\n",
      "Epoch 37/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1436 - root_mean_squared_error: 0.3788 - val_loss: 0.0963 - val_root_mean_squared_error: 0.3102\n",
      "Epoch 38/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1672 - root_mean_squared_error: 0.4090 - val_loss: 0.0931 - val_root_mean_squared_error: 0.3051\n",
      "Epoch 39/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1498 - root_mean_squared_error: 0.3866 - val_loss: 0.1029 - val_root_mean_squared_error: 0.3208\n",
      "Epoch 40/300\n",
      "2/2 [==============================] - 0s 31ms/step - loss: 0.1300 - root_mean_squared_error: 0.3604 - val_loss: 0.1137 - val_root_mean_squared_error: 0.3372\n",
      "Epoch 41/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1548 - root_mean_squared_error: 0.3932 - val_loss: 0.1112 - val_root_mean_squared_error: 0.3334\n",
      "Epoch 42/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1281 - root_mean_squared_error: 0.3577 - val_loss: 0.1003 - val_root_mean_squared_error: 0.3167\n",
      "Epoch 43/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1218 - root_mean_squared_error: 0.3490 - val_loss: 0.0890 - val_root_mean_squared_error: 0.2984\n",
      "Epoch 44/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1303 - root_mean_squared_error: 0.3609 - val_loss: 0.0923 - val_root_mean_squared_error: 0.3038\n",
      "Epoch 45/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1546 - root_mean_squared_error: 0.3923 - val_loss: 0.0974 - val_root_mean_squared_error: 0.3121\n",
      "Epoch 46/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1241 - root_mean_squared_error: 0.3521 - val_loss: 0.0959 - val_root_mean_squared_error: 0.3097\n",
      "Epoch 47/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1442 - root_mean_squared_error: 0.3797 - val_loss: 0.0997 - val_root_mean_squared_error: 0.3157\n",
      "Epoch 48/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1268 - root_mean_squared_error: 0.3559 - val_loss: 0.0998 - val_root_mean_squared_error: 0.3159\n",
      "Epoch 49/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1268 - root_mean_squared_error: 0.3560 - val_loss: 0.0911 - val_root_mean_squared_error: 0.3018\n",
      "Epoch 50/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1355 - root_mean_squared_error: 0.3681 - val_loss: 0.0828 - val_root_mean_squared_error: 0.2878\n",
      "Epoch 51/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1708 - root_mean_squared_error: 0.4133 - val_loss: 0.0859 - val_root_mean_squared_error: 0.2932\n",
      "Epoch 52/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1217 - root_mean_squared_error: 0.3488 - val_loss: 0.1093 - val_root_mean_squared_error: 0.3306\n",
      "Epoch 53/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1481 - root_mean_squared_error: 0.3847 - val_loss: 0.1271 - val_root_mean_squared_error: 0.3564\n",
      "Epoch 54/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1279 - root_mean_squared_error: 0.3574 - val_loss: 0.1113 - val_root_mean_squared_error: 0.3336\n",
      "Epoch 55/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1469 - root_mean_squared_error: 0.3832 - val_loss: 0.0843 - val_root_mean_squared_error: 0.2904\n",
      "Epoch 56/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1300 - root_mean_squared_error: 0.3604 - val_loss: 0.0894 - val_root_mean_squared_error: 0.2991\n",
      "Epoch 57/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1353 - root_mean_squared_error: 0.3678 - val_loss: 0.1082 - val_root_mean_squared_error: 0.3289\n",
      "Epoch 58/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1377 - root_mean_squared_error: 0.3704 - val_loss: 0.1335 - val_root_mean_squared_error: 0.3653\n",
      "Epoch 59/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1476 - root_mean_squared_error: 0.3841 - val_loss: 0.1044 - val_root_mean_squared_error: 0.3231\n",
      "Epoch 60/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1175 - root_mean_squared_error: 0.3426 - val_loss: 0.0813 - val_root_mean_squared_error: 0.2852\n",
      "Epoch 61/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1464 - root_mean_squared_error: 0.3827 - val_loss: 0.0826 - val_root_mean_squared_error: 0.2873\n",
      "Epoch 62/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1280 - root_mean_squared_error: 0.3576 - val_loss: 0.1030 - val_root_mean_squared_error: 0.3209\n",
      "Epoch 63/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1216 - root_mean_squared_error: 0.3488 - val_loss: 0.1103 - val_root_mean_squared_error: 0.3322\n",
      "Epoch 64/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1311 - root_mean_squared_error: 0.3618 - val_loss: 0.0992 - val_root_mean_squared_error: 0.3150\n",
      "Epoch 65/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1249 - root_mean_squared_error: 0.3534 - val_loss: 0.0967 - val_root_mean_squared_error: 0.3110\n",
      "Epoch 66/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1084 - root_mean_squared_error: 0.3292 - val_loss: 0.1028 - val_root_mean_squared_error: 0.3206\n",
      "Epoch 67/300\n",
      "2/2 [==============================] - 0s 33ms/step - loss: 0.1047 - root_mean_squared_error: 0.3236 - val_loss: 0.0944 - val_root_mean_squared_error: 0.3072\n",
      "Epoch 68/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1083 - root_mean_squared_error: 0.3287 - val_loss: 0.0988 - val_root_mean_squared_error: 0.3143\n",
      "Epoch 69/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1165 - root_mean_squared_error: 0.3413 - val_loss: 0.1016 - val_root_mean_squared_error: 0.3188\n",
      "Epoch 70/300\n",
      "2/2 [==============================] - 0s 35ms/step - loss: 0.1031 - root_mean_squared_error: 0.3210 - val_loss: 0.1175 - val_root_mean_squared_error: 0.3428\n",
      "Epoch 71/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1049 - root_mean_squared_error: 0.3223 - val_loss: 0.1097 - val_root_mean_squared_error: 0.3311\n",
      "Epoch 72/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1123 - root_mean_squared_error: 0.3350 - val_loss: 0.0901 - val_root_mean_squared_error: 0.3002\n",
      "Epoch 73/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1039 - root_mean_squared_error: 0.3223 - val_loss: 0.0886 - val_root_mean_squared_error: 0.2976\n",
      "Epoch 74/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1330 - root_mean_squared_error: 0.3645 - val_loss: 0.1177 - val_root_mean_squared_error: 0.3430\n",
      "Epoch 75/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1077 - root_mean_squared_error: 0.3279 - val_loss: 0.1209 - val_root_mean_squared_error: 0.3477\n",
      "Epoch 76/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1314 - root_mean_squared_error: 0.3624 - val_loss: 0.0918 - val_root_mean_squared_error: 0.3030\n",
      "Epoch 77/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1202 - root_mean_squared_error: 0.3467 - val_loss: 0.0911 - val_root_mean_squared_error: 0.3018\n",
      "Epoch 78/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.0979 - root_mean_squared_error: 0.3128 - val_loss: 0.1058 - val_root_mean_squared_error: 0.3252\n",
      "Epoch 79/300\n",
      "2/2 [==============================] - 0s 51ms/step - loss: 0.1194 - root_mean_squared_error: 0.3456 - val_loss: 0.1264 - val_root_mean_squared_error: 0.3555\n",
      "Epoch 80/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1333 - root_mean_squared_error: 0.3651 - val_loss: 0.1231 - val_root_mean_squared_error: 0.3508\n",
      "Epoch 81/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1346 - root_mean_squared_error: 0.3669 - val_loss: 0.0889 - val_root_mean_squared_error: 0.2981\n",
      "Epoch 82/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1096 - root_mean_squared_error: 0.3311 - val_loss: 0.0839 - val_root_mean_squared_error: 0.2896\n",
      "Epoch 83/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1193 - root_mean_squared_error: 0.3451 - val_loss: 0.1076 - val_root_mean_squared_error: 0.3280\n",
      "Epoch 84/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0976 - root_mean_squared_error: 0.3123 - val_loss: 0.1342 - val_root_mean_squared_error: 0.3663\n",
      "Epoch 85/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1163 - root_mean_squared_error: 0.3404 - val_loss: 0.1145 - val_root_mean_squared_error: 0.3383\n",
      "Epoch 86/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1113 - root_mean_squared_error: 0.3336 - val_loss: 0.0901 - val_root_mean_squared_error: 0.3001\n",
      "Epoch 87/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1116 - root_mean_squared_error: 0.3338 - val_loss: 0.0974 - val_root_mean_squared_error: 0.3122\n",
      "Epoch 88/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1082 - root_mean_squared_error: 0.3278 - val_loss: 0.1221 - val_root_mean_squared_error: 0.3494\n",
      "Epoch 89/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1251 - root_mean_squared_error: 0.3537 - val_loss: 0.1076 - val_root_mean_squared_error: 0.3280\n",
      "Epoch 90/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1122 - root_mean_squared_error: 0.3349 - val_loss: 0.0973 - val_root_mean_squared_error: 0.3119\n",
      "Epoch 91/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1244 - root_mean_squared_error: 0.3527 - val_loss: 0.1154 - val_root_mean_squared_error: 0.3397\n",
      "Epoch 92/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1248 - root_mean_squared_error: 0.3533 - val_loss: 0.1098 - val_root_mean_squared_error: 0.3314\n",
      "Epoch 93/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1126 - root_mean_squared_error: 0.3354 - val_loss: 0.1097 - val_root_mean_squared_error: 0.3312\n",
      "Epoch 94/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.1209 - root_mean_squared_error: 0.3477 - val_loss: 0.1058 - val_root_mean_squared_error: 0.3253\n",
      "Epoch 95/300\n",
      "2/2 [==============================] - 0s 34ms/step - loss: 0.1273 - root_mean_squared_error: 0.3568 - val_loss: 0.0913 - val_root_mean_squared_error: 0.3022\n",
      "Epoch 96/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1038 - root_mean_squared_error: 0.3222 - val_loss: 0.1036 - val_root_mean_squared_error: 0.3219\n",
      "Epoch 97/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0895 - root_mean_squared_error: 0.2991 - val_loss: 0.1235 - val_root_mean_squared_error: 0.3514\n",
      "Epoch 98/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1053 - root_mean_squared_error: 0.3245 - val_loss: 0.1087 - val_root_mean_squared_error: 0.3296\n",
      "Epoch 99/300\n",
      "2/2 [==============================] - 0s 39ms/step - loss: 0.1267 - root_mean_squared_error: 0.3558 - val_loss: 0.0987 - val_root_mean_squared_error: 0.3141\n",
      "Epoch 100/300\n",
      "2/2 [==============================] - 0s 38ms/step - loss: 0.0850 - root_mean_squared_error: 0.2915 - val_loss: 0.0900 - val_root_mean_squared_error: 0.3000\n",
      "Epoch 101/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1208 - root_mean_squared_error: 0.3474 - val_loss: 0.1088 - val_root_mean_squared_error: 0.3299\n",
      "Epoch 102/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1108 - root_mean_squared_error: 0.3328 - val_loss: 0.1175 - val_root_mean_squared_error: 0.3428\n",
      "Epoch 103/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1050 - root_mean_squared_error: 0.3238 - val_loss: 0.1019 - val_root_mean_squared_error: 0.3191\n",
      "Epoch 104/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.1013 - root_mean_squared_error: 0.3183 - val_loss: 0.0970 - val_root_mean_squared_error: 0.3114\n",
      "Epoch 105/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.1377 - root_mean_squared_error: 0.3709 - val_loss: 0.1207 - val_root_mean_squared_error: 0.3474\n",
      "Epoch 106/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.0981 - root_mean_squared_error: 0.3132 - val_loss: 0.1235 - val_root_mean_squared_error: 0.3515\n",
      "Epoch 107/300\n",
      "2/2 [==============================] - 0s 37ms/step - loss: 0.0856 - root_mean_squared_error: 0.2925 - val_loss: 0.0926 - val_root_mean_squared_error: 0.3043\n",
      "Epoch 108/300\n",
      "2/2 [==============================] - 0s 40ms/step - loss: 0.0974 - root_mean_squared_error: 0.3121 - val_loss: 0.0886 - val_root_mean_squared_error: 0.2977\n",
      "Epoch 109/300\n",
      "2/2 [==============================] - 0s 36ms/step - loss: 0.1246 - root_mean_squared_error: 0.3529 - val_loss: 0.1140 - val_root_mean_squared_error: 0.3376\n",
      "Epoch 110/300\n",
      "2/2 [==============================] - 0s 41ms/step - loss: 0.1080 - root_mean_squared_error: 0.3287 - val_loss: 0.1369 - val_root_mean_squared_error: 0.3700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b09b611f460>"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model_(\n",
    "    input_dim=len(columns)\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=50,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train_scaled,\n",
    "    y_train_scaled,\n",
    "    epochs=300,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "fbfc6220-0aba-4b67-ae33-7cabc82ada8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_scaled = model.predict(x_test_scaled)\n",
    "# yhat = y_scaler.inverse_transform(yhat)\n",
    "# y_test = y_scaler.inverse_transform(y_test)\n",
    "yhat = np.power(np.e, yhat_scaled) - 5\n",
    "y_test = np.power(np.e, y_test_scaled) - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "1cdc9312-98cf-4050-a221-8975bae1c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.008105415287828"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8284fcf7-8b8d-4ab8-8a71-71e5ff29f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.189603834221511"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_train, y_train)\n",
    "yhat = linear_regression.predict(x_test)\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f9adc1-a46f-41aa-9460-4d8df052c825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b17889-45b9-4735-9a3e-9277b39deffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d79b3f-88ed-4e52-92f4-40552b0f23d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa17b76-af1e-4bc0-9205-a112ff1d74c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fde4d2aa-556e-416e-baa4-79be81c67b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: 5.893, LR: 6.190, skn: 54.0\n",
      "NN: 6.264, LR: 6.505, skn: 79.0\n",
      "NN: 4.954, LR: 4.873, skn: 338.0\n",
      "NN: 2.042, LR: 2.125, skn: 250.0\n",
      "NN: 2.348, LR: 2.326, skn: 267.0\n",
      "NN: 2.102, LR: 1.715, skn: 296.1\n",
      "NN: 1.521, LR: 1.508, skn: 311.0\n",
      "NN: 2.378, LR: 1.754, skn: 396.0\n",
      "NN: 2.067, LR: 1.862, skn: 400.0\n",
      "NN: 2.016, LR: 1.980, skn: 406.0\n",
      "NN: 2.814, LR: 1.983, skn: 410.0\n",
      "NN: 2.563, LR: 2.484, skn: 485.0\n",
      "NN: 2.369, LR: 2.309, skn: 703.0\n",
      "NN: 6.085, LR: 5.473, skn: 718.0\n",
      "NN: 2.570, LR: 2.472, skn: 770.0\n",
      "NN: 5.751, LR: 4.539, skn: 783.0\n",
      "NN: 6.970, LR: 5.822, skn: 784.0\n",
      "NN: 2.065, LR: 1.922, skn: 965.0\n",
      "NN: 3.535, LR: 3.613, skn: 1075.0\n",
      "NN: 4.296, LR: 3.962, skn: 1117.0\n",
      "NN: 3.968, LR: 3.909, skn: 1134.0\n",
      "NN: 7.059, LR: 6.456, skn: 87.0\n",
      "NN: 2.074, LR: 2.013, skn: 702.7\n",
      "NN: 2.566, LR: 2.655, skn: 1020.1\n"
     ]
    }
   ],
   "source": [
    "for skn in df_inner_split['skn'].unique():\n",
    "    df_train = df_inner_split.query(f'(fold == {fold}) & skn == {skn}')\n",
    "    df_test = df_inner_split.query(f'fold != {fold} & skn == {skn}')\n",
    "    x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "    y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "\n",
    "    x_scaler = MinMaxScaler()\n",
    "    # y_scaler = MinMaxScaler()\n",
    "\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_test_scaled = x_scaler.transform(x_test)\n",
    "    \n",
    "    y_train_scaled = np.log(y_train + 5)\n",
    "    y_test_scaled = np.log(y_test + 5)\n",
    "\n",
    "    model = define_model_(\n",
    "        input_dim=len(columns)\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0,\n",
    "            patience=50,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.95,\n",
    "            patience=10\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x_train_scaled,\n",
    "        y_train_scaled,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    yhat_scaled = model.predict(x_test_scaled)\n",
    "    yhat = np.power(np.e, yhat_scaled) - 5\n",
    "    rmse_nn = mean_squared_error(y_test, yhat, squared=False)\n",
    "\n",
    "\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(x_train, y_train)\n",
    "    yhat = linear_regression.predict(x_test)\n",
    "    rmse_lr = mean_squared_error(y_test, yhat, squared=False)\n",
    "    print(f\"NN: {rmse_nn:.3f}, LR: {rmse_lr:.3f}, skn: {skn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "79946328-5f26-4e88-8695-abd143c0f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_inner_split[df_inner_split['skn'] == 54]['data_in'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "450fb077-71ee-4352-b3c1-da72c11b0499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.830e+00, 3.800e+00, 2.280e+00, 1.750e+00, 2.300e-01, 1.000e-02,\n",
       "       6.800e-01, 8.000e-02, 3.700e-01, 3.700e-01, 1.600e+00, 9.600e-01,\n",
       "       5.340e+00, 3.340e+00, 3.700e-01, 2.500e-01, 2.000e-02, 2.400e-01,\n",
       "       8.000e-02, 0.000e+00, 3.000e-02, 1.900e-01, 8.400e-01, 4.100e-01,\n",
       "       7.640e+00, 6.100e-01, 1.220e+00, 5.680e+00, 2.500e-01, 0.000e+00,\n",
       "       2.000e-01, 1.440e+00, 7.000e-02, 1.700e-01, 1.018e+01, 8.660e+00,\n",
       "       2.270e+00, 6.410e+00, 8.350e+00, 9.000e-02, 0.000e+00, 9.000e-02,\n",
       "       8.000e-02, 7.000e-02, 9.000e-02, 3.050e+00, 1.300e-01, 1.970e+00,\n",
       "       5.570e+00, 5.000e-01, 6.300e-01, 1.200e-01, 3.000e-02, 2.000e-02,\n",
       "       3.900e-01, 1.000e-01, 8.000e-02, 1.540e+00, 5.800e-01, 3.900e-01,\n",
       "       7.400e-01, 1.900e+00, 1.770e+00, 4.700e-01, 5.500e-01, 0.000e+00,\n",
       "       3.000e-02, 1.000e-02, 0.000e+00, 1.900e-01, 4.900e-01, 1.990e+00,\n",
       "       5.400e-01, 9.200e-01, 3.090e+00, 1.550e+00, 5.600e-01, 3.000e-02,\n",
       "       6.100e-01, 2.000e-01, 2.000e-02, 2.600e-01, 2.360e+00, 7.680e+00,\n",
       "       2.740e+00, 3.730e+00, 2.600e+00, 1.300e-01, 1.000e-01, 0.000e+00,\n",
       "       1.700e-01, 2.700e-01, 2.000e-02, 1.000e-02, 2.340e+00, 1.177e+01,\n",
       "       6.830e+00, 4.820e+00, 5.000e-01, 1.480e+00, 3.000e-02, 1.300e-01,\n",
       "       3.200e-01, 2.300e-01, 2.600e-01, 1.400e+00, 1.900e+00, 1.840e+00,\n",
       "       1.900e+00, 2.290e+00, 7.000e-02, 6.800e-01, 1.500e-01, 0.000e+00,\n",
       "       2.300e-01, 1.260e+00, 0.000e+00, 2.000e-01, 4.760e+00, 3.660e+00,\n",
       "       6.700e-01, 7.700e-01, 5.280e+00, 1.300e-01, 5.300e-01, 1.800e-01,\n",
       "       8.300e-01, 1.520e+00, 2.400e-01, 1.830e+00, 3.500e-01, 4.550e+00,\n",
       "       1.013e+01, 6.610e+00, 4.800e-01, 1.410e+00, 8.000e-01, 2.000e-02,\n",
       "       6.000e-02, 2.000e-02, 3.000e-02, 3.000e-02, 1.270e+00, 8.100e-01,\n",
       "       5.400e-01, 2.270e+00, 2.220e+00, 3.900e-01, 1.100e-01, 3.000e-02,\n",
       "       8.200e-01, 2.000e-02, 8.900e-01, 4.000e-02, 8.100e-01, 2.290e+00,\n",
       "       4.570e+00, 3.310e+00, 0.000e+00, 5.010e+00, 1.200e-01, 7.000e-02,\n",
       "       7.000e-02, 2.000e-02, 1.400e-01, 4.150e+00, 5.500e+00, 6.100e-01,\n",
       "       5.930e+00, 1.280e+00, 4.570e+00, 1.800e-01, 4.500e-01, 0.000e+00,\n",
       "       7.000e-02, 2.600e-01, 5.000e-02, 4.100e-01, 1.600e-01, 1.980e+00,\n",
       "       6.570e+00, 1.960e+00, 4.100e+00, 1.470e+00, 2.490e+00, 4.000e-02,\n",
       "       1.200e-01, 0.000e+00, 1.150e+00, 7.800e-01, 3.200e-01, 1.090e+00,\n",
       "       1.700e+00, 3.900e-01, 1.830e+00, 2.800e-01, 1.200e-01, 1.000e-01,\n",
       "       1.500e-01, 6.000e-02, 1.200e-01, 9.000e-02, 3.900e+00, 4.200e+00,\n",
       "       3.880e+00, 4.730e+00, 3.570e+00, 1.080e+00, 2.300e-01, 7.000e-02,\n",
       "       8.200e-01, 3.200e-01, 6.000e-01, 2.230e+00, 9.690e+00, 1.940e+00,\n",
       "       1.320e+00, 5.710e+00, 1.010e+00, 8.500e-01, 2.300e-01, 0.000e+00,\n",
       "       2.600e-01, 1.400e-01, 4.000e-01, 6.800e-01, 3.190e+00, 1.120e+00,\n",
       "       4.040e+00, 1.860e+00, 1.035e+01, 1.130e+00, 1.980e+00, 2.150e+00,\n",
       "       1.580e+00, 1.070e+00, 1.000e-02, 7.700e-01, 1.520e+00, 5.400e+00,\n",
       "       5.120e+00, 6.510e+00, 5.320e+00, 1.260e+00, 2.550e+00, 2.000e-02,\n",
       "       3.100e-01, 6.000e-02, 1.300e-01, 3.600e-01, 3.970e+00, 8.640e+00,\n",
       "       8.880e+00, 1.990e+00, 1.700e+00, 1.950e+00, 2.000e-01, 2.300e-01,\n",
       "       2.100e-01, 2.300e-01, 7.000e-02, 3.200e-01, 1.900e-01, 3.130e+00,\n",
       "       3.150e+00, 2.080e+00, 1.800e-01, 1.140e+00, 7.000e-02, 0.000e+00,\n",
       "       9.000e-02, 9.000e-02, 1.300e-01, 1.100e-01, 5.020e+00, 1.100e+00,\n",
       "       1.403e+01, 8.700e-01, 2.940e+00, 1.000e+00, 0.000e+00, 7.900e-01,\n",
       "       0.000e+00, 2.700e-01, 1.100e-01, 0.000e+00, 1.000e-01, 1.000e-01,\n",
       "       8.900e-01, 3.510e+00, 3.500e+00, 2.200e-01, 0.000e+00, 4.700e-01,\n",
       "       2.000e-02, 1.220e+00, 4.000e-02, 5.700e-01, 2.600e-01, 2.310e+00,\n",
       "       2.660e+00, 8.100e-01, 7.100e-01, 2.700e-01, 6.600e-01, 0.000e+00,\n",
       "       0.000e+00, 1.000e-02, 0.000e+00, 4.000e-01, 9.100e-01, 3.080e+00,\n",
       "       9.620e+00, 1.090e+00, 2.940e+00, 6.100e-01, 3.000e-02, 4.500e-01,\n",
       "       1.600e-01, 3.000e-02, 8.200e-01, 1.090e+00, 1.930e+00, 1.500e-01,\n",
       "       3.140e+00, 3.730e+00, 1.830e+00, 4.000e-02, 0.000e+00, 0.000e+00,\n",
       "       9.000e-02, 3.200e-01, 0.000e+00, 2.400e-01, 1.980e+00, 6.000e-02,\n",
       "       1.760e+00, 2.520e+00, 3.630e+00, 5.400e-01, 4.000e-02, 0.000e+00,\n",
       "       4.000e-02, 1.000e-02, 9.000e-02, 3.600e-01, 1.950e+00, 4.000e-02,\n",
       "       2.200e-01, 3.500e-01, 1.840e+00, 3.940e+00, 4.800e-01, 1.400e-01,\n",
       "       3.000e-01, 4.100e-01, 0.000e+00, 4.100e-01, 1.500e-01, 1.710e+00,\n",
       "       8.300e-01, 2.900e-01, 2.520e+00, 2.200e-01, 9.400e-01, 3.600e-01,\n",
       "       2.200e-01, 7.300e-01, 1.300e-01, 2.240e+00, 5.460e+00, 5.220e+00,\n",
       "       6.850e+00, 7.560e+00, 5.010e+00, 2.980e+00, 0.000e+00, 4.000e-02,\n",
       "       1.400e-01, 3.200e-01, 9.000e-02, 6.800e-01, 1.000e+00, 3.030e+00,\n",
       "       1.006e+01, 4.820e+00, 2.290e+00, 2.110e+00, 2.900e-01, 1.200e-01,\n",
       "       3.200e-01, 3.800e-01, 1.200e-01, 2.700e-01, 2.200e-01, 1.930e+00,\n",
       "       5.300e-01, 2.370e+00, 3.600e-01, 5.900e-01, 1.500e+00, 0.000e+00,\n",
       "       0.000e+00, 5.500e-01, 8.600e-01, 8.000e-01, 1.710e+00, 3.030e+00,\n",
       "       1.048e+01, 3.740e+00, 6.450e+00, 3.030e+00, 7.000e-02, 2.400e-01,\n",
       "       1.900e-01, 1.030e+00, 5.100e-01, 2.490e+00, 2.030e+00, 6.960e+00,\n",
       "       5.500e-01, 9.000e-02, 5.800e-01, 4.000e-02, 8.800e-01, 0.000e+00,\n",
       "       2.300e-01, 5.200e-01, 1.300e-01, 1.230e+00, 6.800e-01, 8.620e+00,\n",
       "       2.220e+00, 5.600e-01, 1.460e+00, 5.500e-01, 2.100e-01, 0.000e+00,\n",
       "       1.300e-01, 2.400e-01, 1.000e-01, 6.000e-02, 1.290e+00, 5.200e-01,\n",
       "       1.540e+00, 2.280e+00, 1.270e+00, 1.800e-01, 8.700e-01, 0.000e+00,\n",
       "       3.800e-01, 5.700e-01, 0.000e+00, 7.340e+00, 4.490e+00, 2.120e+00,\n",
       "       1.720e+00, 2.160e+00, 5.470e+00, 2.850e+00, 7.400e-01, 4.000e-01,\n",
       "       1.300e-01, 1.300e-01, 5.000e-02, 9.500e-01, 1.410e+00, 2.850e+00,\n",
       "       3.000e+00, 2.280e+00, 4.600e-01, 4.780e+00, 4.020e+00, 0.000e+00,\n",
       "       3.400e-01, 9.000e-02, 1.760e+00, 3.000e-02, 2.350e+00, 5.380e+00,\n",
       "       7.180e+00, 9.000e-01, 8.700e-01, 1.320e+00, 1.000e-02, 0.000e+00,\n",
       "       0.000e+00, 1.200e-01, 8.000e-02, 6.300e-01, 3.620e+00, 1.103e+01,\n",
       "       1.430e+00, 4.900e+00, 3.960e+00, 1.427e+01, 5.000e-02, 1.000e-02,\n",
       "       1.350e+00, 3.500e-01, 1.100e-01, 6.460e+00, 1.850e+00, 4.830e+00,\n",
       "       7.240e+00, 5.520e+00, 4.210e+00, 0.000e+00, 6.000e-01, 2.000e-01,\n",
       "       7.000e-02, 0.000e+00, 1.800e-01, 6.300e-01, 5.560e+00, 6.270e+00,\n",
       "       2.300e+00, 4.350e+00, 1.880e+00, 1.200e-01, 4.900e-01, 0.000e+00,\n",
       "       7.000e-02, 8.300e-01, 9.100e-01, 3.000e-01, 0.000e+00, 1.000e+00,\n",
       "       4.050e+00, 1.700e+00, 1.800e-01, 6.100e-01, 2.360e+00, 4.000e-02,\n",
       "       1.180e+00, 7.000e-02, 9.200e-01, 2.120e+00, 5.880e+00, 3.840e+00,\n",
       "       3.960e+00, 1.900e-01, 1.950e+00, 1.090e+00, 7.400e-01, 0.000e+00,\n",
       "       5.200e-01, 7.500e-01, 1.100e-01, 1.750e+00, 3.250e+00, 5.900e-01,\n",
       "       8.800e-01, 2.380e+00, 7.650e+00, 8.600e-01, 0.000e+00, 6.700e-01,\n",
       "       1.180e+00, 2.000e-01, 5.700e-01, 0.000e+00, 1.000e+00, 3.300e-01,\n",
       "       3.930e+00, 1.200e+00, 2.760e+00, 1.440e+00, 3.000e-01, 0.000e+00,\n",
       "       1.300e-01, 1.900e-01, 0.000e+00, 1.300e+00, 4.500e-01, 3.300e+00,\n",
       "       4.610e+00, 3.180e+00, 5.180e+00, 8.100e-01, 1.130e+00, 7.000e-02,\n",
       "       1.000e-01, 8.000e-02, 2.200e-01, 0.000e+00, 7.440e+00, 1.156e+01,\n",
       "       8.280e+00, 1.400e+00, 4.110e+00, 1.670e+00, 1.400e+00, 4.000e-02,\n",
       "       5.500e-01, 0.000e+00, 0.000e+00, 9.100e-01, 1.510e+00, 4.600e+00,\n",
       "       6.200e-01, 8.800e-01, 2.500e-01, 8.900e-01, 8.000e-02, 8.000e-02,\n",
       "       0.000e+00, 3.800e-01, 0.000e+00, 1.190e+00, 3.500e-01, 2.700e+00,\n",
       "       2.680e+00, 1.190e+00, 1.400e-01, 6.500e-01, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 5.000e-01, 1.300e-01])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df_inner_split[df_inner_split['skn'] == 396]['data_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0be02c4c-deda-4404-bcd4-382badb8e938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.55178618, 2.17475172, 1.98513086, 1.9095425 , 1.65441128,\n",
       "       1.61143592, 1.73695123, 1.62531126, 1.68082791, 1.68082791,\n",
       "       1.88706965, 1.78507048, 2.33601987, 2.12106322, 1.68082791,\n",
       "       1.65822808, 1.61342993, 1.6563215 , 1.62531126, 1.60943791,\n",
       "       1.61541998, 1.6467337 , 1.7647308 , 1.68824909, 2.53686639,\n",
       "       1.72455072, 1.82776991, 2.36837283, 1.65822808, 1.60943791,\n",
       "       1.64865863, 1.86252854, 1.62334082, 1.64287269, 2.71997877,\n",
       "       2.61447185, 1.98375629, 2.43449016, 2.59151638, 1.62727783,\n",
       "       1.60943791, 1.62727783, 1.62531126, 1.62334082, 1.62727783,\n",
       "       2.08567209, 1.63510566, 1.94161522, 2.3580198 , 1.70474809,\n",
       "       1.72810944, 1.63315444, 1.61541998, 1.61342993, 1.68454538,\n",
       "       1.62924054, 1.62531126, 1.87793717, 1.71918878, 1.68454538,\n",
       "       1.74745921, 1.93152141, 1.91250109, 1.69927862, 1.71379793,\n",
       "       1.60943791, 1.61541998, 1.61143592, 1.60943791, 1.6467337 ,\n",
       "       1.70292826, 1.94448056, 1.7119945 , 1.77833645, 2.09062873,\n",
       "       1.87946505, 1.71559811, 1.61541998, 1.72455072, 1.64865863,\n",
       "       1.61342993, 1.66013103, 1.99605993, 2.54002595, 2.04640169,\n",
       "       2.16676537, 2.02814825, 1.63510566, 1.62924054, 1.60943791,\n",
       "       1.64287269, 1.66203036, 1.61342993, 1.61143592, 1.99333884,\n",
       "       2.81959158, 2.47063868, 2.28442112, 1.70474809, 1.86872051,\n",
       "       1.61541998, 1.63510566, 1.6714733 , 1.65441128, 1.66013103,\n",
       "       1.85629799, 1.93152141, 1.92278773, 1.93152141, 1.98650355,\n",
       "       1.62334082, 1.73695123, 1.63899671, 1.60943791, 1.65441128,\n",
       "       1.83418019, 1.60943791, 1.64865863, 2.2782924 , 2.15871472,\n",
       "       1.73518912, 1.75267208, 2.33020026, 1.63510566, 1.71018782,\n",
       "       1.64480506, 1.763017  , 1.87487438, 1.6563215 , 1.92132467,\n",
       "       1.67709656, 2.25654115, 2.71667953, 2.4518668 , 1.7011051 ,\n",
       "       1.85785927, 1.75785792, 1.61342993, 1.62136648, 1.61342993,\n",
       "       1.61541998, 1.61541998, 1.83577635, 1.75958057, 1.7119945 ,\n",
       "       1.98375629, 1.97685495, 1.68454538, 1.6311994 , 1.61541998,\n",
       "       1.76130026, 1.61342993, 1.773256  , 1.61740608, 1.75958057,\n",
       "       1.98650355, 2.25863321, 2.11745961, 1.60943791, 2.30358459,\n",
       "       1.63315444, 1.62334082, 1.62334082, 1.61342993, 1.63705308,\n",
       "       2.21375388, 2.35137526, 1.72455072, 2.3915113 , 1.83736998,\n",
       "       2.25863321, 1.64480506, 1.69561561, 1.60943791, 1.62334082,\n",
       "       1.66013103, 1.61938824, 1.68824909, 1.64093658, 1.94304892,\n",
       "       2.44841554, 1.94017947, 2.20827441, 1.86717611, 2.0135688 ,\n",
       "       1.61740608, 1.63315444, 1.60943791, 1.81645208, 1.75440368,\n",
       "       1.6714733 , 1.80664808, 1.90210753, 1.68454538, 1.92132467,\n",
       "       1.6639261 , 1.63315444, 1.62924054, 1.63899671, 1.62136648,\n",
       "       1.63315444, 1.62727783, 2.18605128, 2.21920348, 2.18380156,\n",
       "       2.2752139 , 2.14826773, 1.8050047 , 1.65441128, 1.62334082,\n",
       "       1.76130026, 1.6714733 , 1.7227666 , 1.97823904, 2.68716699,\n",
       "       1.93730177, 1.84371921, 2.37117788, 1.79342475, 1.76644166,\n",
       "       1.65441128, 1.60943791, 1.66013103, 1.63705308, 1.68639895,\n",
       "       1.73695123, 2.1029139 , 1.8115621 , 2.20165917, 1.92570744,\n",
       "       2.73111547, 1.81319475, 1.94304892, 1.96711236, 1.88403475,\n",
       "       1.80335861, 1.61143592, 1.75267208, 1.87487438, 2.34180581,\n",
       "       2.31451366, 2.44321622, 2.33408376, 1.83418019, 2.02154756,\n",
       "       1.61342993, 1.66959184, 1.62136648, 1.63510566, 1.67896398,\n",
       "       2.19388568, 2.61300665, 2.63044896, 1.94448056, 1.90210753,\n",
       "       1.93874166, 1.64865863, 1.65441128, 1.65057986, 1.65441128,\n",
       "       1.62334082, 1.6714733 , 1.6467337 , 2.09556092, 2.09801793,\n",
       "       1.95727391, 1.64480506, 1.81482474, 1.62334082, 1.60943791,\n",
       "       1.62727783, 1.62727783, 1.63510566, 1.6311994 , 2.3045831 ,\n",
       "       1.80828877, 2.94601668, 1.76985463, 2.07191328, 1.79175947,\n",
       "       1.60943791, 1.75613229, 1.60943791, 1.66203036, 1.6311994 ,\n",
       "       1.60943791, 1.62924054, 1.62924054, 1.773256  , 2.14124194,\n",
       "       2.14006616, 1.6524974 , 1.60943791, 1.69927862, 1.61342993,\n",
       "       1.82776991, 1.61740608, 1.71739505, 1.66013103, 1.98924327,\n",
       "       2.03601198, 1.75958057, 1.74221902, 1.66203036, 1.73342389,\n",
       "       1.60943791, 1.60943791, 1.61143592, 1.60943791, 1.68639895,\n",
       "       1.77664583, 2.08939187, 2.68239045, 1.80664808, 2.07191328,\n",
       "       1.72455072, 1.61541998, 1.69561561, 1.64093658, 1.61541998,\n",
       "       1.76130026, 1.80664808, 1.93585981, 1.63899671, 2.09679018,\n",
       "       2.16676537, 1.92132467, 1.61740608, 1.60943791, 1.60943791,\n",
       "       1.62727783, 1.6714733 , 1.60943791, 1.6563215 , 1.94304892,\n",
       "       1.62136648, 1.91102289, 2.01756614, 2.15524451, 1.7119945 ,\n",
       "       1.61740608, 1.60943791, 1.61740608, 1.61143592, 1.62727783,\n",
       "       1.67896398, 1.93874166, 1.61740608, 1.6524974 , 1.67709656,\n",
       "       1.92278773, 2.19053559, 1.7011051 , 1.63705308, 1.66770682,\n",
       "       1.68824909, 1.60943791, 1.68824909, 1.63899671, 1.90359895,\n",
       "       1.763017  , 1.66581825, 2.01756614, 1.6524974 , 1.78170913,\n",
       "       1.67896398, 1.6524974 , 1.74571553, 1.63510566, 1.97962121,\n",
       "       2.34755846, 2.32434658, 2.47232787, 2.53051716, 2.30358459,\n",
       "       2.07693841, 1.60943791, 1.61740608, 1.63705308, 1.6714733 ,\n",
       "       1.62727783, 1.73695123, 1.79175947, 2.08318453, 2.71204222,\n",
       "       2.28442112, 1.98650355, 1.96150224, 1.66581825, 1.63315444,\n",
       "       1.6714733 , 1.68268837, 1.63315444, 1.66203036, 1.6524974 ,\n",
       "       1.93585981, 1.71018782, 1.99741771, 1.67896398, 1.72097929,\n",
       "       1.87180218, 1.60943791, 1.60943791, 1.71379793, 1.7681496 ,\n",
       "       1.75785792, 1.90359895, 2.08318453, 2.73954887, 2.16791019,\n",
       "       2.43798973, 2.08318453, 1.62334082, 1.6563215 , 1.6467337 ,\n",
       "       1.79674701, 1.70656462, 2.0135688 , 1.95018671, 2.48156775,\n",
       "       1.71379793, 1.62727783, 1.71918878, 1.61740608, 1.77155676,\n",
       "       1.60943791, 1.65441128, 1.70837786, 1.63510566, 1.82937633,\n",
       "       1.73695123, 2.6115393 , 1.97685495, 1.71559811, 1.86562932,\n",
       "       1.71379793, 1.65057986, 1.60943791, 1.63510566, 1.6563215 ,\n",
       "       1.62924054, 1.62136648, 1.83896107, 1.70837786, 1.87793717,\n",
       "       1.98513086, 1.83577635, 1.64480506, 1.76985463, 1.60943791,\n",
       "       1.68268837, 1.71739505, 1.60943791, 2.51284602, 2.25023861,\n",
       "       1.96290773, 1.90508815, 1.96850998, 2.34851402, 2.06051353,\n",
       "       1.74745921, 1.68639895, 1.63510566, 1.63510566, 1.61938824,\n",
       "       1.78339122, 1.85785927, 2.06051353, 2.07944154, 1.98513086,\n",
       "       1.69744879, 2.28033948, 2.19944433, 1.60943791, 1.67522565,\n",
       "       1.62727783, 1.91102289, 1.61541998, 1.99470031, 2.33988088,\n",
       "       2.49979526, 1.77495235, 1.76985463, 1.84371921, 1.61143592,\n",
       "       1.60943791, 1.60943791, 1.63315444, 1.62531126, 1.72810944,\n",
       "       2.15408508, 2.77446197, 1.86097454, 2.29253476, 2.19277023,\n",
       "       2.95854948, 1.61938824, 1.61143592, 1.84845481, 1.67709656,\n",
       "       1.6311994 , 2.43886271, 1.92424865, 2.28543893, 2.50470928,\n",
       "       2.35327821, 2.22028985, 1.60943791, 1.7227666 , 1.64865863,\n",
       "       1.62334082, 1.60943791, 1.64480506, 1.72810944, 2.35707328,\n",
       "       2.42214433, 1.98787435, 2.23537634, 1.92861865, 1.63315444,\n",
       "       1.70292826, 1.60943791, 1.62334082, 1.763017  , 1.77664583,\n",
       "       1.66770682, 1.60943791, 1.79175947, 2.20276476, 1.90210753,\n",
       "       1.64480506, 1.72455072, 1.99605993, 1.61740608, 1.82131827,\n",
       "       1.62334082, 1.77833645, 1.96290773, 2.38692624, 2.17928688,\n",
       "       2.19277023, 1.6467337 , 1.93874166, 1.80664808, 1.74745921,\n",
       "       1.60943791, 1.70837786, 1.74919985, 1.6311994 , 1.9095425 ,\n",
       "       2.1102132 , 1.72097929, 1.77155676, 1.99877364, 2.53765722,\n",
       "       1.7681496 , 1.60943791, 1.73518912, 1.82131827, 1.64865863,\n",
       "       1.71739505, 1.60943791, 1.79175947, 1.67335124, 2.18941639,\n",
       "       1.82454929, 2.04898233, 1.86252854, 1.66770682, 1.60943791,\n",
       "       1.63510566, 1.6467337 , 1.60943791, 1.84054963, 1.69561561,\n",
       "       2.11625551, 2.26280422, 2.10169215, 2.32042501, 1.75958057,\n",
       "       1.81319475, 1.62334082, 1.62924054, 1.62531126, 1.6524974 ,\n",
       "       1.60943791, 2.52091709, 2.80699015, 2.58625914, 1.85629799,\n",
       "       2.20937271, 1.89761986, 1.85629799, 1.61740608, 1.71379793,\n",
       "       1.60943791, 1.60943791, 1.77664583, 1.87333946, 2.2617631 ,\n",
       "       1.72633166, 1.77155676, 1.65822808, 1.773256  , 1.62531126,\n",
       "       1.62531126, 1.60943791, 1.68268837, 1.60943791, 1.82293509,\n",
       "       1.67709656, 2.04122033, 2.03861955, 1.82293509, 1.63705308,\n",
       "       1.73165555, 1.60943791, 1.60943791, 1.60943791, 1.60943791,\n",
       "       1.70474809, 1.63510566])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.array(df_inner_split[df_inner_split['skn'] == 396]['data_in']) + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b5ef7698-a431-4e6b-912f-5ce7ca0b3255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.69314718, 1.09861229, 4.60517019])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3, 100])\n",
    "np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df3d68-b064-4b84-9451-7df5df8de147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
