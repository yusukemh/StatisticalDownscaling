{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "5cef254e-2af1-4129-a6b0-bbdd6687d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sherpa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/writeup')\n",
    "from config import C_COMMON, C_GRID, C_SINGLE, FILENAME\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "fc39d4b4-02b6-4ffe-b9f4-de9f899deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = C_SINGLE\n",
    "df = pd.read_csv(FILENAME, usecols=C_COMMON + columns).sort_values(['year', 'month'])\n",
    "\n",
    "# we use the last 1/5 data as the heldout clean dataset. We do not use this fold for any use except for just reporting the result.\n",
    "df_train_outer = df.query('fold != 4')\n",
    "df_test_outer = df.query('fold == 4')\n",
    "assert (sorted(df_test_outer['skn'].unique()) == sorted(df_train_outer['skn'].unique()))\n",
    "\n",
    "# split the trainig data into 5 folds for inner cross validation\n",
    "def assign_inner_fold(df, n_folds=5):\n",
    "    # assign fold for each sample\n",
    "    df_len_by_month = pd.DataFrame(df.groupby(by=['year', 'month']).size()).reset_index().rename({0: \"len\"}, axis=1)\n",
    "    df_len_by_month = df_len_by_month.sort_values(['year', 'month'])\n",
    "    df_len_by_month['cumsum'] = df_len_by_month['len'].cumsum()\n",
    "    n_samples_total = df_len_by_month['cumsum'].iloc[-1]\n",
    "    n_samples_per_fold = np.ceil(n_samples_total / n_folds)\n",
    "    \n",
    "    df_len_by_month['inner_fold'] = df_len_by_month.apply(lambda row: int(row['cumsum'] / n_samples_per_fold), axis=1)\n",
    "    \n",
    "    df_w_fold = pd.merge(left=df, right=df_len_by_month, left_on=['year', 'month'], right_on=['year', 'month'])\n",
    "    \n",
    "    return df_w_fold\n",
    "\n",
    "df_inner_split = assign_inner_fold(df_train_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "c52c61ab-7678-4ed8-beea-ec6aec33b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(\n",
    "    input_dim=20,\n",
    "    n_units=512,\n",
    "    activation='selu',#selu\n",
    "    learning_rate=0.00001,\n",
    "    loss='mse',\n",
    "    batch_size=64\n",
    "):\n",
    "    inputs = Input(shape=(input_dim))\n",
    "    # x = Dense(units=n_units, activation=activation, kernel_regularizer='l1')(inputs)\n",
    "    x = Dense(units=n_units, activation=activation)(inputs)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)# serves as regularization\n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model, batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "857a1b2b-3e8d-4c75-905e-cc5308fd6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_dataset(df, skn, inner_fold):\n",
    "#     \"\"\"\n",
    "#     Splits dataset into train and test, and scales x\n",
    "#     \"\"\"\n",
    "#     df_station = df[df['skn'] == skn]\n",
    "#     df_train = df_station[df_station['inner_fold'] != inner_fold]\n",
    "#     df_test = df_station[df_station['inner_fold'] == inner_fold]\n",
    "#     x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "#     y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "    \n",
    "#     x_scaler = MinMaxScaler()\n",
    "#     x_train = x_scaler.fit_transform(x_train)\n",
    "#     x_test = x_scaler.transform(x_test)\n",
    "    \n",
    "#     return x_train, x_test, y_train, y_test\n",
    "\n",
    "# def transform_y(y_train, y_test):\n",
    "#     scaler = MinMaxScaler(feature_range=(0,1))\n",
    "#     y_train = np.log(y_train + 1.)\n",
    "#     y_test = np.log(y_test + 1.)\n",
    "    \n",
    "#     y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "#     y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "#     return y_train, y_test, scaler\n",
    "    \n",
    "# def inverse_transform_y(y, scaler):\n",
    "#     y = scaler.inverse_transform(y)\n",
    "#     y = np.power(np.e, y) - 1\n",
    "#     return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "7e82b924-e2b5-43e7-a6df-b7b37f565812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self, model_func, params):\n",
    "        self.model_func = model_func\n",
    "        self.params = params\n",
    "        pass\n",
    "    \n",
    "    def cross_val_predict(self, df, skn, verbose=0, n_folds=5):\n",
    "        assert 'inner_fold' in df.columns, 'define fold with column name \"inner_fold\"'\n",
    "        df_station = df[df['skn'] == skn]\n",
    "        \n",
    "        list_ytrue = []\n",
    "        list_ypred = []\n",
    "        for k in range(n_folds):\n",
    "            # split the dataset\n",
    "            df_train = df_station[df_station['inner_fold'] != k]\n",
    "            df_test = df_station[df_station['inner_fold'] == k]\n",
    "            \n",
    "            # convert to numpy\n",
    "            x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "            y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "            \n",
    "            # scale the input and output\n",
    "            x_train, x_test = self.transform_x(x_train, x_test)\n",
    "            y_train, y_test, y_scaler = self.transform_y(y_train, y_test)\n",
    "            \n",
    "            # train the model\n",
    "            self.train(x_train, y_train, verbose=0, retrain_full=False) # to speed up computation for hyperparaemter tuning\n",
    "            \n",
    "            # make prediction and scale\n",
    "            y_pred = self.model.predict(x_test)\n",
    "            y_pred = self.inverse_transform_y(y_pred, y_scaler)\n",
    "            # scale y_test\n",
    "            y_test = self.inverse_transform_y(y_test, y_scaler)\n",
    "            \n",
    "            # keep the record\n",
    "            list_ytrue.extend(y_test)\n",
    "            list_ypred.extend(y_pred)\n",
    "        \n",
    "        # calculate the loss and return\n",
    "        return {\n",
    "            \"mse\": mean_squared_error(list_ytrue, list_ypred, squared=False),\n",
    "            \"mae\": mean_absolute_error(list_ytrue, list_ypred)\n",
    "        }\n",
    "\n",
    "    def transform_x(self, x_train, x_test):\n",
    "        scaler = MinMaxScaler()\n",
    "        x_train = scaler.fit_transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        return x_train, x_test\n",
    "    \n",
    "    def transform_y(self, y_train, y_test):\n",
    "        scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        y_train = np.log(y_train + 1.)\n",
    "        y_test = np.log(y_test + 1.)\n",
    "\n",
    "        y_train = scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test = scaler.transform(y_test.reshape(-1, 1))\n",
    "\n",
    "        return y_train, y_test, scaler\n",
    "    \n",
    "    def inverse_transform_y(self, y, scaler):\n",
    "        y = scaler.inverse_transform(y)\n",
    "        y = np.power(np.e, y) - 1\n",
    "        return y\n",
    "    \n",
    "    def train(self, x, y, verbose=0, retrain_full=False):\n",
    "        # build the model\n",
    "        self.model, batch_size = self.model_func(**self.params)\n",
    "        \n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                min_delta=0,\n",
    "                patience=20,\n",
    "                restore_best_weights=True,\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.95,\n",
    "                patience=10\n",
    "            )\n",
    "        ]\n",
    "        history = self.model.fit(\n",
    "            x, y,\n",
    "            epochs=500,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        \n",
    "        if retrain_full:\n",
    "            epochs = len(history.history['loss'])\n",
    "            # rebuild the model\n",
    "            self.model, batch_size = self.model_func(**params)\n",
    "            callbacks = [EarlyStopping(monitor='loss', min_delta=0, patience=1e3, restore_best_weights=True)]\n",
    "            history = self.model.fit(\n",
    "                x, y,\n",
    "                epochs=epochs,\n",
    "                validation_split=0,\n",
    "                callbacks=callbacks,\n",
    "                batch_size=batch_size,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        return history        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "id": "207727a9-f94b-4877-ba71-4d57c3f1d3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mse': 5.321785170696368, 'mae': 3.4525449770133205}"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skn = 54\n",
    "model = NeuralNetwork(\n",
    "    model_func=define_model,\n",
    "    params = {\n",
    "        'input_dim': 16,\n",
    "        'n_units': 362,\n",
    "        'learning_rate': 0.000695,\n",
    "        'loss': 'mse',\n",
    "        'batch_size': 128\n",
    "    }\n",
    ")\n",
    "model.cross_val_predict(df_inner_split, skn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "25600fec-a566-495a-ba84-78e4b4e4f777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.538217695379\n",
      "4.441658202184386\n",
      "5.186904581988482\n",
      "6.106725221154468\n",
      "5.592084881258681\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\tn_units\tlearning_rate\tloss\tskn\tbatch_size\tmae\trmse\ttrial_id\n",
    "291.966094\t0.000833\tmae\t54.0\t256\t3.495661\t5.385094\t0\n",
    "455.336094\t0.008304\tmae\t54.0\t64\t18.907637\t29.576737\t\n",
    "16\t362.231815\t0.000695\tmse\t54.0\t128\n",
    "'''\n",
    "skn = 54\n",
    "# df_station = df_inner_split.query(f'skn == {skn}')\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "batch_size=128\n",
    "a = []\n",
    "b = []\n",
    "for inner_fold in range(5):\n",
    "    x_train, x_test, y_train, y_test = prepare_dataset(df_inner_split, skn=skn, inner_fold=inner_fold)\n",
    "    y_train, y_test, scaler = transform_y(y_train, y_test)\n",
    "\n",
    "    params = {'input_dim': 16,\n",
    "     'n_units': 362,\n",
    "     'learning_rate': 0.000695,\n",
    "     'loss': 'mse'}\n",
    "    model, _ = define_model(**params)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=500,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = inverse_transform_y(y_pred, scaler)\n",
    "    y_test = inverse_transform_y(y_test, scaler)\n",
    "    a.extend(y_pred)\n",
    "    b.extend(y_test)\n",
    "    print(mean_squared_error(y_test, y_pred, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "1c0cd493-6aea-4d7b-8b1a-a2fc31ae08c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.402684042795685, 3.4198443049863214)"
      ]
     },
     "execution_count": 521,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(a, b, squared=False), mean_absolute_error(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "ebd7fd0f-1d1a-414a-b34f-8c0a1a938e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7998480148265161"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test= prepare_dataset(df_inner_split, skn=skn, inner_fold=0)\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_train, y_train)\n",
    "yhat = linear_regression.predict(x_test)\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "2bba97a3-8478-46aa-a8b0-1b65c62a569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://10.100.11.207:8898 if on a cluster or\n",
      "http://localhost:8898 if running locally.\n",
      "-------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'sherpa.app.app' (lazy loading)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "100%|██████████| 24/24 [04:44<00:00, 11.84s/it]\n",
      "100%|██████████| 24/24 [03:00<00:00,  7.53s/it]\n"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    sherpa.Continuous('n_units', [256, 512, 1024]),\n",
    "    sherpa.Continuous('learning_rate', [0.00001, 0.01]),\n",
    "    sherpa.Choice('batch_size', [64, 128, 192, 256, 512]),\n",
    "    sherpa.Choice('loss', ['mse', 'mae'])\n",
    "]\n",
    "n_run = 2\n",
    "alg = sherpa.algorithms.RandomSearch(max_num_trials=n_run)\n",
    "study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True)\n",
    "dfs = []\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "dfs = []\n",
    "for i, trial in enumerate(study):\n",
    "    start = time.time()\n",
    "    params = {\n",
    "        'input_dim': len(columns),\n",
    "        'n_units': trial.parameters['n_units'],\n",
    "        'learning_rate': trial.parameters['learning_rate'],\n",
    "        'loss': trial.parameters['loss']\n",
    "    }\n",
    "    batch_size = trial.parameters['batch_size']\n",
    "    \n",
    "    for skn in tqdm(df_inner_split['skn'].unique()):        \n",
    "        ytest_station = []\n",
    "        yhat_station = []\n",
    "        for inner_fold in range(5):\n",
    "            x_train, x_test, y_train, y_test = prepare_dataset(df_inner_split, skn=skn, inner_fold=inner_fold)\n",
    "            y_train, y_test, scaler = transform_y(y_train, y_test)\n",
    "            model = define_model(**params)\n",
    "            model.fit(x_train, y_train, epochs=500, validation_split=0.2, callbacks=callbacks,\n",
    "                batch_size=batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            yhat = model.predict(x_test)\n",
    "            yhat = inverse_transform_y(yhat, scaler)\n",
    "            y_test = inverse_transform_y(y_test, scaler)\n",
    "            \n",
    "            # record the result\n",
    "            yhat_station.extend(yhat)\n",
    "            ytest_station.extend(y_test)\n",
    "        \n",
    "        mae_station = mean_absolute_error(ytest_station, yhat_station)\n",
    "        rmse_station = mean_squared_error(ytest_station, yhat_station, squared=False)\n",
    "        \n",
    "        _ = pd.DataFrame([params])\n",
    "        _['skn'] = [skn]\n",
    "        _['batch_size'] = [batch_size]\n",
    "        _['mae'] = [mae_station]\n",
    "        _['rmse'] = [rmse_station]\n",
    "        _['trial_id'] = [i]\n",
    "        dfs.append(_)\n",
    "#     pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b019884-c745-4808-ba73-8fb7ec6893ad",
   "metadata": {},
   "source": [
    "# get the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "e0cf5c5b-7619-4d80-a4f5-5e65ad459afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report = pd.read_csv('nn_report_240_single.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "d5177b70-3303-4cd4-ad01-95e233874a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = df_report.groupby(by='trial_id').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "e17ae9dd-90c1-4345-a52e-abb8959290d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_mae_index = df_groupby[df_groupby['mae'] == df_groupby['mae'].min()].index.values[0]\n",
    "min_mse_index = df_groupby[df_groupby['rmse'] == df_groupby['rmse'].min()].index.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "2b15e498-f9ef-4230-bd78-434f41a3a502",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "idx = min_mse_index\n",
    "params['input_dim'] = int(df_report[df_report['trial_id'] == idx].iloc[0]['input_dim'])\n",
    "params['n_units'] = int(df_report[df_report['trial_id'] == idx].iloc[0]['n_units'])\n",
    "params['learning_rate'] = float(df_report[df_report['trial_id'] == idx].iloc[0]['learning_rate'])\n",
    "params['loss'] = df_report[df_report['trial_id'] == idx].iloc[0]['loss']\n",
    "batch_size = df_report[df_report['trial_id'] == idx].iloc[0]['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "364208ef-a99d-478e-8597-54c55843fba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:43<00:00,  8.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6282724449488883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "skn = 54\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "list_ytrue = []\n",
    "list_ypred = []\n",
    "for inner_fold in tqdm(range(5)):\n",
    "    x_train, x_test, y_train, y_test = prepare_dataset(df_inner_split, skn=skn, inner_fold=inner_fold)\n",
    "    y_train, y_test, scaler = transform_y(y_train, y_test)\n",
    "    model = define_model(**params)\n",
    "    model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs=500,\n",
    "        batch_size=batch_size,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_pred = inverse_transform_y(y_pred, scaler)\n",
    "    y_test = inverse_transform_y(y_test, scaler)\n",
    "    list_ypred.extend(y_pred)\n",
    "    list_ytrue.extend(y_test)\n",
    "    # print(mean_squared_error(y_test, y_pred, squared=False))\n",
    "print(mean_absolute_error(list_ytrue, list_ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "54f7178f-cf2d-4161-87be-e6ccf255c7c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>input_dim</th>\n",
       "      <th>n_units</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>skn</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>trial_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1776</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>54.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.433046</td>\n",
       "      <td>5.296056</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1777</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>79.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.886207</td>\n",
       "      <td>5.973462</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1778</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>338.0</td>\n",
       "      <td>128</td>\n",
       "      <td>2.544070</td>\n",
       "      <td>4.762007</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1779</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>250.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.394395</td>\n",
       "      <td>2.049286</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1780</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>267.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.505138</td>\n",
       "      <td>2.224552</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>296.1</td>\n",
       "      <td>128</td>\n",
       "      <td>0.799617</td>\n",
       "      <td>1.681203</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>311.0</td>\n",
       "      <td>128</td>\n",
       "      <td>0.796940</td>\n",
       "      <td>1.520546</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1783</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>396.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.051577</td>\n",
       "      <td>1.784216</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1784</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>400.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.138457</td>\n",
       "      <td>1.805463</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1785</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>406.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.241432</td>\n",
       "      <td>1.891523</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1786</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>410.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.374516</td>\n",
       "      <td>2.157048</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1787</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>485.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.682477</td>\n",
       "      <td>2.451334</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1788</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>703.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.069706</td>\n",
       "      <td>2.117684</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1789</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>718.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.898725</td>\n",
       "      <td>5.374983</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>770.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.435372</td>\n",
       "      <td>2.314500</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1791</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>783.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.270729</td>\n",
       "      <td>4.529812</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1792</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>784.0</td>\n",
       "      <td>128</td>\n",
       "      <td>4.192942</td>\n",
       "      <td>5.592003</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>965.0</td>\n",
       "      <td>128</td>\n",
       "      <td>1.245860</td>\n",
       "      <td>2.106967</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>128</td>\n",
       "      <td>2.565427</td>\n",
       "      <td>4.470704</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1795</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>128</td>\n",
       "      <td>2.602223</td>\n",
       "      <td>3.759634</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1796</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>128</td>\n",
       "      <td>2.223426</td>\n",
       "      <td>3.740062</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1797</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>87.0</td>\n",
       "      <td>128</td>\n",
       "      <td>3.806575</td>\n",
       "      <td>5.621916</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1798</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>702.7</td>\n",
       "      <td>128</td>\n",
       "      <td>1.008225</td>\n",
       "      <td>2.055014</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1799</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>362.231815</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>mse</td>\n",
       "      <td>1020.1</td>\n",
       "      <td>128</td>\n",
       "      <td>1.605019</td>\n",
       "      <td>2.486570</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  input_dim     n_units  learning_rate loss     skn  \\\n",
       "1776           0         16  362.231815       0.000695  mse    54.0   \n",
       "1777           0         16  362.231815       0.000695  mse    79.0   \n",
       "1778           0         16  362.231815       0.000695  mse   338.0   \n",
       "1779           0         16  362.231815       0.000695  mse   250.0   \n",
       "1780           0         16  362.231815       0.000695  mse   267.0   \n",
       "1781           0         16  362.231815       0.000695  mse   296.1   \n",
       "1782           0         16  362.231815       0.000695  mse   311.0   \n",
       "1783           0         16  362.231815       0.000695  mse   396.0   \n",
       "1784           0         16  362.231815       0.000695  mse   400.0   \n",
       "1785           0         16  362.231815       0.000695  mse   406.0   \n",
       "1786           0         16  362.231815       0.000695  mse   410.0   \n",
       "1787           0         16  362.231815       0.000695  mse   485.0   \n",
       "1788           0         16  362.231815       0.000695  mse   703.0   \n",
       "1789           0         16  362.231815       0.000695  mse   718.0   \n",
       "1790           0         16  362.231815       0.000695  mse   770.0   \n",
       "1791           0         16  362.231815       0.000695  mse   783.0   \n",
       "1792           0         16  362.231815       0.000695  mse   784.0   \n",
       "1793           0         16  362.231815       0.000695  mse   965.0   \n",
       "1794           0         16  362.231815       0.000695  mse  1075.0   \n",
       "1795           0         16  362.231815       0.000695  mse  1117.0   \n",
       "1796           0         16  362.231815       0.000695  mse  1134.0   \n",
       "1797           0         16  362.231815       0.000695  mse    87.0   \n",
       "1798           0         16  362.231815       0.000695  mse   702.7   \n",
       "1799           0         16  362.231815       0.000695  mse  1020.1   \n",
       "\n",
       "      batch_size       mae      rmse  trial_id  \n",
       "1776         128  3.433046  5.296056        74  \n",
       "1777         128  3.886207  5.973462        74  \n",
       "1778         128  2.544070  4.762007        74  \n",
       "1779         128  1.394395  2.049286        74  \n",
       "1780         128  1.505138  2.224552        74  \n",
       "1781         128  0.799617  1.681203        74  \n",
       "1782         128  0.796940  1.520546        74  \n",
       "1783         128  1.051577  1.784216        74  \n",
       "1784         128  1.138457  1.805463        74  \n",
       "1785         128  1.241432  1.891523        74  \n",
       "1786         128  1.374516  2.157048        74  \n",
       "1787         128  1.682477  2.451334        74  \n",
       "1788         128  1.069706  2.117684        74  \n",
       "1789         128  3.898725  5.374983        74  \n",
       "1790         128  1.435372  2.314500        74  \n",
       "1791         128  3.270729  4.529812        74  \n",
       "1792         128  4.192942  5.592003        74  \n",
       "1793         128  1.245860  2.106967        74  \n",
       "1794         128  2.565427  4.470704        74  \n",
       "1795         128  2.602223  3.759634        74  \n",
       "1796         128  2.223426  3.740062        74  \n",
       "1797         128  3.806575  5.621916        74  \n",
       "1798         128  1.008225  2.055014        74  \n",
       "1799         128  1.605019  2.486570        74  "
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_report[df_report['trial_id'] == idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "ecb9bc16-cd24-459e-8c69-8570643c22fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.524202419975154"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_ypred = []\n",
    "list_ytrue = []\n",
    "for fold in range(5):\n",
    "    x_train, x_test, y_train, y_test= prepare_dataset(df_inner_split, skn=skn, inner_fold=0)\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(x_train, y_train)\n",
    "    yhat = linear_regression.predict(x_test)\n",
    "    list_ytrue.append(y_test)\n",
    "    list_ypred.append(yhat)\n",
    "mean_absolute_error(list_ytrue, list_ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be59392-f412-4436-97b6-c00223b5329b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
