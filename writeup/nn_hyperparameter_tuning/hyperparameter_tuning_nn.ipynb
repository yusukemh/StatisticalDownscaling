{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cef254e-2af1-4129-a6b0-bbdd6687d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 23:51:43.583375: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import sherpa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/yusukemh/github/yusukemh/StatisticalDownscaling/writeup')\n",
    "from config import C_COMMON, C_GRID, C_SINGLE, FILENAME\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc39d4b4-02b6-4ffe-b9f4-de9f899deb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = C_SINGLE\n",
    "df = pd.read_csv(FILENAME, usecols=C_COMMON + columns).sort_values(['year', 'month'])\n",
    "\n",
    "# we use the last 1/5 data as the heldout clean dataset. We do not use this fold for any use except for just reporting the result.\n",
    "df_train_outer = df.query('fold != 4')\n",
    "df_test_outer = df.query('fold == 4')\n",
    "assert (sorted(df_test_outer['skn'].unique()) == sorted(df_train_outer['skn'].unique()))\n",
    "\n",
    "# split the trainig data into 5 folds for inner cross validation\n",
    "def assign_inner_fold(df, n_folds=5):\n",
    "    # assign fold for each sample\n",
    "    df_len_by_month = pd.DataFrame(df.groupby(by=['year', 'month']).size()).reset_index().rename({0: \"len\"}, axis=1)\n",
    "    df_len_by_month = df_len_by_month.sort_values(['year', 'month'])\n",
    "    df_len_by_month['cumsum'] = df_len_by_month['len'].cumsum()\n",
    "    n_samples_total = df_len_by_month['cumsum'].iloc[-1]\n",
    "    n_samples_per_fold = np.ceil(n_samples_total / n_folds)\n",
    "    \n",
    "    df_len_by_month['inner_fold'] = df_len_by_month.apply(lambda row: int(row['cumsum'] / n_samples_per_fold), axis=1)\n",
    "    \n",
    "    df_w_fold = pd.merge(left=df, right=df_len_by_month, left_on=['year', 'month'], right_on=['year', 'month'])\n",
    "    \n",
    "    return df_w_fold\n",
    "\n",
    "df_inner_split = assign_inner_fold(df_train_outer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c52c61ab-7678-4ed8-beea-ec6aec33b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def define_model(\n",
    "    input_dim=20,\n",
    "    n_units=512,\n",
    "    activation='selu',#selu\n",
    "    learning_rate=0.0001,\n",
    "    loss='mse'\n",
    "):\n",
    "    inputs = Input(shape=(input_dim))\n",
    "    x = Dense(units=n_units, activation=activation)(inputs)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(units=n_units, activation=activation)(x)\n",
    "    x = Dropout(rate=0.5)(x)# serves as regularization\n",
    "    # x = Dense(units=int(n_units/2), activation=activation)(x)\n",
    "    outputs = Dense(units=1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(\n",
    "        optimizer=tf.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=loss,\n",
    "        metrics=[RootMeanSquaredError()]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "857a1b2b-3e8d-4c75-905e-cc5308fd6742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(df, skn, inner_fold, x_scale=True, y_scale=True):\n",
    "    df_station = df[df['skn'] == skn]\n",
    "    df_train = df_station[df_station['inner_fold'] != inner_fold]\n",
    "    df_test = df_station[df_station['inner_fold'] == inner_fold]\n",
    "    x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "    y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "    \n",
    "    if x_scale:\n",
    "        x_scaler = MinMaxScaler()\n",
    "        x_train = x_scaler.fit_transform(x_train)\n",
    "        x_test = x_scaler.transform(x_test)\n",
    "        \n",
    "    if y_scale:\n",
    "        y_scaler = MinMaxScaler(feature_range=(0,1))\n",
    "        y_train = np.log(y_train + 1.)\n",
    "        y_test = np.log(y_test + 1.)\n",
    "    \n",
    "        y_train = y_scaler.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_test = y_scaler.transform(y_test.reshape(-1, 1))\n",
    "    else:\n",
    "        y_scaler = None\n",
    "\n",
    "    return x_train, x_test, y_train, y_test, y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "25600fec-a566-495a-ba84-78e4b4e4f777",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 1s 78ms/step - loss: 0.1826 - root_mean_squared_error: 0.4230 - val_loss: 0.1917 - val_root_mean_squared_error: 0.4378\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.2971 - root_mean_squared_error: 0.5447 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3670 - root_mean_squared_error: 0.6058 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3627 - root_mean_squared_error: 0.6022 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3656 - root_mean_squared_error: 0.6046 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3678 - root_mean_squared_error: 0.6065 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3678 - root_mean_squared_error: 0.6064 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3687 - root_mean_squared_error: 0.6072 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3680 - root_mean_squared_error: 0.6066 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3670 - root_mean_squared_error: 0.6058 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3655 - root_mean_squared_error: 0.6046 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3657 - root_mean_squared_error: 0.6047 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3604 - root_mean_squared_error: 0.6003 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3647 - root_mean_squared_error: 0.6039 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3664 - root_mean_squared_error: 0.6053 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3637 - root_mean_squared_error: 0.6031 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 20ms/step - loss: 0.3691 - root_mean_squared_error: 0.6075 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3672 - root_mean_squared_error: 0.6060 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3680 - root_mean_squared_error: 0.6066 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3653 - root_mean_squared_error: 0.6044 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.3653 - root_mean_squared_error: 0.6044 - val_loss: 0.3797 - val_root_mean_squared_error: 0.6162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.27957300085156395"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skn = 54\n",
    "# df_station = df_inner_split.query(f'skn == {skn}')\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "batch_size=192\n",
    "\n",
    "x_train, x_test, y_train, y_test, y_scaler = prepare_dataset(df_inner_split, skn=skn, inner_fold=2, y_scale=True)\n",
    "params = {'input_dim': 16,\n",
    " 'n_units': 256,\n",
    " 'learning_rate': 0.09599436158210974,\n",
    " 'loss': 'mse'}\n",
    "model = define_model(**params)\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=500,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "y_pred = y_scaler.inverse_transform(y_pred)\n",
    "y_true = y_scaler.inverse_transform(y_test)\n",
    "\n",
    "y_pred = np.power(np.e, y_pred) - 1\n",
    "y_true = np.power(np.e, y_true) - 1\n",
    "y_true = y_test\n",
    "mean_squared_error(y_true, y_pred, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "ebd7fd0f-1d1a-414a-b34f-8c0a1a938e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.805417009638563"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test, _ = prepare_dataset(df_inner_split, skn=skn, inner_fold=0, y_scale=False)\n",
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_train, y_train)\n",
    "yhat = linear_regression.predict(x_test)\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "f2ed0e39-4b4b-4016-8e39-1a1201b85b5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_units': 256,\n",
       " 'learning_rate': 0.09599436158210974,\n",
       " 'batch_size': 192,\n",
       " 'loss': 'mse'}"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trial.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba97a3-8478-46aa-a8b0-1b65c62a569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sherpa.core:\n",
      "-------------------------------------------------------\n",
      "SHERPA Dashboard running. Access via\n",
      "http://10.100.11.207:8893 if on a cluster or\n",
      "http://localhost:8893 if running locally.\n",
      "-------------------------------------------------------\n",
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app 'sherpa.app.app' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:werkzeug: * Running on all addresses.\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "  4%|▍         | 1/24 [00:06<02:35,  6.76s/it]"
     ]
    }
   ],
   "source": [
    "parameters = [\n",
    "    sherpa.Choice('n_units', [256, 512, 1024]),\n",
    "    sherpa.Continuous('learning_rate', [0.001, 0.1]),\n",
    "    sherpa.Choice('batch_size', [64, 128, 192, 256, 512]),\n",
    "    sherpa.Choice('loss', ['mse', 'mae'])\n",
    "]\n",
    "n_run = 1\n",
    "alg = sherpa.algorithms.RandomSearch(max_num_trials=n_run)\n",
    "study = sherpa.Study(parameters=parameters, algorithm=alg, lower_is_better=True)\n",
    "dfs = []\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "dfs = []\n",
    "for i, trial in enumerate(study):\n",
    "    start = time.time()\n",
    "    params = {\n",
    "        'input_dim': len(columns),\n",
    "        'n_units': trial.parameters['n_units'],\n",
    "        'learning_rate': trial.parameters['learning_rate'],\n",
    "        'loss': trial.parameters['loss']\n",
    "    }\n",
    "    batch_size = trial.parameters['batch_size']\n",
    "    \n",
    "    for skn in tqdm(df_inner_split['skn'].unique()):        \n",
    "        ytest_station = []\n",
    "        yhat_station = []\n",
    "        for inner_fold in range(5):\n",
    "            x_train, x_test, y_train, y_test, y_scaler = prepare_dataset(df_inner_split, skn=skn, inner_fold=inner_fold)\n",
    "            model = define_model(**params)\n",
    "            model.fit(x_train, y_train, epochs=500, validation_split=0.2, callbacks=callbacks,\n",
    "                batch_size=batch_size,\n",
    "                verbose=0\n",
    "            )\n",
    "            yhat = model.predict(x_test)\n",
    "            # transform\n",
    "            yhat = y_scaler.inverse_transform(yhat)\n",
    "            yhat = np.power(np.e, yhat) - 1\n",
    "            # transform\n",
    "            y_test = y_scaler.inverse_transform(y_test)\n",
    "            y_test = np.power(np.e, y_test) - 1\n",
    "            \n",
    "            # record the result\n",
    "            yhat_station.extend(yhat)\n",
    "            ytest_station.extend(y_test)\n",
    "        \n",
    "        mae_station = mean_absolute_error(ytest_station, yhat_station)\n",
    "        rmse_station = mean_squared_error(ytest_station, yhat_station, squared=False)\n",
    "        \n",
    "        _ = pd.DataFrame([params])\n",
    "        _['skn'] = [skn]\n",
    "        _['batch_size'] = [batch_size]\n",
    "        _['mae'] = [mae_station]\n",
    "        _['rmse'] = [rmse_station]\n",
    "        _['trial_id'] = [i]\n",
    "        dfs.append(_)\n",
    "#     pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "423f941a-5312-4c39-a355-2a45b746706f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_dim</th>\n",
       "      <th>n_units</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>loss</th>\n",
       "      <th>skn</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>trial_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>54.0</td>\n",
       "      <td>256</td>\n",
       "      <td>29.453431</td>\n",
       "      <td>40.064505</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>79.0</td>\n",
       "      <td>256</td>\n",
       "      <td>36.228678</td>\n",
       "      <td>50.187502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>338.0</td>\n",
       "      <td>256</td>\n",
       "      <td>24.166258</td>\n",
       "      <td>29.664231</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>250.0</td>\n",
       "      <td>256</td>\n",
       "      <td>14.806082</td>\n",
       "      <td>15.067831</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>267.0</td>\n",
       "      <td>256</td>\n",
       "      <td>18.489656</td>\n",
       "      <td>19.001440</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>296.1</td>\n",
       "      <td>256</td>\n",
       "      <td>14.432521</td>\n",
       "      <td>14.680449</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>311.0</td>\n",
       "      <td>256</td>\n",
       "      <td>9.134524</td>\n",
       "      <td>11.522283</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>396.0</td>\n",
       "      <td>256</td>\n",
       "      <td>8.142749</td>\n",
       "      <td>10.036230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>400.0</td>\n",
       "      <td>256</td>\n",
       "      <td>10.048276</td>\n",
       "      <td>12.242109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>406.0</td>\n",
       "      <td>256</td>\n",
       "      <td>13.420642</td>\n",
       "      <td>14.718124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>410.0</td>\n",
       "      <td>256</td>\n",
       "      <td>5.041127</td>\n",
       "      <td>8.423491</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>485.0</td>\n",
       "      <td>256</td>\n",
       "      <td>7.086223</td>\n",
       "      <td>9.712690</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>703.0</td>\n",
       "      <td>256</td>\n",
       "      <td>18.632186</td>\n",
       "      <td>18.843185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>718.0</td>\n",
       "      <td>256</td>\n",
       "      <td>27.164066</td>\n",
       "      <td>29.288414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>770.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16.556475</td>\n",
       "      <td>18.241268</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>783.0</td>\n",
       "      <td>256</td>\n",
       "      <td>27.056257</td>\n",
       "      <td>29.110929</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>784.0</td>\n",
       "      <td>256</td>\n",
       "      <td>23.395987</td>\n",
       "      <td>25.954411</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>965.0</td>\n",
       "      <td>256</td>\n",
       "      <td>11.174728</td>\n",
       "      <td>13.672734</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>1075.0</td>\n",
       "      <td>256</td>\n",
       "      <td>36.535610</td>\n",
       "      <td>40.467782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>1117.0</td>\n",
       "      <td>256</td>\n",
       "      <td>12.785497</td>\n",
       "      <td>15.630019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>1134.0</td>\n",
       "      <td>256</td>\n",
       "      <td>29.370210</td>\n",
       "      <td>35.676391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>87.0</td>\n",
       "      <td>256</td>\n",
       "      <td>16.708656</td>\n",
       "      <td>21.979387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>702.7</td>\n",
       "      <td>256</td>\n",
       "      <td>9.423263</td>\n",
       "      <td>13.563096</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>0.097957</td>\n",
       "      <td>mse</td>\n",
       "      <td>1020.1</td>\n",
       "      <td>256</td>\n",
       "      <td>13.204036</td>\n",
       "      <td>15.615515</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   input_dim  n_units  learning_rate loss     skn  batch_size        mae  \\\n",
       "0         16     1024       0.097957  mse    54.0         256  29.453431   \n",
       "0         16     1024       0.097957  mse    79.0         256  36.228678   \n",
       "0         16     1024       0.097957  mse   338.0         256  24.166258   \n",
       "0         16     1024       0.097957  mse   250.0         256  14.806082   \n",
       "0         16     1024       0.097957  mse   267.0         256  18.489656   \n",
       "0         16     1024       0.097957  mse   296.1         256  14.432521   \n",
       "0         16     1024       0.097957  mse   311.0         256   9.134524   \n",
       "0         16     1024       0.097957  mse   396.0         256   8.142749   \n",
       "0         16     1024       0.097957  mse   400.0         256  10.048276   \n",
       "0         16     1024       0.097957  mse   406.0         256  13.420642   \n",
       "0         16     1024       0.097957  mse   410.0         256   5.041127   \n",
       "0         16     1024       0.097957  mse   485.0         256   7.086223   \n",
       "0         16     1024       0.097957  mse   703.0         256  18.632186   \n",
       "0         16     1024       0.097957  mse   718.0         256  27.164066   \n",
       "0         16     1024       0.097957  mse   770.0         256  16.556475   \n",
       "0         16     1024       0.097957  mse   783.0         256  27.056257   \n",
       "0         16     1024       0.097957  mse   784.0         256  23.395987   \n",
       "0         16     1024       0.097957  mse   965.0         256  11.174728   \n",
       "0         16     1024       0.097957  mse  1075.0         256  36.535610   \n",
       "0         16     1024       0.097957  mse  1117.0         256  12.785497   \n",
       "0         16     1024       0.097957  mse  1134.0         256  29.370210   \n",
       "0         16     1024       0.097957  mse    87.0         256  16.708656   \n",
       "0         16     1024       0.097957  mse   702.7         256   9.423263   \n",
       "0         16     1024       0.097957  mse  1020.1         256  13.204036   \n",
       "\n",
       "         mse  trial_id  \n",
       "0  40.064505         0  \n",
       "0  50.187502         0  \n",
       "0  29.664231         0  \n",
       "0  15.067831         0  \n",
       "0  19.001440         0  \n",
       "0  14.680449         0  \n",
       "0  11.522283         0  \n",
       "0  10.036230         0  \n",
       "0  12.242109         0  \n",
       "0  14.718124         0  \n",
       "0   8.423491         0  \n",
       "0   9.712690         0  \n",
       "0  18.843185         0  \n",
       "0  29.288414         0  \n",
       "0  18.241268         0  \n",
       "0  29.110929         0  \n",
       "0  25.954411         0  \n",
       "0  13.672734         0  \n",
       "0  40.467782         0  \n",
       "0  15.630019         0  \n",
       "0  35.676391         0  \n",
       "0  21.979387         0  \n",
       "0  13.563096         0  \n",
       "0  15.615515         0  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a979d19f-0f4b-4edc-b8c3-c910ce45dd66",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "46/46 - 1s - loss: 38.9308 - root_mean_squared_error: 6.2395 - val_loss: 26.6914 - val_root_mean_squared_error: 5.1664\n",
      "Epoch 2/300\n",
      "46/46 - 0s - loss: 30.4772 - root_mean_squared_error: 5.5206 - val_loss: 29.0285 - val_root_mean_squared_error: 5.3878\n",
      "Epoch 3/300\n",
      "46/46 - 0s - loss: 29.3935 - root_mean_squared_error: 5.4216 - val_loss: 26.9698 - val_root_mean_squared_error: 5.1932\n",
      "Epoch 4/300\n",
      "46/46 - 0s - loss: 29.2583 - root_mean_squared_error: 5.4091 - val_loss: 26.8287 - val_root_mean_squared_error: 5.1796\n",
      "Epoch 5/300\n",
      "46/46 - 0s - loss: 29.8452 - root_mean_squared_error: 5.4631 - val_loss: 28.0896 - val_root_mean_squared_error: 5.3000\n",
      "Epoch 6/300\n",
      "46/46 - 0s - loss: 28.5969 - root_mean_squared_error: 5.3476 - val_loss: 26.9990 - val_root_mean_squared_error: 5.1961\n",
      "Epoch 7/300\n",
      "46/46 - 0s - loss: 29.5139 - root_mean_squared_error: 5.4327 - val_loss: 26.5525 - val_root_mean_squared_error: 5.1529\n",
      "Epoch 8/300\n",
      "46/46 - 0s - loss: 29.9113 - root_mean_squared_error: 5.4691 - val_loss: 26.7959 - val_root_mean_squared_error: 5.1765\n",
      "Epoch 9/300\n",
      "46/46 - 0s - loss: 29.1568 - root_mean_squared_error: 5.3997 - val_loss: 27.7263 - val_root_mean_squared_error: 5.2656\n",
      "Epoch 10/300\n",
      "46/46 - 0s - loss: 29.2692 - root_mean_squared_error: 5.4101 - val_loss: 27.0605 - val_root_mean_squared_error: 5.2020\n",
      "Epoch 11/300\n",
      "46/46 - 0s - loss: 28.5423 - root_mean_squared_error: 5.3425 - val_loss: 27.1275 - val_root_mean_squared_error: 5.2084\n",
      "Epoch 12/300\n",
      "46/46 - 0s - loss: 28.6262 - root_mean_squared_error: 5.3503 - val_loss: 26.8223 - val_root_mean_squared_error: 5.1790\n",
      "Epoch 13/300\n",
      "46/46 - 0s - loss: 28.9278 - root_mean_squared_error: 5.3785 - val_loss: 26.3747 - val_root_mean_squared_error: 5.1356\n",
      "Epoch 14/300\n",
      "46/46 - 0s - loss: 27.8261 - root_mean_squared_error: 5.2750 - val_loss: 29.0201 - val_root_mean_squared_error: 5.3870\n",
      "Epoch 15/300\n",
      "46/46 - 0s - loss: 27.9147 - root_mean_squared_error: 5.2834 - val_loss: 27.2567 - val_root_mean_squared_error: 5.2208\n",
      "Epoch 16/300\n",
      "46/46 - 0s - loss: 28.8894 - root_mean_squared_error: 5.3749 - val_loss: 26.1582 - val_root_mean_squared_error: 5.1145\n",
      "Epoch 17/300\n",
      "46/46 - 0s - loss: 29.9070 - root_mean_squared_error: 5.4687 - val_loss: 26.7999 - val_root_mean_squared_error: 5.1769\n",
      "Epoch 18/300\n",
      "46/46 - 0s - loss: 27.9975 - root_mean_squared_error: 5.2913 - val_loss: 27.5839 - val_root_mean_squared_error: 5.2520\n",
      "Epoch 19/300\n",
      "46/46 - 0s - loss: 28.6811 - root_mean_squared_error: 5.3555 - val_loss: 29.3407 - val_root_mean_squared_error: 5.4167\n",
      "Epoch 20/300\n",
      "46/46 - 0s - loss: 28.6265 - root_mean_squared_error: 5.3504 - val_loss: 29.6689 - val_root_mean_squared_error: 5.4469\n",
      "Epoch 21/300\n",
      "46/46 - 0s - loss: 29.1706 - root_mean_squared_error: 5.4010 - val_loss: 29.9214 - val_root_mean_squared_error: 5.4700\n",
      "Epoch 22/300\n",
      "46/46 - 0s - loss: 28.3761 - root_mean_squared_error: 5.3269 - val_loss: 26.5185 - val_root_mean_squared_error: 5.1496\n",
      "Epoch 23/300\n",
      "46/46 - 0s - loss: 27.8233 - root_mean_squared_error: 5.2748 - val_loss: 26.7365 - val_root_mean_squared_error: 5.1707\n",
      "Epoch 24/300\n",
      "46/46 - 0s - loss: 28.5710 - root_mean_squared_error: 5.3452 - val_loss: 25.9193 - val_root_mean_squared_error: 5.0911\n",
      "Epoch 25/300\n",
      "46/46 - 0s - loss: 27.5126 - root_mean_squared_error: 5.2452 - val_loss: 27.7999 - val_root_mean_squared_error: 5.2726\n",
      "Epoch 26/300\n",
      "46/46 - 0s - loss: 28.1859 - root_mean_squared_error: 5.3090 - val_loss: 27.1088 - val_root_mean_squared_error: 5.2066\n",
      "Epoch 27/300\n",
      "46/46 - 0s - loss: 29.1880 - root_mean_squared_error: 5.4026 - val_loss: 26.0248 - val_root_mean_squared_error: 5.1015\n",
      "Epoch 28/300\n",
      "46/46 - 0s - loss: 28.1646 - root_mean_squared_error: 5.3070 - val_loss: 27.7215 - val_root_mean_squared_error: 5.2651\n",
      "Epoch 29/300\n",
      "46/46 - 0s - loss: 27.4372 - root_mean_squared_error: 5.2381 - val_loss: 26.4701 - val_root_mean_squared_error: 5.1449\n",
      "Epoch 30/300\n",
      "46/46 - 0s - loss: 28.4839 - root_mean_squared_error: 5.3370 - val_loss: 26.7804 - val_root_mean_squared_error: 5.1750\n",
      "Epoch 31/300\n",
      "46/46 - 0s - loss: 27.8289 - root_mean_squared_error: 5.2753 - val_loss: 26.5672 - val_root_mean_squared_error: 5.1543\n",
      "Epoch 32/300\n",
      "46/46 - 0s - loss: 27.6872 - root_mean_squared_error: 5.2619 - val_loss: 27.5346 - val_root_mean_squared_error: 5.2473\n",
      "Epoch 33/300\n",
      "46/46 - 0s - loss: 27.9348 - root_mean_squared_error: 5.2853 - val_loss: 28.2994 - val_root_mean_squared_error: 5.3197\n",
      "Epoch 34/300\n",
      "46/46 - 0s - loss: 28.3258 - root_mean_squared_error: 5.3222 - val_loss: 28.3210 - val_root_mean_squared_error: 5.3218\n",
      "Epoch 35/300\n",
      "46/46 - 0s - loss: 28.1787 - root_mean_squared_error: 5.3084 - val_loss: 29.8421 - val_root_mean_squared_error: 5.4628\n",
      "Epoch 36/300\n",
      "46/46 - 0s - loss: 27.4843 - root_mean_squared_error: 5.2425 - val_loss: 25.8032 - val_root_mean_squared_error: 5.0797\n",
      "Epoch 37/300\n",
      "46/46 - 0s - loss: 27.1310 - root_mean_squared_error: 5.2087 - val_loss: 26.5978 - val_root_mean_squared_error: 5.1573\n",
      "Epoch 38/300\n",
      "46/46 - 0s - loss: 27.3974 - root_mean_squared_error: 5.2343 - val_loss: 27.1779 - val_root_mean_squared_error: 5.2132\n",
      "Epoch 39/300\n",
      "46/46 - 0s - loss: 27.1753 - root_mean_squared_error: 5.2130 - val_loss: 27.0412 - val_root_mean_squared_error: 5.2001\n",
      "Epoch 40/300\n",
      "46/46 - 0s - loss: 27.0531 - root_mean_squared_error: 5.2013 - val_loss: 27.2145 - val_root_mean_squared_error: 5.2167\n",
      "Epoch 41/300\n",
      "46/46 - 0s - loss: 27.6116 - root_mean_squared_error: 5.2547 - val_loss: 25.6540 - val_root_mean_squared_error: 5.0650\n",
      "Epoch 42/300\n",
      "46/46 - 0s - loss: 27.7365 - root_mean_squared_error: 5.2665 - val_loss: 29.3175 - val_root_mean_squared_error: 5.4146\n",
      "Epoch 43/300\n",
      "46/46 - 0s - loss: 27.0852 - root_mean_squared_error: 5.2043 - val_loss: 27.9645 - val_root_mean_squared_error: 5.2882\n",
      "Epoch 44/300\n",
      "46/46 - 0s - loss: 27.2523 - root_mean_squared_error: 5.2204 - val_loss: 27.2607 - val_root_mean_squared_error: 5.2212\n",
      "Epoch 45/300\n",
      "46/46 - 0s - loss: 26.8973 - root_mean_squared_error: 5.1863 - val_loss: 26.1896 - val_root_mean_squared_error: 5.1176\n",
      "Epoch 46/300\n",
      "46/46 - 0s - loss: 27.4436 - root_mean_squared_error: 5.2387 - val_loss: 26.6527 - val_root_mean_squared_error: 5.1626\n",
      "Epoch 47/300\n",
      "46/46 - 0s - loss: 27.2254 - root_mean_squared_error: 5.2178 - val_loss: 28.5285 - val_root_mean_squared_error: 5.3412\n",
      "Epoch 48/300\n",
      "46/46 - 0s - loss: 26.0803 - root_mean_squared_error: 5.1069 - val_loss: 27.7828 - val_root_mean_squared_error: 5.2709\n",
      "Epoch 49/300\n",
      "46/46 - 0s - loss: 27.1141 - root_mean_squared_error: 5.2071 - val_loss: 26.3152 - val_root_mean_squared_error: 5.1298\n",
      "Epoch 50/300\n",
      "46/46 - 0s - loss: 26.4044 - root_mean_squared_error: 5.1385 - val_loss: 26.0356 - val_root_mean_squared_error: 5.1025\n",
      "Epoch 51/300\n",
      "46/46 - 0s - loss: 27.0906 - root_mean_squared_error: 5.2049 - val_loss: 26.9560 - val_root_mean_squared_error: 5.1919\n",
      "Epoch 52/300\n",
      "46/46 - 0s - loss: 26.6014 - root_mean_squared_error: 5.1577 - val_loss: 30.3866 - val_root_mean_squared_error: 5.5124\n",
      "Epoch 53/300\n",
      "46/46 - 0s - loss: 26.9212 - root_mean_squared_error: 5.1886 - val_loss: 25.9517 - val_root_mean_squared_error: 5.0943\n",
      "Epoch 54/300\n",
      "46/46 - 0s - loss: 26.7505 - root_mean_squared_error: 5.1721 - val_loss: 28.2039 - val_root_mean_squared_error: 5.3107\n",
      "Epoch 55/300\n",
      "46/46 - 0s - loss: 26.6031 - root_mean_squared_error: 5.1578 - val_loss: 26.4874 - val_root_mean_squared_error: 5.1466\n",
      "Epoch 56/300\n",
      "46/46 - 0s - loss: 26.0856 - root_mean_squared_error: 5.1074 - val_loss: 26.9454 - val_root_mean_squared_error: 5.1909\n",
      "Epoch 57/300\n",
      "46/46 - 0s - loss: 27.6773 - root_mean_squared_error: 5.2609 - val_loss: 25.4608 - val_root_mean_squared_error: 5.0459\n",
      "Epoch 58/300\n",
      "46/46 - 0s - loss: 27.6343 - root_mean_squared_error: 5.2568 - val_loss: 28.1078 - val_root_mean_squared_error: 5.3017\n",
      "Epoch 59/300\n",
      "46/46 - 0s - loss: 27.8806 - root_mean_squared_error: 5.2802 - val_loss: 27.5842 - val_root_mean_squared_error: 5.2521\n",
      "Epoch 60/300\n",
      "46/46 - 0s - loss: 26.9153 - root_mean_squared_error: 5.1880 - val_loss: 26.9786 - val_root_mean_squared_error: 5.1941\n",
      "Epoch 61/300\n",
      "46/46 - 0s - loss: 27.0492 - root_mean_squared_error: 5.2009 - val_loss: 29.4015 - val_root_mean_squared_error: 5.4223\n",
      "Epoch 62/300\n",
      "46/46 - 0s - loss: 27.8045 - root_mean_squared_error: 5.2730 - val_loss: 27.5940 - val_root_mean_squared_error: 5.2530\n",
      "Epoch 63/300\n",
      "46/46 - 0s - loss: 27.3881 - root_mean_squared_error: 5.2334 - val_loss: 28.0707 - val_root_mean_squared_error: 5.2982\n",
      "Epoch 64/300\n",
      "46/46 - 0s - loss: 26.7554 - root_mean_squared_error: 5.1726 - val_loss: 25.9903 - val_root_mean_squared_error: 5.0981\n",
      "Epoch 65/300\n",
      "46/46 - 0s - loss: 27.4802 - root_mean_squared_error: 5.2422 - val_loss: 26.6471 - val_root_mean_squared_error: 5.1621\n",
      "Epoch 66/300\n",
      "46/46 - 0s - loss: 27.0590 - root_mean_squared_error: 5.2018 - val_loss: 25.6459 - val_root_mean_squared_error: 5.0642\n",
      "Epoch 67/300\n",
      "46/46 - 0s - loss: 27.6208 - root_mean_squared_error: 5.2555 - val_loss: 28.2886 - val_root_mean_squared_error: 5.3187\n",
      "Epoch 68/300\n",
      "46/46 - 0s - loss: 26.7452 - root_mean_squared_error: 5.1716 - val_loss: 27.0490 - val_root_mean_squared_error: 5.2009\n",
      "Epoch 69/300\n",
      "46/46 - 0s - loss: 26.4369 - root_mean_squared_error: 5.1417 - val_loss: 28.3061 - val_root_mean_squared_error: 5.3204\n",
      "Epoch 70/300\n",
      "46/46 - 0s - loss: 26.6248 - root_mean_squared_error: 5.1599 - val_loss: 28.3442 - val_root_mean_squared_error: 5.3239\n",
      "Epoch 71/300\n",
      "46/46 - 0s - loss: 26.8295 - root_mean_squared_error: 5.1797 - val_loss: 27.3941 - val_root_mean_squared_error: 5.2339\n",
      "Epoch 72/300\n",
      "46/46 - 0s - loss: 27.3013 - root_mean_squared_error: 5.2251 - val_loss: 26.9668 - val_root_mean_squared_error: 5.1930\n",
      "Epoch 73/300\n",
      "46/46 - 0s - loss: 27.6096 - root_mean_squared_error: 5.2545 - val_loss: 26.7124 - val_root_mean_squared_error: 5.1684\n",
      "Epoch 74/300\n",
      "46/46 - 0s - loss: 27.3439 - root_mean_squared_error: 5.2291 - val_loss: 29.6303 - val_root_mean_squared_error: 5.4434\n",
      "Epoch 75/300\n",
      "46/46 - 0s - loss: 27.1347 - root_mean_squared_error: 5.2091 - val_loss: 27.5853 - val_root_mean_squared_error: 5.2522\n",
      "Epoch 76/300\n",
      "46/46 - 0s - loss: 26.4911 - root_mean_squared_error: 5.1470 - val_loss: 26.9481 - val_root_mean_squared_error: 5.1912\n",
      "Epoch 77/300\n",
      "46/46 - 0s - loss: 26.2495 - root_mean_squared_error: 5.1234 - val_loss: 27.5400 - val_root_mean_squared_error: 5.2479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b5aef437370>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = define_model_(\n",
    "    input_dim=len(columns)\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        min_delta=0,\n",
    "        patience=20,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.95,\n",
    "        patience=10\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train_scaled,\n",
    "    y_train,\n",
    "    # y_train_scaled,\n",
    "    epochs=500,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896ba8-6d28-4840-9c20-e5559453004e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff7f04a6-15c6-44a4-80ab-c0448a40682f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2035904468.8554227"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_scaled = model.predict(x_test_scaled)\n",
    "yhat = np.power(np.e, yhat_scaled) - 5\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1cdc9312-98cf-4050-a221-8975bae1c1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.361176043158557"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat = model.predict(x_test_scaled)\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8284fcf7-8b8d-4ab8-8a71-71e5ff29f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.4736767091311265"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_regression = LinearRegression()\n",
    "linear_regression.fit(x_train, y_train)\n",
    "yhat = linear_regression.predict(x_test)\n",
    "mean_squared_error(y_test, yhat, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b17889-45b9-4735-9a3e-9277b39deffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d79b3f-88ed-4e52-92f4-40552b0f23d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa17b76-af1e-4bc0-9205-a112ff1d74c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "fde4d2aa-556e-416e-baa4-79be81c67b1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: 5.893, LR: 6.190, skn: 54.0\n",
      "NN: 6.264, LR: 6.505, skn: 79.0\n",
      "NN: 4.954, LR: 4.873, skn: 338.0\n",
      "NN: 2.042, LR: 2.125, skn: 250.0\n",
      "NN: 2.348, LR: 2.326, skn: 267.0\n",
      "NN: 2.102, LR: 1.715, skn: 296.1\n",
      "NN: 1.521, LR: 1.508, skn: 311.0\n",
      "NN: 2.378, LR: 1.754, skn: 396.0\n",
      "NN: 2.067, LR: 1.862, skn: 400.0\n",
      "NN: 2.016, LR: 1.980, skn: 406.0\n",
      "NN: 2.814, LR: 1.983, skn: 410.0\n",
      "NN: 2.563, LR: 2.484, skn: 485.0\n",
      "NN: 2.369, LR: 2.309, skn: 703.0\n",
      "NN: 6.085, LR: 5.473, skn: 718.0\n",
      "NN: 2.570, LR: 2.472, skn: 770.0\n",
      "NN: 5.751, LR: 4.539, skn: 783.0\n",
      "NN: 6.970, LR: 5.822, skn: 784.0\n",
      "NN: 2.065, LR: 1.922, skn: 965.0\n",
      "NN: 3.535, LR: 3.613, skn: 1075.0\n",
      "NN: 4.296, LR: 3.962, skn: 1117.0\n",
      "NN: 3.968, LR: 3.909, skn: 1134.0\n",
      "NN: 7.059, LR: 6.456, skn: 87.0\n",
      "NN: 2.074, LR: 2.013, skn: 702.7\n",
      "NN: 2.566, LR: 2.655, skn: 1020.1\n"
     ]
    }
   ],
   "source": [
    "for skn in df_inner_split['skn'].unique():\n",
    "    df_train = df_inner_split.query(f'(fold == {fold}) & skn == {skn}')\n",
    "    df_test = df_inner_split.query(f'fold != {fold} & skn == {skn}')\n",
    "    x_train, x_test = np.array(df_train[columns]), np.array(df_test[columns])\n",
    "    y_train, y_test = np.array(df_train['data_in']), np.array(df_test['data_in'])\n",
    "\n",
    "    x_scaler = MinMaxScaler()\n",
    "    # y_scaler = MinMaxScaler()\n",
    "\n",
    "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "    x_test_scaled = x_scaler.transform(x_test)\n",
    "    \n",
    "    y_train_scaled = np.log(y_train + 5)\n",
    "    y_test_scaled = np.log(y_test + 5)\n",
    "\n",
    "    model = define_model_(\n",
    "        input_dim=len(columns)\n",
    "    )\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=0,\n",
    "            patience=50,\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.95,\n",
    "            patience=10\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.fit(\n",
    "        x_train_scaled,\n",
    "        y_train_scaled,\n",
    "        epochs=300,\n",
    "        batch_size=64,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    yhat_scaled = model.predict(x_test_scaled)\n",
    "    yhat = np.power(np.e, yhat_scaled) - 5\n",
    "    rmse_nn = mean_squared_error(y_test, yhat, squared=False)\n",
    "\n",
    "\n",
    "    linear_regression = LinearRegression()\n",
    "    linear_regression.fit(x_train, y_train)\n",
    "    yhat = linear_regression.predict(x_test)\n",
    "    rmse_lr = mean_squared_error(y_test, yhat, squared=False)\n",
    "    print(f\"NN: {rmse_nn:.3f}, LR: {rmse_lr:.3f}, skn: {skn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "79946328-5f26-4e88-8695-abd143c0f65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_inner_split[df_inner_split['skn'] == 54]['data_in'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "450fb077-71ee-4352-b3c1-da72c11b0499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.830e+00, 3.800e+00, 2.280e+00, 1.750e+00, 2.300e-01, 1.000e-02,\n",
       "       6.800e-01, 8.000e-02, 3.700e-01, 3.700e-01, 1.600e+00, 9.600e-01,\n",
       "       5.340e+00, 3.340e+00, 3.700e-01, 2.500e-01, 2.000e-02, 2.400e-01,\n",
       "       8.000e-02, 0.000e+00, 3.000e-02, 1.900e-01, 8.400e-01, 4.100e-01,\n",
       "       7.640e+00, 6.100e-01, 1.220e+00, 5.680e+00, 2.500e-01, 0.000e+00,\n",
       "       2.000e-01, 1.440e+00, 7.000e-02, 1.700e-01, 1.018e+01, 8.660e+00,\n",
       "       2.270e+00, 6.410e+00, 8.350e+00, 9.000e-02, 0.000e+00, 9.000e-02,\n",
       "       8.000e-02, 7.000e-02, 9.000e-02, 3.050e+00, 1.300e-01, 1.970e+00,\n",
       "       5.570e+00, 5.000e-01, 6.300e-01, 1.200e-01, 3.000e-02, 2.000e-02,\n",
       "       3.900e-01, 1.000e-01, 8.000e-02, 1.540e+00, 5.800e-01, 3.900e-01,\n",
       "       7.400e-01, 1.900e+00, 1.770e+00, 4.700e-01, 5.500e-01, 0.000e+00,\n",
       "       3.000e-02, 1.000e-02, 0.000e+00, 1.900e-01, 4.900e-01, 1.990e+00,\n",
       "       5.400e-01, 9.200e-01, 3.090e+00, 1.550e+00, 5.600e-01, 3.000e-02,\n",
       "       6.100e-01, 2.000e-01, 2.000e-02, 2.600e-01, 2.360e+00, 7.680e+00,\n",
       "       2.740e+00, 3.730e+00, 2.600e+00, 1.300e-01, 1.000e-01, 0.000e+00,\n",
       "       1.700e-01, 2.700e-01, 2.000e-02, 1.000e-02, 2.340e+00, 1.177e+01,\n",
       "       6.830e+00, 4.820e+00, 5.000e-01, 1.480e+00, 3.000e-02, 1.300e-01,\n",
       "       3.200e-01, 2.300e-01, 2.600e-01, 1.400e+00, 1.900e+00, 1.840e+00,\n",
       "       1.900e+00, 2.290e+00, 7.000e-02, 6.800e-01, 1.500e-01, 0.000e+00,\n",
       "       2.300e-01, 1.260e+00, 0.000e+00, 2.000e-01, 4.760e+00, 3.660e+00,\n",
       "       6.700e-01, 7.700e-01, 5.280e+00, 1.300e-01, 5.300e-01, 1.800e-01,\n",
       "       8.300e-01, 1.520e+00, 2.400e-01, 1.830e+00, 3.500e-01, 4.550e+00,\n",
       "       1.013e+01, 6.610e+00, 4.800e-01, 1.410e+00, 8.000e-01, 2.000e-02,\n",
       "       6.000e-02, 2.000e-02, 3.000e-02, 3.000e-02, 1.270e+00, 8.100e-01,\n",
       "       5.400e-01, 2.270e+00, 2.220e+00, 3.900e-01, 1.100e-01, 3.000e-02,\n",
       "       8.200e-01, 2.000e-02, 8.900e-01, 4.000e-02, 8.100e-01, 2.290e+00,\n",
       "       4.570e+00, 3.310e+00, 0.000e+00, 5.010e+00, 1.200e-01, 7.000e-02,\n",
       "       7.000e-02, 2.000e-02, 1.400e-01, 4.150e+00, 5.500e+00, 6.100e-01,\n",
       "       5.930e+00, 1.280e+00, 4.570e+00, 1.800e-01, 4.500e-01, 0.000e+00,\n",
       "       7.000e-02, 2.600e-01, 5.000e-02, 4.100e-01, 1.600e-01, 1.980e+00,\n",
       "       6.570e+00, 1.960e+00, 4.100e+00, 1.470e+00, 2.490e+00, 4.000e-02,\n",
       "       1.200e-01, 0.000e+00, 1.150e+00, 7.800e-01, 3.200e-01, 1.090e+00,\n",
       "       1.700e+00, 3.900e-01, 1.830e+00, 2.800e-01, 1.200e-01, 1.000e-01,\n",
       "       1.500e-01, 6.000e-02, 1.200e-01, 9.000e-02, 3.900e+00, 4.200e+00,\n",
       "       3.880e+00, 4.730e+00, 3.570e+00, 1.080e+00, 2.300e-01, 7.000e-02,\n",
       "       8.200e-01, 3.200e-01, 6.000e-01, 2.230e+00, 9.690e+00, 1.940e+00,\n",
       "       1.320e+00, 5.710e+00, 1.010e+00, 8.500e-01, 2.300e-01, 0.000e+00,\n",
       "       2.600e-01, 1.400e-01, 4.000e-01, 6.800e-01, 3.190e+00, 1.120e+00,\n",
       "       4.040e+00, 1.860e+00, 1.035e+01, 1.130e+00, 1.980e+00, 2.150e+00,\n",
       "       1.580e+00, 1.070e+00, 1.000e-02, 7.700e-01, 1.520e+00, 5.400e+00,\n",
       "       5.120e+00, 6.510e+00, 5.320e+00, 1.260e+00, 2.550e+00, 2.000e-02,\n",
       "       3.100e-01, 6.000e-02, 1.300e-01, 3.600e-01, 3.970e+00, 8.640e+00,\n",
       "       8.880e+00, 1.990e+00, 1.700e+00, 1.950e+00, 2.000e-01, 2.300e-01,\n",
       "       2.100e-01, 2.300e-01, 7.000e-02, 3.200e-01, 1.900e-01, 3.130e+00,\n",
       "       3.150e+00, 2.080e+00, 1.800e-01, 1.140e+00, 7.000e-02, 0.000e+00,\n",
       "       9.000e-02, 9.000e-02, 1.300e-01, 1.100e-01, 5.020e+00, 1.100e+00,\n",
       "       1.403e+01, 8.700e-01, 2.940e+00, 1.000e+00, 0.000e+00, 7.900e-01,\n",
       "       0.000e+00, 2.700e-01, 1.100e-01, 0.000e+00, 1.000e-01, 1.000e-01,\n",
       "       8.900e-01, 3.510e+00, 3.500e+00, 2.200e-01, 0.000e+00, 4.700e-01,\n",
       "       2.000e-02, 1.220e+00, 4.000e-02, 5.700e-01, 2.600e-01, 2.310e+00,\n",
       "       2.660e+00, 8.100e-01, 7.100e-01, 2.700e-01, 6.600e-01, 0.000e+00,\n",
       "       0.000e+00, 1.000e-02, 0.000e+00, 4.000e-01, 9.100e-01, 3.080e+00,\n",
       "       9.620e+00, 1.090e+00, 2.940e+00, 6.100e-01, 3.000e-02, 4.500e-01,\n",
       "       1.600e-01, 3.000e-02, 8.200e-01, 1.090e+00, 1.930e+00, 1.500e-01,\n",
       "       3.140e+00, 3.730e+00, 1.830e+00, 4.000e-02, 0.000e+00, 0.000e+00,\n",
       "       9.000e-02, 3.200e-01, 0.000e+00, 2.400e-01, 1.980e+00, 6.000e-02,\n",
       "       1.760e+00, 2.520e+00, 3.630e+00, 5.400e-01, 4.000e-02, 0.000e+00,\n",
       "       4.000e-02, 1.000e-02, 9.000e-02, 3.600e-01, 1.950e+00, 4.000e-02,\n",
       "       2.200e-01, 3.500e-01, 1.840e+00, 3.940e+00, 4.800e-01, 1.400e-01,\n",
       "       3.000e-01, 4.100e-01, 0.000e+00, 4.100e-01, 1.500e-01, 1.710e+00,\n",
       "       8.300e-01, 2.900e-01, 2.520e+00, 2.200e-01, 9.400e-01, 3.600e-01,\n",
       "       2.200e-01, 7.300e-01, 1.300e-01, 2.240e+00, 5.460e+00, 5.220e+00,\n",
       "       6.850e+00, 7.560e+00, 5.010e+00, 2.980e+00, 0.000e+00, 4.000e-02,\n",
       "       1.400e-01, 3.200e-01, 9.000e-02, 6.800e-01, 1.000e+00, 3.030e+00,\n",
       "       1.006e+01, 4.820e+00, 2.290e+00, 2.110e+00, 2.900e-01, 1.200e-01,\n",
       "       3.200e-01, 3.800e-01, 1.200e-01, 2.700e-01, 2.200e-01, 1.930e+00,\n",
       "       5.300e-01, 2.370e+00, 3.600e-01, 5.900e-01, 1.500e+00, 0.000e+00,\n",
       "       0.000e+00, 5.500e-01, 8.600e-01, 8.000e-01, 1.710e+00, 3.030e+00,\n",
       "       1.048e+01, 3.740e+00, 6.450e+00, 3.030e+00, 7.000e-02, 2.400e-01,\n",
       "       1.900e-01, 1.030e+00, 5.100e-01, 2.490e+00, 2.030e+00, 6.960e+00,\n",
       "       5.500e-01, 9.000e-02, 5.800e-01, 4.000e-02, 8.800e-01, 0.000e+00,\n",
       "       2.300e-01, 5.200e-01, 1.300e-01, 1.230e+00, 6.800e-01, 8.620e+00,\n",
       "       2.220e+00, 5.600e-01, 1.460e+00, 5.500e-01, 2.100e-01, 0.000e+00,\n",
       "       1.300e-01, 2.400e-01, 1.000e-01, 6.000e-02, 1.290e+00, 5.200e-01,\n",
       "       1.540e+00, 2.280e+00, 1.270e+00, 1.800e-01, 8.700e-01, 0.000e+00,\n",
       "       3.800e-01, 5.700e-01, 0.000e+00, 7.340e+00, 4.490e+00, 2.120e+00,\n",
       "       1.720e+00, 2.160e+00, 5.470e+00, 2.850e+00, 7.400e-01, 4.000e-01,\n",
       "       1.300e-01, 1.300e-01, 5.000e-02, 9.500e-01, 1.410e+00, 2.850e+00,\n",
       "       3.000e+00, 2.280e+00, 4.600e-01, 4.780e+00, 4.020e+00, 0.000e+00,\n",
       "       3.400e-01, 9.000e-02, 1.760e+00, 3.000e-02, 2.350e+00, 5.380e+00,\n",
       "       7.180e+00, 9.000e-01, 8.700e-01, 1.320e+00, 1.000e-02, 0.000e+00,\n",
       "       0.000e+00, 1.200e-01, 8.000e-02, 6.300e-01, 3.620e+00, 1.103e+01,\n",
       "       1.430e+00, 4.900e+00, 3.960e+00, 1.427e+01, 5.000e-02, 1.000e-02,\n",
       "       1.350e+00, 3.500e-01, 1.100e-01, 6.460e+00, 1.850e+00, 4.830e+00,\n",
       "       7.240e+00, 5.520e+00, 4.210e+00, 0.000e+00, 6.000e-01, 2.000e-01,\n",
       "       7.000e-02, 0.000e+00, 1.800e-01, 6.300e-01, 5.560e+00, 6.270e+00,\n",
       "       2.300e+00, 4.350e+00, 1.880e+00, 1.200e-01, 4.900e-01, 0.000e+00,\n",
       "       7.000e-02, 8.300e-01, 9.100e-01, 3.000e-01, 0.000e+00, 1.000e+00,\n",
       "       4.050e+00, 1.700e+00, 1.800e-01, 6.100e-01, 2.360e+00, 4.000e-02,\n",
       "       1.180e+00, 7.000e-02, 9.200e-01, 2.120e+00, 5.880e+00, 3.840e+00,\n",
       "       3.960e+00, 1.900e-01, 1.950e+00, 1.090e+00, 7.400e-01, 0.000e+00,\n",
       "       5.200e-01, 7.500e-01, 1.100e-01, 1.750e+00, 3.250e+00, 5.900e-01,\n",
       "       8.800e-01, 2.380e+00, 7.650e+00, 8.600e-01, 0.000e+00, 6.700e-01,\n",
       "       1.180e+00, 2.000e-01, 5.700e-01, 0.000e+00, 1.000e+00, 3.300e-01,\n",
       "       3.930e+00, 1.200e+00, 2.760e+00, 1.440e+00, 3.000e-01, 0.000e+00,\n",
       "       1.300e-01, 1.900e-01, 0.000e+00, 1.300e+00, 4.500e-01, 3.300e+00,\n",
       "       4.610e+00, 3.180e+00, 5.180e+00, 8.100e-01, 1.130e+00, 7.000e-02,\n",
       "       1.000e-01, 8.000e-02, 2.200e-01, 0.000e+00, 7.440e+00, 1.156e+01,\n",
       "       8.280e+00, 1.400e+00, 4.110e+00, 1.670e+00, 1.400e+00, 4.000e-02,\n",
       "       5.500e-01, 0.000e+00, 0.000e+00, 9.100e-01, 1.510e+00, 4.600e+00,\n",
       "       6.200e-01, 8.800e-01, 2.500e-01, 8.900e-01, 8.000e-02, 8.000e-02,\n",
       "       0.000e+00, 3.800e-01, 0.000e+00, 1.190e+00, 3.500e-01, 2.700e+00,\n",
       "       2.680e+00, 1.190e+00, 1.400e-01, 6.500e-01, 0.000e+00, 0.000e+00,\n",
       "       0.000e+00, 0.000e+00, 5.000e-01, 1.300e-01])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(df_inner_split[df_inner_split['skn'] == 396]['data_in'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "0be02c4c-deda-4404-bcd4-382badb8e938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.55178618, 2.17475172, 1.98513086, 1.9095425 , 1.65441128,\n",
       "       1.61143592, 1.73695123, 1.62531126, 1.68082791, 1.68082791,\n",
       "       1.88706965, 1.78507048, 2.33601987, 2.12106322, 1.68082791,\n",
       "       1.65822808, 1.61342993, 1.6563215 , 1.62531126, 1.60943791,\n",
       "       1.61541998, 1.6467337 , 1.7647308 , 1.68824909, 2.53686639,\n",
       "       1.72455072, 1.82776991, 2.36837283, 1.65822808, 1.60943791,\n",
       "       1.64865863, 1.86252854, 1.62334082, 1.64287269, 2.71997877,\n",
       "       2.61447185, 1.98375629, 2.43449016, 2.59151638, 1.62727783,\n",
       "       1.60943791, 1.62727783, 1.62531126, 1.62334082, 1.62727783,\n",
       "       2.08567209, 1.63510566, 1.94161522, 2.3580198 , 1.70474809,\n",
       "       1.72810944, 1.63315444, 1.61541998, 1.61342993, 1.68454538,\n",
       "       1.62924054, 1.62531126, 1.87793717, 1.71918878, 1.68454538,\n",
       "       1.74745921, 1.93152141, 1.91250109, 1.69927862, 1.71379793,\n",
       "       1.60943791, 1.61541998, 1.61143592, 1.60943791, 1.6467337 ,\n",
       "       1.70292826, 1.94448056, 1.7119945 , 1.77833645, 2.09062873,\n",
       "       1.87946505, 1.71559811, 1.61541998, 1.72455072, 1.64865863,\n",
       "       1.61342993, 1.66013103, 1.99605993, 2.54002595, 2.04640169,\n",
       "       2.16676537, 2.02814825, 1.63510566, 1.62924054, 1.60943791,\n",
       "       1.64287269, 1.66203036, 1.61342993, 1.61143592, 1.99333884,\n",
       "       2.81959158, 2.47063868, 2.28442112, 1.70474809, 1.86872051,\n",
       "       1.61541998, 1.63510566, 1.6714733 , 1.65441128, 1.66013103,\n",
       "       1.85629799, 1.93152141, 1.92278773, 1.93152141, 1.98650355,\n",
       "       1.62334082, 1.73695123, 1.63899671, 1.60943791, 1.65441128,\n",
       "       1.83418019, 1.60943791, 1.64865863, 2.2782924 , 2.15871472,\n",
       "       1.73518912, 1.75267208, 2.33020026, 1.63510566, 1.71018782,\n",
       "       1.64480506, 1.763017  , 1.87487438, 1.6563215 , 1.92132467,\n",
       "       1.67709656, 2.25654115, 2.71667953, 2.4518668 , 1.7011051 ,\n",
       "       1.85785927, 1.75785792, 1.61342993, 1.62136648, 1.61342993,\n",
       "       1.61541998, 1.61541998, 1.83577635, 1.75958057, 1.7119945 ,\n",
       "       1.98375629, 1.97685495, 1.68454538, 1.6311994 , 1.61541998,\n",
       "       1.76130026, 1.61342993, 1.773256  , 1.61740608, 1.75958057,\n",
       "       1.98650355, 2.25863321, 2.11745961, 1.60943791, 2.30358459,\n",
       "       1.63315444, 1.62334082, 1.62334082, 1.61342993, 1.63705308,\n",
       "       2.21375388, 2.35137526, 1.72455072, 2.3915113 , 1.83736998,\n",
       "       2.25863321, 1.64480506, 1.69561561, 1.60943791, 1.62334082,\n",
       "       1.66013103, 1.61938824, 1.68824909, 1.64093658, 1.94304892,\n",
       "       2.44841554, 1.94017947, 2.20827441, 1.86717611, 2.0135688 ,\n",
       "       1.61740608, 1.63315444, 1.60943791, 1.81645208, 1.75440368,\n",
       "       1.6714733 , 1.80664808, 1.90210753, 1.68454538, 1.92132467,\n",
       "       1.6639261 , 1.63315444, 1.62924054, 1.63899671, 1.62136648,\n",
       "       1.63315444, 1.62727783, 2.18605128, 2.21920348, 2.18380156,\n",
       "       2.2752139 , 2.14826773, 1.8050047 , 1.65441128, 1.62334082,\n",
       "       1.76130026, 1.6714733 , 1.7227666 , 1.97823904, 2.68716699,\n",
       "       1.93730177, 1.84371921, 2.37117788, 1.79342475, 1.76644166,\n",
       "       1.65441128, 1.60943791, 1.66013103, 1.63705308, 1.68639895,\n",
       "       1.73695123, 2.1029139 , 1.8115621 , 2.20165917, 1.92570744,\n",
       "       2.73111547, 1.81319475, 1.94304892, 1.96711236, 1.88403475,\n",
       "       1.80335861, 1.61143592, 1.75267208, 1.87487438, 2.34180581,\n",
       "       2.31451366, 2.44321622, 2.33408376, 1.83418019, 2.02154756,\n",
       "       1.61342993, 1.66959184, 1.62136648, 1.63510566, 1.67896398,\n",
       "       2.19388568, 2.61300665, 2.63044896, 1.94448056, 1.90210753,\n",
       "       1.93874166, 1.64865863, 1.65441128, 1.65057986, 1.65441128,\n",
       "       1.62334082, 1.6714733 , 1.6467337 , 2.09556092, 2.09801793,\n",
       "       1.95727391, 1.64480506, 1.81482474, 1.62334082, 1.60943791,\n",
       "       1.62727783, 1.62727783, 1.63510566, 1.6311994 , 2.3045831 ,\n",
       "       1.80828877, 2.94601668, 1.76985463, 2.07191328, 1.79175947,\n",
       "       1.60943791, 1.75613229, 1.60943791, 1.66203036, 1.6311994 ,\n",
       "       1.60943791, 1.62924054, 1.62924054, 1.773256  , 2.14124194,\n",
       "       2.14006616, 1.6524974 , 1.60943791, 1.69927862, 1.61342993,\n",
       "       1.82776991, 1.61740608, 1.71739505, 1.66013103, 1.98924327,\n",
       "       2.03601198, 1.75958057, 1.74221902, 1.66203036, 1.73342389,\n",
       "       1.60943791, 1.60943791, 1.61143592, 1.60943791, 1.68639895,\n",
       "       1.77664583, 2.08939187, 2.68239045, 1.80664808, 2.07191328,\n",
       "       1.72455072, 1.61541998, 1.69561561, 1.64093658, 1.61541998,\n",
       "       1.76130026, 1.80664808, 1.93585981, 1.63899671, 2.09679018,\n",
       "       2.16676537, 1.92132467, 1.61740608, 1.60943791, 1.60943791,\n",
       "       1.62727783, 1.6714733 , 1.60943791, 1.6563215 , 1.94304892,\n",
       "       1.62136648, 1.91102289, 2.01756614, 2.15524451, 1.7119945 ,\n",
       "       1.61740608, 1.60943791, 1.61740608, 1.61143592, 1.62727783,\n",
       "       1.67896398, 1.93874166, 1.61740608, 1.6524974 , 1.67709656,\n",
       "       1.92278773, 2.19053559, 1.7011051 , 1.63705308, 1.66770682,\n",
       "       1.68824909, 1.60943791, 1.68824909, 1.63899671, 1.90359895,\n",
       "       1.763017  , 1.66581825, 2.01756614, 1.6524974 , 1.78170913,\n",
       "       1.67896398, 1.6524974 , 1.74571553, 1.63510566, 1.97962121,\n",
       "       2.34755846, 2.32434658, 2.47232787, 2.53051716, 2.30358459,\n",
       "       2.07693841, 1.60943791, 1.61740608, 1.63705308, 1.6714733 ,\n",
       "       1.62727783, 1.73695123, 1.79175947, 2.08318453, 2.71204222,\n",
       "       2.28442112, 1.98650355, 1.96150224, 1.66581825, 1.63315444,\n",
       "       1.6714733 , 1.68268837, 1.63315444, 1.66203036, 1.6524974 ,\n",
       "       1.93585981, 1.71018782, 1.99741771, 1.67896398, 1.72097929,\n",
       "       1.87180218, 1.60943791, 1.60943791, 1.71379793, 1.7681496 ,\n",
       "       1.75785792, 1.90359895, 2.08318453, 2.73954887, 2.16791019,\n",
       "       2.43798973, 2.08318453, 1.62334082, 1.6563215 , 1.6467337 ,\n",
       "       1.79674701, 1.70656462, 2.0135688 , 1.95018671, 2.48156775,\n",
       "       1.71379793, 1.62727783, 1.71918878, 1.61740608, 1.77155676,\n",
       "       1.60943791, 1.65441128, 1.70837786, 1.63510566, 1.82937633,\n",
       "       1.73695123, 2.6115393 , 1.97685495, 1.71559811, 1.86562932,\n",
       "       1.71379793, 1.65057986, 1.60943791, 1.63510566, 1.6563215 ,\n",
       "       1.62924054, 1.62136648, 1.83896107, 1.70837786, 1.87793717,\n",
       "       1.98513086, 1.83577635, 1.64480506, 1.76985463, 1.60943791,\n",
       "       1.68268837, 1.71739505, 1.60943791, 2.51284602, 2.25023861,\n",
       "       1.96290773, 1.90508815, 1.96850998, 2.34851402, 2.06051353,\n",
       "       1.74745921, 1.68639895, 1.63510566, 1.63510566, 1.61938824,\n",
       "       1.78339122, 1.85785927, 2.06051353, 2.07944154, 1.98513086,\n",
       "       1.69744879, 2.28033948, 2.19944433, 1.60943791, 1.67522565,\n",
       "       1.62727783, 1.91102289, 1.61541998, 1.99470031, 2.33988088,\n",
       "       2.49979526, 1.77495235, 1.76985463, 1.84371921, 1.61143592,\n",
       "       1.60943791, 1.60943791, 1.63315444, 1.62531126, 1.72810944,\n",
       "       2.15408508, 2.77446197, 1.86097454, 2.29253476, 2.19277023,\n",
       "       2.95854948, 1.61938824, 1.61143592, 1.84845481, 1.67709656,\n",
       "       1.6311994 , 2.43886271, 1.92424865, 2.28543893, 2.50470928,\n",
       "       2.35327821, 2.22028985, 1.60943791, 1.7227666 , 1.64865863,\n",
       "       1.62334082, 1.60943791, 1.64480506, 1.72810944, 2.35707328,\n",
       "       2.42214433, 1.98787435, 2.23537634, 1.92861865, 1.63315444,\n",
       "       1.70292826, 1.60943791, 1.62334082, 1.763017  , 1.77664583,\n",
       "       1.66770682, 1.60943791, 1.79175947, 2.20276476, 1.90210753,\n",
       "       1.64480506, 1.72455072, 1.99605993, 1.61740608, 1.82131827,\n",
       "       1.62334082, 1.77833645, 1.96290773, 2.38692624, 2.17928688,\n",
       "       2.19277023, 1.6467337 , 1.93874166, 1.80664808, 1.74745921,\n",
       "       1.60943791, 1.70837786, 1.74919985, 1.6311994 , 1.9095425 ,\n",
       "       2.1102132 , 1.72097929, 1.77155676, 1.99877364, 2.53765722,\n",
       "       1.7681496 , 1.60943791, 1.73518912, 1.82131827, 1.64865863,\n",
       "       1.71739505, 1.60943791, 1.79175947, 1.67335124, 2.18941639,\n",
       "       1.82454929, 2.04898233, 1.86252854, 1.66770682, 1.60943791,\n",
       "       1.63510566, 1.6467337 , 1.60943791, 1.84054963, 1.69561561,\n",
       "       2.11625551, 2.26280422, 2.10169215, 2.32042501, 1.75958057,\n",
       "       1.81319475, 1.62334082, 1.62924054, 1.62531126, 1.6524974 ,\n",
       "       1.60943791, 2.52091709, 2.80699015, 2.58625914, 1.85629799,\n",
       "       2.20937271, 1.89761986, 1.85629799, 1.61740608, 1.71379793,\n",
       "       1.60943791, 1.60943791, 1.77664583, 1.87333946, 2.2617631 ,\n",
       "       1.72633166, 1.77155676, 1.65822808, 1.773256  , 1.62531126,\n",
       "       1.62531126, 1.60943791, 1.68268837, 1.60943791, 1.82293509,\n",
       "       1.67709656, 2.04122033, 2.03861955, 1.82293509, 1.63705308,\n",
       "       1.73165555, 1.60943791, 1.60943791, 1.60943791, 1.60943791,\n",
       "       1.70474809, 1.63510566])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.array(df_inner_split[df_inner_split['skn'] == 396]['data_in']) + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b5ef7698-a431-4e6b-912f-5ce7ca0b3255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.69314718, 1.09861229, 4.60517019])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3, 100])\n",
    "np.log(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df3d68-b064-4b84-9451-7df5df8de147",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "climate",
   "language": "python",
   "name": "climate"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
